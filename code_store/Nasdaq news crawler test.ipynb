{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c757e753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retry\n"
     ]
    }
   ],
   "source": [
    "## import load !\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import openpyxl\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime as dtime\n",
    "from datetime import timedelta\n",
    "from dateutil.parser import parse\n",
    "from time import strptime\n",
    "import pytz\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import time\n",
    "import pymysql\n",
    "import random\n",
    "\n",
    "from retrying import retry\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "@retry(stop_max_attempt_number=7, wait_random_min=600, wait_random_max=1200)\n",
    "def never_give_up_never_surrender():\n",
    "    print('retry')\n",
    "    #raise ConnectionError, raise MaxRetryError, rasie TimeoutException\n",
    "never_give_up_never_surrender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82f4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 DB TEST 접속\n",
    "db = pymysql.connect(host='127.0.0.1',  port = 3306 , user = 'root',\n",
    "                           password = \"1234\", db = 'jisoo', charset = 'utf8')\n",
    "\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd814ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 갱신 업데이트 표 데이터 프레임화 221125\n",
    "###  최종 예상 아웃풋 :   Symbol   Name   Class  * 5 개 \n",
    "###  최종 예상 아웃풋 :   GC:CMX   Gold   Metals\n",
    "\n",
    "commdities_url = 'https://www.nasdaq.com/market-activity/commodities/'\n",
    "\n",
    "path = 'chromedriver_107.exe'\n",
    "driver = webdriver.Chrome(path)  # , chrome_options=chrome_options)\n",
    "driver.get(commdities_url)\n",
    "\n",
    "table_data = []\n",
    "i = 0\n",
    "while True:  ## class가 변경될까봐, whlie 문으로 타협\n",
    "    table_i = '/html/body/div[3]/div/main/div[2]/article/div[2]/div/section['+str(i+1)+']/div/div'\n",
    "    table_xpath = driver.find_elements_by_xpath(table_i )\n",
    "    if len(table_xpath) == 0: # 더 이상 class 구분이 없다면.\n",
    "        break\n",
    "    table_info = commdities_table_info(table_xpath)\n",
    "    table_data += table_info\n",
    "    i += 1\n",
    "driver.close()\n",
    "commdities_df = pd.DataFrame(table_data, columns = ['Symbol','Name', 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36a3b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Symbol, Name, Class]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commdities_df\n",
    "#driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d2b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "for c in range(len(commdities_df)):\n",
    "    commditiy, class_name = commdities_df['Symbol'][c], commdities_df['Class'][c]\n",
    "    surl ='https://www.nasdaq.com/market-activity/commodities/' + commditiy  + '/news-headlines'\n",
    "    url_list.append(surl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cfdc730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity_nasdaq_news(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aecb10d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-29 17:31:07.881755\n",
      "0 https://www.nasdaq.com/market-activity/commodities/GC:CMX/news-headlines 8\n",
      "지연 에러!\n",
      "1 https://www.nasdaq.com/market-activity/commodities/SI:CMX/news-headlines 16\n",
      "2 https://www.nasdaq.com/market-activity/commodities/HG:CMX/news-headlines 24\n",
      "3 https://www.nasdaq.com/market-activity/commodities/PL:NMX/news-headlines 32\n",
      "4 https://www.nasdaq.com/market-activity/commodities/PA:NMX/news-headlines 40\n",
      "5 https://www.nasdaq.com/market-activity/commodities/CL:NMX/news-headlines 48\n",
      "6 https://www.nasdaq.com/market-activity/commodities/HO:NMX/news-headlines 56\n",
      "7 https://www.nasdaq.com/market-activity/commodities/RB:NMX/news-headlines 64\n",
      "8 https://www.nasdaq.com/market-activity/commodities/NG:NMX/news-headlines 72\n",
      "9 https://www.nasdaq.com/market-activity/commodities/BZ:NMX/news-headlines 80\n",
      "10 https://www.nasdaq.com/market-activity/commodities/ZW/news-headlines 88\n",
      "11 https://www.nasdaq.com/market-activity/commodities/ZC/news-headlines 89\n",
      "12 https://www.nasdaq.com/market-activity/commodities/ZS/news-headlines 97\n",
      "지연 에러!\n",
      "13 https://www.nasdaq.com/market-activity/commodities/ZM/news-headlines 105\n",
      "지연 에러!\n",
      "14 https://www.nasdaq.com/market-activity/commodities/ZL/news-headlines 113\n",
      "지연 에러!\n",
      "15 https://www.nasdaq.com/market-activity/commodities/ZO/news-headlines 121\n",
      "16 https://www.nasdaq.com/market-activity/commodities/ZR/news-headlines 129\n",
      "17 https://www.nasdaq.com/market-activity/commodities/KE/news-headlines 137\n",
      "18 https://www.nasdaq.com/market-activity/commodities/LE/news-headlines 138\n",
      "19 https://www.nasdaq.com/market-activity/commodities/GF/news-headlines 146\n",
      "20 https://www.nasdaq.com/market-activity/commodities/HE/news-headlines 154\n",
      "지연 에러!\n",
      "21 https://www.nasdaq.com/market-activity/commodities/DC/news-headlines 162\n",
      "지연 에러!\n",
      "지연 에러!\n",
      "지연 에러!\n",
      "22 https://www.nasdaq.com/market-activity/commodities/TT:NMX/news-headlines 170\n",
      "23 https://www.nasdaq.com/market-activity/commodities/YO:NMX/news-headlines 178\n",
      "24 https://www.nasdaq.com/market-activity/commodities/KT:NMX/news-headlines 186\n",
      "지연 에러!\n",
      "지연 에러!\n",
      "지연 에러!\n",
      "25 https://www.nasdaq.com/market-activity/commodities/CJ:NMX/news-headlines 194\n",
      "지연 에러!\n",
      "지연 에러!\n",
      "26 https://www.nasdaq.com/market-activity/commodities/LBS/news-headlines 197\n",
      "WorkingTime: 1489.3450632095337 sec\n",
      "2022-11-29 17:55:57.226818\n",
      "==================================================\n",
      "2022-11-29 18:31:08.359630\n",
      "지연 에러!\n",
      "지연 에러!\n",
      "0 https://www.nasdaq.com/market-activity/commodities/GC:CMX/news-headlines 8\n",
      "1 https://www.nasdaq.com/market-activity/commodities/SI:CMX/news-headlines 16\n",
      "2 https://www.nasdaq.com/market-activity/commodities/HG:CMX/news-headlines 24\n",
      "지연 에러!\n",
      "3 https://www.nasdaq.com/market-activity/commodities/PL:NMX/news-headlines 32\n",
      "지연 에러!\n",
      "4 https://www.nasdaq.com/market-activity/commodities/PA:NMX/news-headlines 40\n",
      "지연 에러!\n",
      "5 https://www.nasdaq.com/market-activity/commodities/CL:NMX/news-headlines 48\n",
      "6 https://www.nasdaq.com/market-activity/commodities/HO:NMX/news-headlines 56\n",
      "7 https://www.nasdaq.com/market-activity/commodities/RB:NMX/news-headlines 64\n",
      "8 https://www.nasdaq.com/market-activity/commodities/NG:NMX/news-headlines 72\n",
      "9 https://www.nasdaq.com/market-activity/commodities/BZ:NMX/news-headlines 80\n",
      "지연 에러!\n",
      "10 https://www.nasdaq.com/market-activity/commodities/ZW/news-headlines 88\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 무한 반복\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mschedule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\schedule\\__init__.py:780\u001b[0m, in \u001b[0;36mrun_pending\u001b[1;34m()\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pending\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;124;03m\"\"\"Calls :meth:`run_pending <Scheduler.run_pending>` on the\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;124;03m    :data:`default scheduler instance <default_scheduler>`.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m     \u001b[43mdefault_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\schedule\\__init__.py:100\u001b[0m, in \u001b[0;36mScheduler.run_pending\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m runnable_jobs \u001b[38;5;241m=\u001b[39m (job \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs \u001b[38;5;28;01mif\u001b[39;00m job\u001b[38;5;241m.\u001b[39mshould_run)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(runnable_jobs):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\schedule\\__init__.py:172\u001b[0m, in \u001b[0;36mScheduler._run_job\u001b[1;34m(self, job)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, CancelJob) \u001b[38;5;129;01mor\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m CancelJob:\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_job(job)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\schedule\\__init__.py:661\u001b[0m, in \u001b[0;36mJob.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CancelJob\n\u001b[0;32m    660\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning job \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 661\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_run \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_next_run()\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mcommodity_nasdaq_news\u001b[1;34m(url_list)\u001b[0m\n\u001b[0;32m     34\u001b[0m purl \u001b[38;5;241m=\u001b[39m href_xpath[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m purl_list\u001b[38;5;241m.\u001b[39mappend(purl)\n\u001b[1;32m---> 36\u001b[0m news_info \u001b[38;5;241m=\u001b[39m \u001b[43mnasdaq_news_inlink_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m news_list\u001b[38;5;241m.\u001b[39mappend(news_list)\n\u001b[0;32m     38\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mnasdaq_news_inlink_parser\u001b[1;34m(url, headers)\u001b[0m\n\u001b[0;32m     10\u001b[0m title \u001b[38;5;241m=\u001b[39m title[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#  언론사\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m press \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msyndicate-logo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-Logo\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#  날짜 \u001b[39;00m\n\u001b[0;32m     16\u001b[0m dt \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "#commodity_nasdaq_news(url_list)\n",
    "\n",
    "# 60분에 한번씩 함수 실행\n",
    "schedule.every(1).hour.do(commodity_nasdaq_news, url_list)\n",
    "\n",
    "\n",
    "# 무한 반복\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc3bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89e7db36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GC:CMX</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SI:CMX</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG:CMX</td>\n",
       "      <td>Copper</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PL:NMX</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PA:NMX</td>\n",
       "      <td>Palladium</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CL:NMX</td>\n",
       "      <td>Crude Oil</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HO:NMX</td>\n",
       "      <td>Heating Oil</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RB:NMX</td>\n",
       "      <td>RBOB Gasoline</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NG:NMX</td>\n",
       "      <td>Natural Gas</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BZ:NMX</td>\n",
       "      <td>Brent Crude</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZW</td>\n",
       "      <td>CBOT Wheat Future</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ZC</td>\n",
       "      <td>Corn</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ZS</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ZM</td>\n",
       "      <td>Soybean Meal</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ZL</td>\n",
       "      <td>Soybean Oil</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ZO</td>\n",
       "      <td>Oat</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ZR</td>\n",
       "      <td>Rough Rice</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KE</td>\n",
       "      <td>Red Wheat</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LE</td>\n",
       "      <td>Live Cattle</td>\n",
       "      <td>Meats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GF</td>\n",
       "      <td>Feeder Cattle</td>\n",
       "      <td>Meats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HE</td>\n",
       "      <td>Lean Hogs</td>\n",
       "      <td>Meats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DC</td>\n",
       "      <td>Milk</td>\n",
       "      <td>Meats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TT:NMX</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>Softs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>YO:NMX</td>\n",
       "      <td>NYMEX No. 11 Sugar</td>\n",
       "      <td>Softs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>KT:NMX</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Softs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CJ:NMX</td>\n",
       "      <td>Cocoa</td>\n",
       "      <td>Softs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LBS</td>\n",
       "      <td>Lumber</td>\n",
       "      <td>Softs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol                Name     Class\n",
       "0   GC:CMX                Gold    Metals\n",
       "1   SI:CMX              Silver    Metals\n",
       "2   HG:CMX              Copper    Metals\n",
       "3   PL:NMX            Platinum    Metals\n",
       "4   PA:NMX           Palladium    Metals\n",
       "5   CL:NMX           Crude Oil  Energies\n",
       "6   HO:NMX         Heating Oil  Energies\n",
       "7   RB:NMX       RBOB Gasoline  Energies\n",
       "8   NG:NMX         Natural Gas  Energies\n",
       "9   BZ:NMX         Brent Crude  Energies\n",
       "10      ZW   CBOT Wheat Future    Grains\n",
       "11      ZC                Corn    Grains\n",
       "12      ZS            Soybeans    Grains\n",
       "13      ZM        Soybean Meal    Grains\n",
       "14      ZL         Soybean Oil    Grains\n",
       "15      ZO                 Oat    Grains\n",
       "16      ZR          Rough Rice    Grains\n",
       "17      KE           Red Wheat    Grains\n",
       "18      LE         Live Cattle     Meats\n",
       "19      GF       Feeder Cattle     Meats\n",
       "20      HE           Lean Hogs     Meats\n",
       "21      DC                Milk     Meats\n",
       "22  TT:NMX              Cotton     Softs\n",
       "23  YO:NMX  NYMEX No. 11 Sugar     Softs\n",
       "24  KT:NMX              Coffee     Softs\n",
       "25  CJ:NMX               Cocoa     Softs\n",
       "26     LBS              Lumber     Softs"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nasdaq_commdities_table_info()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def nasdaq_news_crawling(url_list):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707f913f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def symbols_load():\n",
    "    etf_path = './data/ETF_top100_sample.xlsx'\n",
    "    df = pd.read_excel(etf_path)\n",
    "\n",
    "    symbol_list = []\n",
    "\n",
    "    for d in range(len(df['Symbol'])):\n",
    "        ticker = df['Symbol'].iloc[d]\n",
    "        symbol_list.append(ticker)\n",
    "    return symbol_list\n",
    "\n",
    "# 임의 100개\n",
    "def etf_symbol_list():\n",
    "    ## 임의 etf url_list 생성기 \n",
    "    etf_symbol_list = symbols_load()\n",
    "\n",
    "    etf_url_list = []\n",
    "\n",
    "    for e in range(len(etf_symbol_list)):\n",
    "        etf_url = 'https://www.barchart.com/etfs-funds/quotes/'+etf_symbol+'/news'\n",
    "        etf_url_list.append(etf_url)\n",
    "    return etf_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a778d73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print('='* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fc6b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07715053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_url_list(commdities_df):\n",
    "    url_list = []\n",
    "    for c in range(len(commdities_df)):\n",
    "        commditiy, class_name = commdities_df['Symbol'][c], commdities_df['Class'][c]\n",
    "        surl ='https://www.nasdaq.com/market-activity/commodities/' + commditiy  + '/news-headlines'\n",
    "        url_list.append(surl)\n",
    "    return url_list\n",
    "\n",
    "def nasdaq_commdities_table_info():\n",
    "    commdities_url = 'https://www.nasdaq.com/market-activity/commodities/'\n",
    "\n",
    "    path = 'chromedriver_107.exe'\n",
    "    driver = webdriver.Chrome(path)  # , chrome_options=chrome_options)\n",
    "    driver.get(commdities_url)\n",
    "\n",
    "    table_data = []\n",
    "    i = 0\n",
    "    while True:  ## class가 변경될까봐, whlie 문으로 타협\n",
    "        table_i = '/html/body/div[3]/div/main/div[2]/article/div[2]/div/section['+str(i+1)+']/div/div'\n",
    "        table_xpath = driver.find_elements_by_xpath(table_i )\n",
    "        if len(table_xpath) == 0: # 더 이상 class 구분이 없다면.\n",
    "            break\n",
    "        table_info = nasdaq_commdities_table_xpath(table_xpath)\n",
    "        table_data += table_info\n",
    "        i += 1\n",
    "    driver.close()\n",
    "    commdities_df = pd.DataFrame(table_data, columns = ['Symbol','Name', 'Class'])\n",
    "    \n",
    "    return commdities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd748c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 함수 \n",
    "def nasdaq_news_crawler(url_list):\n",
    "    now = dtime.now()\n",
    "    print(now)\n",
    "    # 시간 재기용  -> 오후 4:47 분 (30분후) -> 오후 5시 17분  ~ 25분까지 확인 \n",
    "    start = time.time()\n",
    "\n",
    "    # 뉴스 url 리스트 가져오기\n",
    "    purl_list = []\n",
    "    #url = \"https://www.nasdaq.com/market-activity/commodities/cl%3Anmx/news-headlines\"\n",
    "  \n",
    "    final_cnt = len(url_list)\n",
    "    d = 0\n",
    "    news_list = []\n",
    "    while True :\n",
    "        path = 'chromedriver_107.exe'\n",
    "        driver = webdriver.Chrome(path)  # , chrome_options=chrome_options)\n",
    "        url = url_list[d]\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "        except TimeoutException:\n",
    "            print('timeout!')\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "        cnt = 0\n",
    "\n",
    "        # 뉴스 게시글 \n",
    "        for i in range(8):\n",
    "            select_i = '/html/body/div[3]/div/main/div[2]/div[4]/div[3]/div/div[1]/div/div[1]/ul/li[' + str(i+1)+']/a'\n",
    "            href_xpath = driver.find_elements_by_xpath(select_i)\n",
    "            if len(href_xpath) == 0 :\n",
    "                continue\n",
    "            purl = href_xpath[0].get_attribute('href')\n",
    "            purl_list.append(purl)\n",
    "            news_info = nasdaq_news_inlink_parser(purl, headers)\n",
    "            news_list.append(news_list)\n",
    "            time.sleep(2)\n",
    "            cnt += 1\n",
    "        if cnt == 0:\n",
    "            print('지연 에러!')\n",
    "            driver.quit()\n",
    "            continue\n",
    "        print(d, url, len(purl_list))\n",
    "        d +=1\n",
    "        final_cnt -= 1 \n",
    "        driver.quit()\n",
    "        if final_cnt == 0:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "    print(\"WorkingTime: {} sec\".format(time.time() - start))  # 현재시각 - 시작시간 = 실행 시간\n",
    "    now = dtime.now()\n",
    "    print(now)\n",
    "    print('='* 50)\n",
    "    return purl_list, news_info\n",
    "\n",
    "def list_chunk(lsn,n):\n",
    "    return [lsn[i:i+n] for i in range(0, len(lsn), n)]\n",
    "\n",
    "def nasdaq_commdities_table_xpath(table_xpath):\n",
    "    class_name = table_xpath[0].find_element_by_tag_name('h2').text\n",
    "    record_extract = table_xpath[0].find_element_by_tag_name('tbody').text\n",
    "\n",
    "    record_list = record_extract.split('\\n')\n",
    "    sn_list = []\n",
    "\n",
    "    for r in range(len(record_list)):\n",
    "        record = record_list[r]\n",
    "        if record.find('%') >= 0:\n",
    "            continue\n",
    "        sn_list.append(record)\n",
    "    \n",
    "    list_chunked = list_chunk(sn_list, 2 )\n",
    "\n",
    "    for c in range(len(list_chunked)):\n",
    "        sn_data = list_chunked[c]\n",
    "        sn_data.append(class_name)\n",
    "    return list_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d293e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasdaq\n",
    "def nasdaq_news_inlink_parser(url, headers): #v2\n",
    "\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "    \n",
    "    #  언론사\n",
    "    press = soup.find('div', {'class':'syndicate-logo'}).find('img')['alt'].replace('-Logo','')\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('time' )\n",
    "\n",
    "    if dt == None:\n",
    "        dt = soup.find('p',{'class':'jupiter22-c-author-byline__timestamp'} )\n",
    "        dt_text = dt.text.replace(',','')\n",
    "        dt_split =  dt_text.split('—')\n",
    "        time_split = dt_split[1].split(' ')[1]\n",
    "        ampm = dt_split[1].split(' ')[2]\n",
    "        if ampm == 'am':\n",
    "            time_stamp = time_split\n",
    "        if ampm == 'pm':\n",
    "            time_hour = time_split.split(':')[0]\n",
    "            if time_hour == '12':\n",
    "                hour = time_hour\n",
    "            else:\n",
    "                hour = str(int(time_hour) + 12) \n",
    "            time_stamp = hour + ':' + time_split.split(':')[1]\n",
    "        dt_string = dt_split[0].strip()  + ' ' + time_stamp \n",
    "        \n",
    "        datetime_type = dtime.strptime(dt_string, '%B %d %Y %H:%M')\n",
    "      \n",
    "            \n",
    "    else:\n",
    "        dt = dt.text\n",
    "        if dt.find('AM') >= 0:\n",
    "            dt_string = dt.split('AM')[0]\n",
    "        if dt.find('PM') >= 0:\n",
    "            date_fram = dt.split('PM')[0]\n",
    "            date_split = date_fram.split(' ')\n",
    "            time_split = date_split[2].split(':')\n",
    "            if time_split[0] == '12':\n",
    "                hour = time_split[0] \n",
    "            else:\n",
    "                hour = str(int(time_split[0])+ 12)\n",
    "            time_hm = hour + ':' + time_split[1]\n",
    "            dt_string = date_split[0] + ' ' + date_split[1] + ' ' + time_hm\n",
    "        datetime_type = dtime.strptime(dt_string, '%b %d, %Y %H:%M')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'body__content'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('please visit') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\n','')        \n",
    "    news_data = [url, title, body, datetime_type, now, press]\n",
    "    \n",
    "    return news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8ba843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################### 시장 시간대 구역 ############################\n",
    "# 시간 관련 라이브러리에서 종종 1자리수 숫자에 0 안붙이는거 해결...\n",
    "def date_zero_padding(m):\n",
    "    if m < 10:\n",
    "        m = '0' + str(m)\n",
    "    else:\n",
    "        m = str(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "# 노멀한 메타 데이터에서 시간 추출\n",
    "def yahoo_market_time(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def yahoo_market_search_time(url, market_time):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if market_time == 0:\n",
    "        market_time = ['00:10:00', '00:20:00']\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "\n",
    "    start_hms = market_time[0]\n",
    "    end_hms = market_time[1]\n",
    "    # start_hms = '16:00:00'\n",
    "    # end_hms = '16:30:00'\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "    ymd_date = str(usa_est.year) + '-' + str(usa_est.month) + '-' + str(usa_est.day)\n",
    "\n",
    "    limit_start = ymd_date + ' ' + start_hms\n",
    "    limit_end = ymd_date + ' ' + end_hms\n",
    "\n",
    "    limit_start = dtime.strptime(limit_start, '%Y-%m-%d %H:%M:%S')\n",
    "    limit_end = dtime.strptime(limit_end, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return date_time, limit_start, limit_end\n",
    "\n",
    "\n",
    "###################핀 비즈 구역 ############################\n",
    "\n",
    "def finviz_news_crawler(db, category, ticker_list, ymd_date):\n",
    "    news_info_list = []\n",
    "    error_cnt = 0\n",
    "\n",
    "    for t in range(len(ticker_list)):\n",
    "        ticker = ticker_list[t][0]\n",
    "        # ticker, s_name, s_code = ticker_list[1][0], ticker_list[1][1], ticker_list[1][2]\n",
    "        try:\n",
    "            fin_dic = finviz_news_crawling(ticker, ymd_date)\n",
    "        except:\n",
    "            error_cnt += 1\n",
    "            print('[' + str(error_cnt) + ']', ticker)\n",
    "\n",
    "            continue\n",
    "        if len(fin_dic) == 0:\n",
    "            continue  # 오늘 데이터 없으면 다음 티커 를 탐색\n",
    "        p_list = fin_dic[ymd_date]\n",
    "\n",
    "        for u in range(len(p_list)):\n",
    "            url = p_list[u][-1]\n",
    "            news_checking = finviz_news_check(db, url, ticker)\n",
    "            if news_checking == 1:\n",
    "                continue  # 중복된 url + 종목의 뉴스가 있다면 스킵\n",
    "\n",
    "            try:\n",
    "                news_info = fyahoo_stock_news_inlink_extract(url, headers)\n",
    "                # [news_info] ... + 분류명 (종목 뉴스) , 티커, 종목명, 종목 코드\n",
    "                # [news_info] ... + categroy ('stock_news') , ticker, stock_name, stock_code\n",
    "                # category column 변경 필요 ->  market_time 으로, 기존의 시황 뉴스는 category = 'market_news' 로\n",
    "\n",
    "                news_info_add = news_info + [category] + [ticker]\n",
    "                url, title, body = news_info_add[0], news_info_add[1], news_info_add[2]\n",
    "                w_date, n_date, press = news_info_add[3], news_info_add[4], news_info_add[5]\n",
    "                category, ticker = news_info_add[6], news_info_add[7]\n",
    "\n",
    "                news_input = stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker)\n",
    "                nadd_info = [url, title, body, w_date, n_date, press, category, ticker]\n",
    "                # 나중엔 삭제 똔느 스킵\n",
    "                news_info_list.append(nadd_info)\n",
    "                # print(url,nadd_info[6:])  # 확인용\n",
    "\n",
    "                ## db 저장하는 함수 필요 : 최근 글 중복 처리하는 방법도 필요\n",
    "\n",
    "            except:\n",
    "                print('Abnormal detected :', url)\n",
    "                continue\n",
    "            time.sleep(2)\n",
    "        # print(ticker,len(news_info_list))\n",
    "    print(error_cnt)\n",
    "    return news_info_list\n",
    "\n",
    "\n",
    "def finviz_news_crawling(name, ymd_date):\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks', 'The Telegraph',\n",
    "                    'Insider Monkey',\n",
    "                    'Benzinga', 'Simply Wall St.', 'Business Wire', 'The Independent', 'Fortune', 'CoinDesk', 'Variety',\n",
    "                    'PR Newswire', 'WWD', 'Skift', 'LA Times', 'The Guardian', 'Poets & Quants', 'GuruFocus.com']\n",
    "\n",
    "    url = \"https://finviz.com/quote.ashx?t=\" + name + \"&p=d\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    date_find = soup.find_all('table', {'class': 'fullview-news-outer'})\n",
    "    date_finding = date_find[0].find_all('td', {'align': 'right'})\n",
    "\n",
    "    news_link = date_find[0].find_all('div', {'class': 'news-link-left'})\n",
    "    news_press = date_find[0].find_all('div', {'class': 'news-link-right'})\n",
    "\n",
    "    purl_list = []\n",
    "\n",
    "    date_dic = {}  # 만약을 위해  view에 보이는 날짜 전부를 수집, 평소엔 가장 상단의 키만을 사용 -> 다음 키 생성시 break\n",
    "\n",
    "    for i in range(len(news_link)):\n",
    "        press_name = news_press[i].find('span').text.lstrip()\n",
    "        press_title = news_link[i].text\n",
    "        press_url = news_link[i].find('a')['href']\n",
    "\n",
    "        date_time = date_finding[i].text\n",
    "        date_split_time = date_time.split(' ')\n",
    "        if len(date_split_time) >= 2:\n",
    "            ymdate = date_split_time[0].split('-')\n",
    "            m, d, y = ymdate[0], ymdate[1], ymdate[2]\n",
    "            m = strptime(m, '%b').tm_mon\n",
    "            if m < 10:\n",
    "                m = '0' + str(m)\n",
    "            else:\n",
    "                m = str(m)\n",
    "            ymd_str = '20' + y + '-' + m + '-' + d  # dict 키 용\n",
    "            dtime_pro = date_split_time[1]\n",
    "\n",
    "            if ymd_str != ymd_date:\n",
    "                break  # 오늘 아니면 탈출\n",
    "\n",
    "            date_dic[ymd_str] = []\n",
    "\n",
    "            if len(date_dic) > 2:\n",
    "                del date_dic[ymd_str]\n",
    "                break  # 딱 하루치만 보기\n",
    "\n",
    "        else:\n",
    "            dtime_pro = date_split_time[0]\n",
    "\n",
    "        if press_url.find('finance.yahoo') == -1:\n",
    "            continue\n",
    "        if press_name not in c_press_list:\n",
    "            continue\n",
    "\n",
    "        date_dic[ymd_str].append([dtime_pro, press_name, press_title, press_url])\n",
    "    return date_dic\n",
    "\n",
    "\n",
    "def finviz_news_check(db, url, ticker):\n",
    "    # db에 url 가 있다면 break! ]\n",
    "    checking_sql = \"SELECT * FROM US_Stock_Market_News WHERE url = '\" + url + \"' and symbol ='\" + ticker + \"'\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    if len(result) >= 1:\n",
    "        result = 1  # 1이면 contine\n",
    "    else:\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "\n",
    "def stock_info_load(db, select_ticker):\n",
    "    checking_sql = \"SELECT symbol, name, ISIN FROM Company_Information_Data\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    df_db = pd.DataFrame(result, columns=['symbol', 'name', 'ISIN'])\n",
    "\n",
    "    target_sname = []\n",
    "    if select_ticker != 0:\n",
    "        # 선택한 것만 하는 것\n",
    "        for s in range(len(select_ticker)):\n",
    "            ticker = select_ticker[s]\n",
    "            stock_info = df_db[df_db['symbol'] == ticker]\n",
    "            list_con = stock_info.values.tolist()\n",
    "            target_sname.append(list_con[0])\n",
    "\n",
    "        df = pd.DataFrame(target_sname, columns=['symbol', 'name', 'ISIN'])\n",
    "    else:\n",
    "        df = df_db\n",
    "    ticker_list = []\n",
    "\n",
    "    for d in range(len(df)):\n",
    "        ticker = df['symbol'].iloc[d]\n",
    "        s_name = df['name'].iloc[d]\n",
    "        s_code = df['ISIN'].iloc[d]\n",
    "        ticker_list.append([ticker, s_name, s_code])\n",
    "\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "###################야후 파이낸스 구역 ############################\n",
    "\n",
    "def yahoo_crawler(db, mtime):\n",
    "    ## 미국 주식 거래시간\n",
    "    us_market_time_dic = {'BMO(UST)': ['07:30:00', '09:30:00'], 'AMO(UST)': ['10:30:00', '11:30:00'],\n",
    "                          'AMC(UST)': ['17:00:00', '18:30:00'], 'BMO(DST)': ['06:30:00', '09:00:00'],\n",
    "                          'AMO(DST)': ['09:30:00', '10:30:00'], 'AMC(DST)': ['16:00:00', '17:30:00']}\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks',\n",
    "                    'Fortune' 'LA Times', 'Kiplinger', 'Business Insider', 'USA TODAY', 'Insider']\n",
    "\n",
    "    o_press_list = [\"Investor's  Daily\", 'TheStreet.com']  # 아웃링크 뉴스 언론사\n",
    "    c_press_list = c_press_list + o_press_list\n",
    "    # c_press_list = ['Bloomberg', 'Reuters', 'Kiplinger', 'Business Insider', 'Yahoo Finance', 'USA TODAY', 'LA Times']\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    url_list = yahoo_crawling(db)\n",
    "    market_time = us_market_time_dic[mtime]\n",
    "    # c = category\n",
    "    # 임시 리턴\n",
    "    pre_urls = []\n",
    "    data_list = []\n",
    "    error_list = []\n",
    "    for u in range(len(url_list)):\n",
    "        source = url_list[u][0]\n",
    "        w_press = url_list[u][1]\n",
    "        w_press = w_press.replace('News', '').lstrip()\n",
    "        pre_urls.append([w_press, source])\n",
    "        if w_press in c_press_list:  # 검증한 언론사들이 포함되어 있다면,\n",
    "            # 시간대 좁히기\n",
    "            date_time, limit_start, limit_end = yahoo_market_search_time(source, market_time)\n",
    "            if date_time >= limit_start and date_time < limit_end:  # 시황 마감후 1시 30분 까지가 아니라면\n",
    "                try:\n",
    "                    news_parsing = fyahoo_stock_news_inlink_extract(source, headers)\n",
    "                except:\n",
    "                    error_news = [w_press, source]\n",
    "                    print('error :', error_news)\n",
    "                    error_list.append(error_news)\n",
    "                    continue\n",
    "                title, body = news_parsing[1], news_parsing[2]\n",
    "                # 시황 뉴스의 경우, 시간대 체크 -> 범용으로 쓰이기 위해 따로 둠 v1 -> v2\n",
    "                if title_classifier(title) == 0:\n",
    "                    continue\n",
    "                w_date, n_date, press = news_parsing[3], news_parsing[4], news_parsing[5]\n",
    "                in_data = [source, title, body, w_date, n_date, press, mtime]\n",
    "                # print(in_data)\n",
    "                data_list.append(in_data)\n",
    "                market_news_insert(db, source, title, body, w_date, n_date, press, mtime)\n",
    "\n",
    "    return pre_urls, data_list, error_list\n",
    "\n",
    "\n",
    "## 220614-15 임시 수집 타이밍  :  야후 파이낸스\n",
    "def yahoo_crawling(db):\n",
    "    url = 'https://finance.yahoo.com/topic/stock-market-news/'\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # chrome_options = webdriver.ChromeOptions()\n",
    "    # chrome_options.add_argument('--headless')\n",
    "    # chrome_options.add_argument('--no-sandbox')\n",
    "    # chrome_options.add_argument(\"--single-process\")\n",
    "    # chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    path = 'chromedriver_107.exe'\n",
    "    driver = webdriver.Chrome(path)  # , chrome_options=chrome_options)\n",
    "    count = 200  # 페이지 한번에 보여줄수 있는양\n",
    "    url_list = []\n",
    "    driver.get(url)\n",
    "    # 스크롤 전 높이\n",
    "    before_h = driver.execute_script(\"return window.scrollY\")\n",
    "    # 무한 스크롤\n",
    "    while True:\n",
    "        # 맨 아래로 스크롤을 내린다.\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_css_selector(\"body\").send_keys(Keys.END)\n",
    "        # 스크롤 사이 페이지 로딩 시간\n",
    "        time.sleep(2)\n",
    "        # 스크롤 후 높이\n",
    "        after_h = driver.execute_script(\"return window.scrollY\")\n",
    "        if after_h == before_h:\n",
    "            break\n",
    "        before_h = after_h\n",
    "    #### 페이지내 url 긁어오기\n",
    "    url_list = []\n",
    "\n",
    "    for i in range(count):  # 페이지 한계로 200 개 정도 확인\n",
    "        press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[2]'\n",
    "        href_i = press_i + '/h3/a'\n",
    "        href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "        press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[1]'\n",
    "            href_i = press_i + '/h3/a'\n",
    "            href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "            press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            continue\n",
    "\n",
    "        href = href_xpath[0].get_attribute('href')\n",
    "        title = href_xpath[0].text\n",
    "        press = press_xpath[0].text\n",
    "        press = press.split('•')\n",
    "        press = press[0].replace('Business', '').lstrip()\n",
    "        url_list.append([href, press])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "################### 파싱 함수구역 ############################\n",
    "\n",
    "# 인링크 방식 뉴스 내용 파싱 : 야후 버전 실상 버전 2이나 구별을 위해 inlink만 사용\n",
    "def fyahoo_stock_news_inlink_extract(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    news_data = []\n",
    "\n",
    "    date_time = yahoo_market_time(url)\n",
    "    now = dtime.now()\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    press = soup.find(\"img\", {'class': \"caas-img\"})\n",
    "    press = press['alt']\n",
    "\n",
    "    conts = soup.find_all('p')  # .text\n",
    "    content = soup.find_all('div', {'class': 'caas-body'})\n",
    "    body_p = content[0]('p')\n",
    "    body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'USA TODAY':\n",
    "        headline = context_preprocessing(conts[1:3])\n",
    "        # body = context_preprocessing(conts)\n",
    "    if press == 'Reuters':\n",
    "        # body_p = period_index(body_p)\n",
    "        body = body_p_clean(body_p, press)\n",
    "        body = body.split('(Reporting')\n",
    "        bodys = body[0]\n",
    "        body = press_local_find(bodys, press)\n",
    "    if press == 'LA Times':\n",
    "        body = body.split('This story originally')\n",
    "        body = body[0]\n",
    "    if press == 'Bloomberg':\n",
    "        body = body.split('(Updates')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Investing.com':\n",
    "        body = body.split('Related Articles')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Zacks':\n",
    "        body = body.split('Want the latest')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == \"Investor's Business Daily\":  # 아웃링크\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        content = soup.find_all('div', {'class': 'single-post-content post-content drop-cap'})\n",
    "        body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'TheStreet.com':\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if out_url.find('realmoney') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-author-rail__body'})\n",
    "            body_p = content[0]('p')\n",
    "        elif out_url.find('aap.') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-back-header__body'})\n",
    "            body_p = content[0]('div')\n",
    "        else:\n",
    "            content = soup.find_all('div', {'class': 'm-detail--body'})\n",
    "            body_p = content[0]('p')\n",
    "        # body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    # body = body_split(body)\n",
    "    # headline_token = sent_tokenize(body)\n",
    "    # if len(headline_token) <= 2:\n",
    "    #    headline = body\n",
    "    # else:\n",
    "    #    headline = headline_token[0] + ' ' + headline_token[1]\n",
    "\n",
    "    inlink_news = [url, title, body, date_time, now, press]\n",
    "\n",
    "    return inlink_news\n",
    "\n",
    "\n",
    "################### 기타 전처리 함수구역 ############################\n",
    "\n",
    "# 만일을 위해서 함수화 했는데.. 생각보다 전처리가 잘 되었기에 ...\n",
    "def clean_text(text):\n",
    "    # cleaned_text = text.replace(\"\\'\", \"'\")\n",
    "    cleaned_text = text.replace('  \\n ', '')\n",
    "    cleaned_text = cleaned_text.replace('(Bloomberg) -- ', '')\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# 날짜를 datetime format으로 변환 하기 위한 함수\n",
    "def dt_format(dt_texts):\n",
    "    a = dt_texts.find('(')\n",
    "    b = dt_texts.find(')')\n",
    "\n",
    "    result = dt_texts[a + 1:b]\n",
    "    result = result.replace(',', '')\n",
    "    dt = result.split(' ')\n",
    "    dt_pm = dt[3].find('PM')  # 있으면 + 없으면 -1 반환\n",
    "\n",
    "    if dt_pm != -1:  ## 오후 타임이면,\n",
    "        pre_dt = dt[3].replace('PM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        if int(pre_dt[0]) == 12:\n",
    "            t, m = pre_dt[0], pre_dt[1]\n",
    "        else:\n",
    "            pm_time = str(int(pre_dt[0]) + 12)\n",
    "            t, m = pm_time, pre_dt[1]\n",
    "\n",
    "    else:  # 오전타임이면\n",
    "        pre_dt = dt[3].replace('AM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        t, m = pre_dt[0], pre_dt[1]\n",
    "\n",
    "    dt_mon = strptime(dt[0], '%b').tm_mon\n",
    "\n",
    "    if dt_mon < 10:\n",
    "        dt_mon = '0' + str(dt_mon)\n",
    "\n",
    "    str_datetimed = dt[2] + '-' + str(dt_mon) + '-' + dt[1] + ' ' + t + \":\" + m + \":\" + '00'\n",
    "    currdate = datetime.datetime.strptime(str_datetimed, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return currdate\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def context_preprocessing(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    for i in range(len(cp)):\n",
    "        texts = texts + cp[i].text + ' '\n",
    "        texts_list.append(cp[i].text)\n",
    "    cleans = clean_text(texts)\n",
    "    return cleans\n",
    "\n",
    "\n",
    "### 뉴스 분류  : 키워드 방식인데, 아무래도 일부 키워드는 해결 불가하기에, 모델 구축 방향으로 가야함, 데이터가 쌓이면\n",
    "\n",
    "# 맞는 뉴스인지 아닌지 판단 하기  : 키워드 기반\n",
    "def title_classifier(title):\n",
    "    # headline  = data[2]\n",
    "    t_check = title_key_check(title)\n",
    "    # h_check= news_close_time(headline )\n",
    "    classifier = 0\n",
    "    if t_check >= 1:\n",
    "        classifier = 1  # 제목에 시황 뉴스를 암시하는 'stock market today' 가 있다면,\n",
    "    # 제목엔 없지만 헤드라인에 관련 키워드가 한개이상이라면,\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# 규칙 제목 한정 제목 분류\n",
    "def title_key_check(sentence):\n",
    "    title_key = ['smtody', 'cloftrade', 'wallst', 'pts', 'usastocks', 'liveupdate', 'usatocksturn',\n",
    "                 'marketshows', 'ustocksdrop', 'ustockshigh', 'stockmarketcloses', 'marketswrap', 'stockmarket',\n",
    "                 'stocksendlower']\n",
    "\n",
    "    #  'markets', 'market']  # 'futures',\n",
    "    un_key = ['emerging', 'EMERGING', 'global', 'GLOBAL']\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"stock market today\", title_key[0])\n",
    "    sentence = sentence.replace(\"stock market today:\", title_key[0])\n",
    "    sentence = sentence.replace(\"close of trade\", title_key[1])\n",
    "    sentence = sentence.replace(\"us stocks-wall\", title_key[2])\n",
    "    # sentence = sentence.replace(\"wall st\", title_key[2])\n",
    "    sentence = sentence.replace(\"pts;\", title_key[3])\n",
    "    sentence = sentence.replace(\"u.s. stocks\", title_key[4])\n",
    "    sentence = sentence.replace(\"live updates:\", title_key[5])\n",
    "    sentence = sentence.replace(\"us stocks turn\", title_key[6])\n",
    "    sentence = sentence.replace(\"market shows\", title_key[7])\n",
    "    sentence = sentence.replace(\"us stocks drop\", title_key[8])\n",
    "    sentence = sentence.replace(\"us stocks high\", title_key[9])\n",
    "    sentence = sentence.replace(\"stock market closes\", title_key[10])\n",
    "    sentence = sentence.replace(\"markets wrap\", title_key[11])\n",
    "    sentence = sentence.replace(\"stock market\", title_key[12])\n",
    "    sentence = sentence.replace(\"stocks end lower\", title_key[13])\n",
    "\n",
    "    word_count = 0\n",
    "    for t in range(len(title_key)):\n",
    "        if title_key[t] in sentence:\n",
    "            word_count += 1\n",
    "            # print(title_key[t])\n",
    "        # 제거\n",
    "        if title_key[t] in un_key:\n",
    "            word_count = 0\n",
    "            break\n",
    "\n",
    "    return word_count\n",
    "\n",
    "\n",
    "# 야후 파이낸스 전용 전처리\n",
    "def body_p_clean(body_p, press):\n",
    "    # body_p = soup.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for s in range(len(body_p)):\n",
    "        body_text = body_p[s].text\n",
    "        if press == 'Yahoo Finance':\n",
    "            if body_text == '—':\n",
    "                break\n",
    "        if press == 'Bloomberg':\n",
    "            if s == 0:\n",
    "                body_text = bloomberg_match(body_text)\n",
    "            else:\n",
    "                if body_p[s].find('a') is not None:\n",
    "                    continue\n",
    "                more_finding = body_text.find('Most Read')\n",
    "                if more_finding == 0:\n",
    "                    continue\n",
    "                read_finding = body_text.find('Read more:')\n",
    "                if read_finding == 0:\n",
    "                    continue\n",
    "                body_text = bloomberg_end_match(body_text)\n",
    "                if body_text.find('More market') != -1:\n",
    "                    break\n",
    "                if len(body_text) == 0:\n",
    "                    continue\n",
    "                if body_text.find('this week:') != -1:\n",
    "                    break\n",
    "                if body_text.find('Read:') != -1:\n",
    "                    break\n",
    "\n",
    "        # 여기서부터 언론사별 불필요한 패턴 자르기\n",
    "\n",
    "        if press == 'Kiplinger':\n",
    "            if body_text.find('SEE MORE') != -1:\n",
    "                continue\n",
    "            if body_text.find('Sign up') != -1:\n",
    "                continue\n",
    "            if body_text.find('YCharts') != -1:\n",
    "                break\n",
    "\n",
    "        if press == 'Zacks':\n",
    "            # check_tag = body_p[s].find('strong')\n",
    "            # if check_tag != None:\n",
    "            #    continue\n",
    "            zacks_finding = body_text.find('Zacks Investment Research?')\n",
    "\n",
    "            if zacks_finding >= 0:\n",
    "                continue\n",
    "\n",
    "            infull_finding = body_text.find('[In full disclosure,')\n",
    "            if infull_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Yahoo Finance':\n",
    "            check_tag = body_p[s].find('em')\n",
    "            if check_tag != None:\n",
    "                continue\n",
    "            yahoo_click_finding = body_text.find('Click here for')\n",
    "            if yahoo_click_finding >= 0:\n",
    "                continue\n",
    "            yahoo_read_finding = body_text.find('Read the latest')\n",
    "            if yahoo_read_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Benzinga':\n",
    "            see_benzinga_finding = body_text.find('See more')\n",
    "            if see_benzinga_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Insider Monkey':\n",
    "            insider_monkey_c_finding = body_text.find('Click to continue')\n",
    "            if insider_monkey_c_finding >= 0:\n",
    "                break\n",
    "            insider_monkey_s_finding = body_text.find('Suggested Articles:')\n",
    "            if insider_monkey_s_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Wire':\n",
    "            business_w_finding = body_text.find('NOTE TO')\n",
    "            if business_w_finding >= 0:\n",
    "                break\n",
    "            business_c_finding = body_text.find('businesswire.com')\n",
    "            if business_c_finding >= 0:\n",
    "                break\n",
    "\n",
    "            local_date_press = '[A-Z]+\\s[A-Z]+,\\s[A-Z]{1}[a-z]+\\s[0-9]{2},\\s[0-9]{4}[-]{1,2}[(][A-Z]{8}\\s[A-Z]{4}[)][-]{1,2}'\n",
    "            body_text = re.sub(local_date_press, '', body_text)\n",
    "\n",
    "        if press == 'Fortune':\n",
    "            fortune_finding = body_text.find('featured on Fortune.com')\n",
    "            if fortune_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Simply Wall St.':\n",
    "            sw_st_finding = body_text.find('free platform')\n",
    "            if sw_st_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Variety':\n",
    "            variety_finding = body_text.find('Best of Variety')\n",
    "            if variety_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'PR Newswire':\n",
    "            pr_newswire_finding = body_text.find('Trademarks')\n",
    "            if pr_newswire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Skift':\n",
    "            skift_finding = body_text.find('Subscribe to Skift')\n",
    "            if skift_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Poets & Quants':\n",
    "            pq_finding = body_text.find('MISS POLL')\n",
    "            if pq_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'ACCESSWIRE':\n",
    "            accwire_finding = body_text.find('View additional')\n",
    "            if accwire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'InvestorPlace':\n",
    "            inplace_finding = body_text.find('More From InvestorPlace')\n",
    "            if inplace_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'GuruFocus.com':\n",
    "            gf_finding = body_text.find('appeared on GuruFocus')\n",
    "            if gf_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Insider':\n",
    "            binsider_finding = body_text.find('Read the ')\n",
    "            if binsider_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == \"Investor's Business Daily\":\n",
    "            invest_bd_finding = body_text.find('YOU MAY ALSO LIKE:')\n",
    "            if invest_bd_finding >= 0:\n",
    "                break\n",
    "            invest_plz_finding = body_text.find('Please follow')\n",
    "            if invest_plz_finding >= 0:\n",
    "                break\n",
    "            invest_follow_finding = body_text.find('Follow')\n",
    "            if invest_follow_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == \"TheStreet.com\":\n",
    "            thestreet_finding = body_text.find('Updated at')\n",
    "            if thestreet_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if s == 0:  # 첫 빈 공간 고려하기\n",
    "            body += body_text\n",
    "        else:\n",
    "            body = body + ' ' + body_text\n",
    "        body = body.replace('\\n', '')\n",
    "        body = body.replace('\\xa0', ' ')\n",
    "        body = body.strip()\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def body_split(text):\n",
    "    tlink = ''\n",
    "    tokenized_text = sent_tokenize(text)\n",
    "\n",
    "    for d in range(len(tokenized_text)):\n",
    "        tokend = tokenized_text[d]\n",
    "        report_kill = reporter_match(tokend)\n",
    "        report_kill = report_kill.lstrip()\n",
    "        site_kill = investing_match(report_kill)\n",
    "        tokend = site_kill.strip()\n",
    "        if len(tokend) == 0:\n",
    "            continue\n",
    "        if d == 0:\n",
    "            tlink = tokend\n",
    "        else:\n",
    "            tlink = tlink + ' ' + tokend\n",
    "\n",
    "    return tlink\n",
    "\n",
    "\n",
    "def reporter_match(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def reporter_match_and(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return reporter\n",
    "\n",
    "\n",
    "def investing_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_ = 'Investing.com'\n",
    "    spect_ = '[–|-]{1,3}'\n",
    "\n",
    "    site_pattern = re.compile(site_ + ' ' + spect_)\n",
    "    site_name = site_pattern.match(text)\n",
    "    # print(site_name)\n",
    "    if site_name != None:\n",
    "        site_name = site_name.group(0)\n",
    "\n",
    "        text = text.replace(site_name, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_pattern = '(Bloomberg)'\n",
    "    site_name = text.replace(site_pattern, '').lstrip()\n",
    "    spect_pattern = re.compile('[-]+')\n",
    "\n",
    "    doc = spect_pattern.match(site_name)\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = site_name.replace(doc, '').lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_end_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    # text =  '©2022 Bloomberg L.P.'\n",
    "    c_spect = '©[0-9]+\\s[a-zA-Z]+\\s[a-zA-Z]+.[a-zA-Z]+.'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def period_index(body_p):\n",
    "    period_index = ''\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "\n",
    "    for r in range(len(body_p)):\n",
    "        text = body_p[r].text\n",
    "        report_split = reporter_pattern.match(text)\n",
    "        if report_split != None:\n",
    "            rs_ = body_p.index(body_p[r])\n",
    "            period_index = rs_\n",
    "\n",
    "    body_p = body_p[period_index + 1:]\n",
    "\n",
    "    return body_p\n",
    "\n",
    "\n",
    "# 로이터 용 언론사명 본문에서 끄집어서 짜르기\n",
    "def press_local_find(text, press):\n",
    "    # 본문내 불필요한 요일 언론사 이름 지우기\n",
    "    # press = 'Reuters'\n",
    "    spect_p = '(' + press + ')' + ' ' + '-'\n",
    "    dt_press = text.find(spect_p)\n",
    "    text = text[dt_press + len(spect_p):]\n",
    "    text = text.lstrip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220722 로이터 전처리 최신 버전\n",
    "def reuters_preprocess(body_p):\n",
    "    # body_list = []\n",
    "    texts = ''\n",
    "    cnt = 0\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('(Updates with') >= 0:\n",
    "            continue\n",
    "        if text.find('For a Reuters') >= 0:\n",
    "            continue\n",
    "        if text.find('*') >= 0:\n",
    "            continue\n",
    "        reporter_name = reporter_match_and(text)\n",
    "        # print(len(str(reporter_name)),reporter_name )\n",
    "        if len(str(reporter_name)) > 5:\n",
    "            continue\n",
    "        if text == '':\n",
    "            continue\n",
    "        text = reuters_first_del(text)\n",
    "        if text.find('Reporting') >= 0:\n",
    "            text = text.split('(Reporting')\n",
    "            text = text[0]\n",
    "        if cnt == 0:\n",
    "            texts = text\n",
    "        else:\n",
    "            texts = texts + ' ' + text\n",
    "        # print(text)\n",
    "        cnt += 1\n",
    "        # body_list.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def reuters_first_del(text):\n",
    "    text = text.replace('NEW YORK,', '')\n",
    "    text = text.lstrip()\n",
    "    c_spect = '[a-zA-Z]+\\s[0-9]+\\s[^a-zA-Z0-9]{1}[Reuters]+[^a-zA-Z0-9]{1}'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "\n",
    "    text = text.lstrip('-')\n",
    "    text = text.lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220927 인베스팅 본문 오류 해결 함수\n",
    "def conts_clean(conts):\n",
    "    conts_del = ['Position added successfully to: \\n']\n",
    "    conts_full = ''\n",
    "\n",
    "    for c in range(len(conts)):\n",
    "        page_line = conts[c].text\n",
    "        if page_line in conts_del:\n",
    "            continue\n",
    "        page_line = reporter_match(page_line)\n",
    "        page_line = investing_match(page_line)\n",
    "        conts_full += page_line + ' '\n",
    "    conts_full = conts_full.strip()\n",
    "\n",
    "    return conts_full\n",
    "\n",
    "\n",
    "## 그외 기타 전처리 함수\n",
    "# close_time_headline_keywords = ['usastocks', 'usastock', 'djia' , 'nasdaq', 'nyse','gspc','point']\n",
    "\n",
    "def news_close_time(content):\n",
    "    headline_keywords = ['usastocks', 'usastock', 'djia', 'nasdaq', 'nyse', 'gspc', 'point']\n",
    "    checking_text = market_preprocessing(content)\n",
    "    word_count = 0\n",
    "    ## 내용 검사\n",
    "    for i in range(len(checking_text)):\n",
    "        word = checking_text[i]\n",
    "        if word in headline_keywords:\n",
    "            word_count += 1\n",
    "\n",
    "    return word_count  # 1 이상이면 맞다는 것.\n",
    "\n",
    "\n",
    "def market_preprocessing(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"standard & poor's 500\", 'gspc')\n",
    "    sentence = sentence.replace('s&p 500', 'gspc')\n",
    "    sentence = sentence.replace('s&p', 'gspc')\n",
    "    sentence = sentence.replace('u.s. stock', 'usastock')\n",
    "    sentence = sentence.replace('dow jones industrial average', 'djia')\n",
    "    sentence = sentence.replace('%', ' ')\n",
    "    sentence = sentence.replace('%,', ' ')\n",
    "    word_tokens = word_tokenize(sentence)  # 월스트리트저널 기반 분해 -> s&p 500 을 산산조각냄\n",
    "    result = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            result.append(w)\n",
    "    lemm = WordNetLemmatizer()\n",
    "    wpos = []\n",
    "    # 이건 필요없을 듯\n",
    "    for w in result:\n",
    "        wn = lemm.lemmatize(w, pos='n')\n",
    "        wpos.append(wn)\n",
    "    tagged_list = pos_tag(wpos)\n",
    "\n",
    "    sw = ['.', ',', ':', 'CD', '(', ')', '``', \"''\", 'POS']\n",
    "    removed_list = []\n",
    "    for word in tagged_list:\n",
    "        if word[1] not in sw:\n",
    "            removed_list.append(word[0])\n",
    "    removed_list = set(removed_list)\n",
    "    removed_list = list(removed_list)\n",
    "\n",
    "    return list(removed_list)\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def invest_context_end_kill(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    con = 'continue reading'\n",
    "    e_num_pattern = re.compile('^©[0-9]{1,4}\\s[a-zA-Z]+')  # \\s[a-zA-Z]+\n",
    "\n",
    "    if cp[-1].text.lower().find(con) >= 0:\n",
    "        del cp[-1]\n",
    "    if cp[0].text == '\\n':\n",
    "        del cp[0]\n",
    "\n",
    "    for i in range(len(cp)):\n",
    "        m = e_num_pattern.match(cp[i].text)\n",
    "        if m != None:\n",
    "            pm = m.group(0)\n",
    "            if pm.find(pm) >= 0:\n",
    "                del cp[i]\n",
    "                break\n",
    "\n",
    "    return cp\n",
    "\n",
    "\n",
    "###### 데이터 불러오는 것 ###########################################\n",
    "\n",
    "def korean_like_data_split(df):\n",
    "    used_dk = df[df['사용 여부'] == 1]\n",
    "    df_sample = used_dk.loc[:, ['종목코드', '종목명', '종목 뉴스 주소']]\n",
    "    category_ids = []\n",
    "    category_news = []\n",
    "    category_names = []\n",
    "\n",
    "    for s in range(len(df_sample)):\n",
    "        stock_news = df_sample['종목 뉴스 주소'].iloc[s].split('/')\n",
    "        stock_news = stock_news[-1]\n",
    "        stock_ids = df_sample['종목코드'].iloc[s]\n",
    "        stock_name = df_sample['종목명'].iloc[s]\n",
    "        category_ids.append(stock_ids)\n",
    "        category_news.append(stock_news)\n",
    "        category_names.append(stock_name)\n",
    "\n",
    "    return category_ids, category_news, category_names\n",
    "\n",
    "\n",
    "## 아마존 DB 티커 불러올때, 핀비즈에서 검색 안되는 리스트\n",
    "def notfind_symbol_load():\n",
    "    unfind_df = pd.read_excel('./data/notfind_symbol.xlsx')\n",
    "    unfind_df_list = unfind_df.values.tolist()\n",
    "    un_symbol_list = []\n",
    "\n",
    "    for u in range(len(unfind_df_list)):\n",
    "        un_symbol = unfind_df_list[u][0]\n",
    "        un_symbol_list.append(un_symbol)\n",
    "    return un_symbol_list\n",
    "\n",
    "\n",
    "##### DB 함수들 ################################################\n",
    "\n",
    "\n",
    "## DB 저장용 -> 추후 오라클 DB화\n",
    "# 시황 뉴스 데이터 추가 - Question table\n",
    "def market_news_insert(db, url, title, body, w_date, n_date, press, m_time):\n",
    "    category = 'market_news'\n",
    "    cur = db.cursor()\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, market_time) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, m_time))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        pass\n",
    "\n",
    "\n",
    "# 종목/ETF 뉴스 데이터 추가 - Question table\n",
    "def stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker):\n",
    "    cur = db.cursor()  # ISIN\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, symbol) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, ticker))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        print(msg)\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a3a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
