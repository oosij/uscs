{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bac1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import load !\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import openpyxl\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime as dtime\n",
    "from datetime import timedelta\n",
    "from dateutil.parser import parse\n",
    "from time import strptime\n",
    "import pytz\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import time\n",
    "import pymysql\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f41bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 DB TEST 접속\n",
    "db = pymysql.connect(host='127.0.0.1',  port = 3306 , user = 'root',\n",
    "                           password = \"1234\", db = 'jisoo', charset = 'utf8')\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d0578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db93776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "WorkingTime: 1129.1649947166443 sec\n",
      "WorkingTime: 1129.1649947166443 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "result = barchat_news_crawler_main(db)\n",
    "print(\"WorkingTime: {} sec\".format(time.time() - start))  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f1bad66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.barchart.com/story/news/12035815/crude-prices-higher-as-opec-says-it-may-consider-deeper-production-cuts',\n",
       " 'Crude Prices Higher as OPEC+ Says It May Consider Deeper Production Cuts',\n",
       " 'Jan WTI crude oil (CLF23) on Monday closed up +0.96 (+1.26%), and Jan RBOB gasoline (RBF23) closed down -0.06 (-0.01%). \\xa0Crude oil and gasoline prices Monday recovered from early losses and settled mixed. \\xa0Crude oil recovered from an 11-month nearest-futures low and rose moderately after OPEC+ delegates said the group may consider deeper crude production supply cuts when they meet this Sunday if it is required \"to balance supply and demand.\"Crude prices Monday initially slumped on energy demand concerns in China. \\xa0New Covid infections in China continue to surge to new records, which may prompt the government to expand lockdowns and pandemic restrictions that curb economic activity, travel, and energy demand.Chinese energy demand concerns continue to undercut crude prices. \\xa0China reported a record 38,808 new Covid infections on Sunday, which may lead to more pandemic lockdowns that curb economic growth and energy demand. \\xa0Analytics firm Kpler said Chinese oil demand could average 15.11 million bpd in Q4, down -4.5% from 15.82 million bpd a year ago.Another bearish factor for crude was the action by the Biden administration on Saturday to grant Chevron a license to resume oil production in Venezuela after U.S. sanctions halted all drilling activities there three years ago. \\xa0The sanctions were eased after Norwegian mediators announced the restart of talks between Venezuelan President Maduro and opposition political parties, a key condition for easing sanctions.Oil prices are seeing support ahead of a partial ban on Russian oil beginning December 5. \\xa0Europe is planning to ban the import of Russian seaborne oil beginning December 5. \\xa0Meanwhile, the markets are waiting for details on the G-7\\'s plan for a Russian oil price cap. \\xa0The price cap seeks to curb Russian oil sales by banning G-7 companies from providing shipping and related services unless that oil is sold below the cap price. \\xa0The price-cap embargo should support global oil prices since it is likely to crimp Russian oil exports and reduce the supply of world oil.In a bearish factor, Vortexa reported Monday that the amount of crude stored on tankers that have been stationary for at least a week rose +2.2% w/w to 103.13 million bbls in the week ended November 25.OPEC+ on October 5 agreed to cut its collective output by -2.0 million bpd for November and December, a bigger cut than expectations of -1.0 million bpd. \\xa0Saudi Arabia\\'s energy minister said the real-world impact of the crude production cuts would likely be around 1 million to 1.1 million bpd from November since some members are already pumping well below their quotas. \\xa0OPEC crude production in October rose +30,000 bpd to a 2-1/2 year high of 29.98 million bpd. \\xa0Last Wednesday\\'s EIA report showed that (1) U.S. crude oil inventories as of November 18 were -5.2% below the seasonal 5-year average, (2) gasoline inventories were -3.8% below the seasonal 5-year average, and (3) distillate inventories were -13.1% below the 5-year seasonal average. \\xa0U.S. crude oil production in the week ended November 18 was unchanged w/w at 12.1 million bpd, which is only -1.0 million bpd (-7.6%) below the Feb-2020 record-high of 13.1 million bpd.Baker Hughes reported last Wednesday that active U.S. oil rigs in the week ended November 25 rose by +4 rigs to a 2-1/2 year high of 627 rigs. \\xa0U.S. active oil rigs have more than tripled from the 17-year low of 172 rigs seen in Aug 2020, signaling an increase in U.S. crude oil production capacity.\\xa0More Crude Oil News from Barchart\\n\\n                    Crude Prices Fall on Demand Concerns as Pandemic Worsens in China\\n                \\n                    Crude Prices Weighed Down on Chinese Energy Demand Concerns\\n                \\n                    Crude Slightly Lower on Surging Chinese Covid Cases\\n                \\n                    Crude Sharply Lower on EU Plans to Curb Russian Crude Oil Sales\\n                \\nOn the date of publication, Rich Asplund did not have (either directly or indirectly) positions in any of the securities mentioned in this article. All information and data in this article is solely for informational purposes.',\n",
       " datetime.datetime(2022, 11, 28, 20, 35, 3),\n",
       " datetime.datetime(2022, 11, 29, 13, 37, 14, 111328),\n",
       " 'Barchart']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(result), len(result))\n",
    "result[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd727195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.barchart.com/story/news/12049094/livestock-report',\n",
       " 'Livestock Report',\n",
       " 'Walsh Trading Daily InsightsCommentaryFebruary Lean Hogs collapsed as Chinese protests over the weekend created a firestorm of downside action in the Hog market. The protests and most likely severe crackdowns left the market worried about Chinese demand for pork. The cash market’s continued weakness also pushed February Hogs lower as the December contract is no longer the lead contract and February failed against resistance at the 100-DMA creating negative sentiment. Price opened lower, with the high coming in at 88.15 and then crashing and making the low at 84.575. It settled near the low at 84.75. Settlement was below support at 85.325 and keeps the pressure on Hogs, in my opinion. Continued weakness could see price test support at the 50-DMA now at 83.975. Support then comes in at 83.325. Re-evaluation of the Chines protests could see price hold support and test resistance at the 21-DMA at 86.325. Resistance then comes in at 87.10.The Pork Cutout Index decreased and is at 90.97 as of 11/25/2022.\\xa0The Lean Hog Index decreased and is at 85.563 as of 11/24/2022.Estimated Slaughter for Monday is 493,000 which is above last week’s 488,000 and last year’s 483,000. Last week’s slaughter was revised lower to 2,213,000.For those interested I hold a weekly grain (with Sean Lusk) and livestock webinar on Thursdays (except holiday weeks) and our next webinar will be on Thursday, December 01, 2022 at 3:00 pm. It is free for anyone who wants to sign up and the link for sign up is below. If you cannot attend live a recording will be sent to your email upon completion of the webinar.Sign Up Now**Call me for a free consultation for a marketing plan regarding your livestock needs.**Ben DiCostanzoSenior Market StrategistWalsh Trading, Inc.Direct: 312.957.4163888.391.7894Fax: 312.256.0109bdicostanzo@walshtrading.comwww.walshtrading.comWalsh Trading, Inc. is registered as a Guaranteed Introducing Broker with the Commodity Futures Trading Commission and an NFA Member.\\u200btested support at theFutures and options trading involves substantial risk and is not suitable for all investors. Therefore, individuals should carefully consider their financial condition in deciding whether to trade. Option traders should be aware that the exercise of a long option will result in a futures position. The valuation of futures and options may fluctuate, and as a result, clients may lose more than their original investment. The information contained on this site is the opinion of the writer or was obtained from sources cited within the commentary. The impact on market prices due to seasonal or market cycles and current news events may already be reflected in market prices. PAST PERFORMANCE IS NOT NECESSARILY INDICATIVE OF FUTURE RESULTS.\\u200bAll information, communications, publications, and reports, including this specific material, used and distributed by Walsh Trading, Inc. (“WTI”) shall not be construed as a solicitation for entering into a derivatives transaction. WTI does not distribute research reports, employ research analysts, or maintain a research department as defined in CFTC Regulation 1.71.\\nOn the date of publication, Ben DiCostanzo did not have (either directly or indirectly) positions in any of the securities mentioned in this article. All information and data in this article is solely for informational purposes.',\n",
       " datetime.datetime(2022, 11, 29, 0, 22, 9),\n",
       " datetime.datetime(2022, 11, 29, 13, 24, 24, 588139),\n",
       " 'Walsh Trading']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531dac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "WorkingTime: 1126.5186305046082 sec\n",
      "220\n",
      "Application of Slag in Steel, Other Industries\n",
      "WorkingTime: 1130.8855020999908 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 221129 IP 밴 테스트 \n",
    "\n",
    "#barchat_news_crawler_main(db)\n",
    "\n",
    "# 60분에 한번씩 함수 실행\n",
    "schedule.every(1).hour.do(barchat_news_crawler_main, db)\n",
    "\n",
    "\n",
    "# 무한 반복\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ab59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0b7fdc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZW*0</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZC*0</td>\n",
       "      <td>Corn</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZS*0</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZM*0</td>\n",
       "      <td>Soybean Meal</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZL*0</td>\n",
       "      <td>Soybean Oil</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ZO*0</td>\n",
       "      <td>Oats</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ZR*0</td>\n",
       "      <td>Rough Rice</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KE*0</td>\n",
       "      <td>Hard Red Wheat</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MW*0</td>\n",
       "      <td>Spring Wheat</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RS*0</td>\n",
       "      <td>Canola</td>\n",
       "      <td>Grains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CL*0</td>\n",
       "      <td>Crude Oil WTI</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HO*0</td>\n",
       "      <td>ULSD NY Harbor</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RB*0</td>\n",
       "      <td>Gasoline RBOB</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NG*0</td>\n",
       "      <td>Natural Gas</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>QA*0</td>\n",
       "      <td>Crude Oil Brent</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FL*0</td>\n",
       "      <td>Ethanol</td>\n",
       "      <td>Energies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GC*0</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SI*0</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HG*0</td>\n",
       "      <td>High Grade Copper</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PL*0</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PA*0</td>\n",
       "      <td>Palladium</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AL*0</td>\n",
       "      <td>Aluminum</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Symbol               Name     Class\n",
       "0    ZW*0              Wheat    Grains\n",
       "1    ZC*0               Corn    Grains\n",
       "2    ZS*0           Soybeans    Grains\n",
       "3    ZM*0       Soybean Meal    Grains\n",
       "4    ZL*0        Soybean Oil    Grains\n",
       "5    ZO*0               Oats    Grains\n",
       "6    ZR*0         Rough Rice    Grains\n",
       "7    KE*0     Hard Red Wheat    Grains\n",
       "8    MW*0       Spring Wheat    Grains\n",
       "9    RS*0             Canola    Grains\n",
       "10   CL*0      Crude Oil WTI  Energies\n",
       "11   HO*0     ULSD NY Harbor  Energies\n",
       "12   RB*0      Gasoline RBOB  Energies\n",
       "13   NG*0        Natural Gas  Energies\n",
       "14   QA*0    Crude Oil Brent  Energies\n",
       "15   FL*0            Ethanol  Energies\n",
       "16   GC*0               Gold    Metals\n",
       "17   SI*0             Silver    Metals\n",
       "18   HG*0  High Grade Copper    Metals\n",
       "19   PL*0           Platinum    Metals\n",
       "20   PA*0          Palladium    Metals\n",
       "21   AL*0           Aluminum    Metals"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barchart_commdities_table_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b4e2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbols_load_test():\n",
    "    etf_path = './data/ETF_top100_sample.xlsx'\n",
    "    df = pd.read_excel(etf_path)\n",
    "\n",
    "    symbol_list = []\n",
    "\n",
    "    for d in range(len(df['Symbol'])):\n",
    "        ticker = df['Symbol'].iloc[d]\n",
    "        symbol_list.append([ticker, '', ''])\n",
    "    etf_df = pd.DataFrame(symbol_list, columns=['Symbol', 'Name', 'Class'])\n",
    "        \n",
    "    return etf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 임의 etf url_list 생성기 \n",
    "etf_symbol_list = symbols_load()\n",
    "etf_url = 'https://www.barchart.com/etfs-funds/quotes/'+etf_symbol+'/news'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb03b75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class\n",
       "0       \n",
       "1       \n",
       "2       \n",
       "3       \n",
       "4       \n",
       "..   ...\n",
       "95      \n",
       "96      \n",
       "97      \n",
       "98      \n",
       "99      \n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_df = symbols_load_test()\n",
    "s_df[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e6879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a3736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a2fc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ip 밴 테스트 겸 돌려보기 \n",
    "\n",
    "def barchat_news_crawler_main(db):\n",
    "    start = time.time()\n",
    "    commdities_df = barchart_commdities_table_info()\n",
    "    news_list = barchart_news_crawler(commdities_df) # 실질 running\n",
    "    print(len(news_list))\n",
    "    print(news_list[-1][1])\n",
    "    print(\"WorkingTime: {} sec\".format(time.time() - start))  # 현재시각 - 시작시간 = 실행 시간\n",
    "    \n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97ad434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 함수\n",
    "def barchart_commdities_table_info():\n",
    "    url = 'https://www.barchart.com/futures/highs-lows/all'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    s_info_list = []\n",
    "\n",
    "    category_nasdaq = ['Grains', 'Energies', 'Metals']\n",
    "\n",
    "    commodity_find =  soup.find('div', {'class':'commodity-select'})\n",
    "\n",
    "    for c in range(len(commodity_find)):\n",
    "        category = commodity_find.find_all('optgroup')[c]\n",
    "        category_name = category['label']\n",
    "        if category_name == '--Grains and Oilseeds--':\n",
    "            category_name = category_nasdaq[0]\n",
    "        if category_name == '--Energies--':\n",
    "            category_name = category_nasdaq[1]\n",
    "        if category_name == '--Metals--':\n",
    "            category_name = category_nasdaq[2]\n",
    "        symbol_info = category.find_all('option')\n",
    "    \n",
    "        for s in range(len(symbol_info)):\n",
    "            symbol = symbol_info[s]['value'].split('/')[-2]  # *0 \n",
    "            name = symbol_info[s].text\n",
    "            s_info = [symbol, name, category_name]\n",
    "            s_info_list.append(s_info)\n",
    "        \n",
    "    commdities_df = pd.DataFrame(s_info_list, columns = ['Symbol','Name', 'Class'])\n",
    "    return commdities_df\n",
    "\n",
    "def barchart_news_crawler(commdities_df):\n",
    "    news_list = []\n",
    "\n",
    "    for i in range(len(commdities_df)):\n",
    "        symbol = commdities_df['Symbol'][i]\n",
    "        url = 'https://www.barchart.com/futures/quotes/'+symbol+'/news'\n",
    "        c_name = commdities_df['Symbol'][i]\n",
    "        purl_list = barchart_news_crawling(url, headers)\n",
    "    \n",
    "        for u in range(len(purl_list)):\n",
    "            press =  purl_list[u][0]\n",
    "            turl = purl_list[u][1]\n",
    "            news_info = barchart_news_inlink_parser(turl, headers, press)\n",
    "            news_list.append(news_info)\n",
    "            time.sleep(2)\n",
    "    return news_list\n",
    "\n",
    "\n",
    "def barchart_news_crawling(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    surl = soup.find_all('div', {'class':'story clearfix'})\n",
    "    purl_list = []\n",
    "    \n",
    "    for s in range(len(surl)):\n",
    "        kurl = surl[s]\n",
    "        link_url =  kurl.find('a')['href']\n",
    "        press = kurl.find('span').text\n",
    "        press_split = press.split('-')\n",
    "        press = press_split[0].strip()\n",
    "        purl_list.append([press, link_url])\n",
    "        time.sleep(2)\n",
    "    return purl_list\n",
    "\n",
    "def barchart_news_inlink_parser(url, headers, press):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    now = dtime.now()\n",
    "    dt = soup.find(\"meta\", itemprop=\"datePublished\")\n",
    "    dt_str = dt['content']\n",
    "    datetime_format =  '%m/%d/%y %H:%M:%S'\n",
    "    dt = dtime.strptime(dt_str, datetime_format)\n",
    "\n",
    "    div_content = soup.find('div',{'class':'bc-news-item'})\n",
    "    str_dict = div_content.find('data-media-overlay-news')['data-news-item']\n",
    "    json_content = json.loads(str_dict)\n",
    "    clean_text = json_content['content']\n",
    "    body  = re.sub('(<([^>]+)>)', '', clean_text)\n",
    "\n",
    "    news_data = [url, title, body, dt, now, press]\n",
    "    \n",
    "    return news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f377f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################### 시장 시간대 구역 ############################\n",
    "# 시간 관련 라이브러리에서 종종 1자리수 숫자에 0 안붙이는거 해결...\n",
    "def date_zero_padding(m):\n",
    "    if m < 10:\n",
    "        m = '0' + str(m)\n",
    "    else:\n",
    "        m = str(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "# 노멀한 메타 데이터에서 시간 추출\n",
    "def yahoo_market_time(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def yahoo_market_search_time(url, market_time):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if market_time == 0:\n",
    "        market_time = ['00:10:00', '00:20:00']\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "\n",
    "    start_hms = market_time[0]\n",
    "    end_hms = market_time[1]\n",
    "    # start_hms = '16:00:00'\n",
    "    # end_hms = '16:30:00'\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "    ymd_date = str(usa_est.year) + '-' + str(usa_est.month) + '-' + str(usa_est.day)\n",
    "\n",
    "    limit_start = ymd_date + ' ' + start_hms\n",
    "    limit_end = ymd_date + ' ' + end_hms\n",
    "\n",
    "    limit_start = dtime.strptime(limit_start, '%Y-%m-%d %H:%M:%S')\n",
    "    limit_end = dtime.strptime(limit_end, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return date_time, limit_start, limit_end\n",
    "\n",
    "\n",
    "###################핀 비즈 구역 ############################\n",
    "\n",
    "def finviz_news_crawler(db, category, ticker_list, ymd_date):\n",
    "    news_info_list = []\n",
    "    error_cnt = 0\n",
    "\n",
    "    for t in range(len(ticker_list)):\n",
    "        ticker = ticker_list[t][0]\n",
    "        # ticker, s_name, s_code = ticker_list[1][0], ticker_list[1][1], ticker_list[1][2]\n",
    "        try:\n",
    "            fin_dic = finviz_news_crawling(ticker, ymd_date)\n",
    "        except:\n",
    "            error_cnt += 1\n",
    "            print('[' + str(error_cnt) + ']', ticker)\n",
    "\n",
    "            continue\n",
    "        if len(fin_dic) == 0:\n",
    "            continue  # 오늘 데이터 없으면 다음 티커 를 탐색\n",
    "        p_list = fin_dic[ymd_date]\n",
    "\n",
    "        for u in range(len(p_list)):\n",
    "            url = p_list[u][-1]\n",
    "            news_checking = finviz_news_check(db, url, ticker)\n",
    "            if news_checking == 1:\n",
    "                continue  # 중복된 url + 종목의 뉴스가 있다면 스킵\n",
    "\n",
    "            try:\n",
    "                news_info = fyahoo_stock_news_inlink_extract(url, headers)\n",
    "                # [news_info] ... + 분류명 (종목 뉴스) , 티커, 종목명, 종목 코드\n",
    "                # [news_info] ... + categroy ('stock_news') , ticker, stock_name, stock_code\n",
    "                # category column 변경 필요 ->  market_time 으로, 기존의 시황 뉴스는 category = 'market_news' 로\n",
    "\n",
    "                news_info_add = news_info + [category] + [ticker]\n",
    "                url, title, body = news_info_add[0], news_info_add[1], news_info_add[2]\n",
    "                w_date, n_date, press = news_info_add[3], news_info_add[4], news_info_add[5]\n",
    "                category, ticker = news_info_add[6], news_info_add[7]\n",
    "\n",
    "                news_input = stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker)\n",
    "                nadd_info = [url, title, body, w_date, n_date, press, category, ticker]\n",
    "                # 나중엔 삭제 똔느 스킵\n",
    "                news_info_list.append(nadd_info)\n",
    "                # print(url,nadd_info[6:])  # 확인용\n",
    "\n",
    "                ## db 저장하는 함수 필요 : 최근 글 중복 처리하는 방법도 필요\n",
    "\n",
    "            except:\n",
    "                print('Abnormal detected :', url)\n",
    "                continue\n",
    "            time.sleep(2)\n",
    "        # print(ticker,len(news_info_list))\n",
    "    print(error_cnt)\n",
    "    return news_info_list\n",
    "\n",
    "\n",
    "def finviz_news_crawling(name, ymd_date):\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks', 'The Telegraph',\n",
    "                    'Insider Monkey',\n",
    "                    'Benzinga', 'Simply Wall St.', 'Business Wire', 'The Independent', 'Fortune', 'CoinDesk', 'Variety',\n",
    "                    'PR Newswire', 'WWD', 'Skift', 'LA Times', 'The Guardian', 'Poets & Quants', 'GuruFocus.com']\n",
    "\n",
    "    url = \"https://finviz.com/quote.ashx?t=\" + name + \"&p=d\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    date_find = soup.find_all('table', {'class': 'fullview-news-outer'})\n",
    "    date_finding = date_find[0].find_all('td', {'align': 'right'})\n",
    "\n",
    "    news_link = date_find[0].find_all('div', {'class': 'news-link-left'})\n",
    "    news_press = date_find[0].find_all('div', {'class': 'news-link-right'})\n",
    "\n",
    "    purl_list = []\n",
    "\n",
    "    date_dic = {}  # 만약을 위해  view에 보이는 날짜 전부를 수집, 평소엔 가장 상단의 키만을 사용 -> 다음 키 생성시 break\n",
    "\n",
    "    for i in range(len(news_link)):\n",
    "        press_name = news_press[i].find('span').text.lstrip()\n",
    "        press_title = news_link[i].text\n",
    "        press_url = news_link[i].find('a')['href']\n",
    "\n",
    "        date_time = date_finding[i].text\n",
    "        date_split_time = date_time.split(' ')\n",
    "        if len(date_split_time) >= 2:\n",
    "            ymdate = date_split_time[0].split('-')\n",
    "            m, d, y = ymdate[0], ymdate[1], ymdate[2]\n",
    "            m = strptime(m, '%b').tm_mon\n",
    "            if m < 10:\n",
    "                m = '0' + str(m)\n",
    "            else:\n",
    "                m = str(m)\n",
    "            ymd_str = '20' + y + '-' + m + '-' + d  # dict 키 용\n",
    "            dtime_pro = date_split_time[1]\n",
    "\n",
    "            if ymd_str != ymd_date:\n",
    "                break  # 오늘 아니면 탈출\n",
    "\n",
    "            date_dic[ymd_str] = []\n",
    "\n",
    "            if len(date_dic) > 2:\n",
    "                del date_dic[ymd_str]\n",
    "                break  # 딱 하루치만 보기\n",
    "\n",
    "        else:\n",
    "            dtime_pro = date_split_time[0]\n",
    "\n",
    "        if press_url.find('finance.yahoo') == -1:\n",
    "            continue\n",
    "        if press_name not in c_press_list:\n",
    "            continue\n",
    "\n",
    "        date_dic[ymd_str].append([dtime_pro, press_name, press_title, press_url])\n",
    "    return date_dic\n",
    "\n",
    "\n",
    "def finviz_news_check(db, url, ticker):\n",
    "    # db에 url 가 있다면 break! ]\n",
    "    checking_sql = \"SELECT * FROM US_Stock_Market_News WHERE url = '\" + url + \"' and symbol ='\" + ticker + \"'\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    if len(result) >= 1:\n",
    "        result = 1  # 1이면 contine\n",
    "    else:\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "\n",
    "def stock_info_load(db, select_ticker):\n",
    "    checking_sql = \"SELECT symbol, name, ISIN FROM Company_Information_Data\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    df_db = pd.DataFrame(result, columns=['symbol', 'name', 'ISIN'])\n",
    "\n",
    "    target_sname = []\n",
    "    if select_ticker != 0:\n",
    "        # 선택한 것만 하는 것\n",
    "        for s in range(len(select_ticker)):\n",
    "            ticker = select_ticker[s]\n",
    "            stock_info = df_db[df_db['symbol'] == ticker]\n",
    "            list_con = stock_info.values.tolist()\n",
    "            target_sname.append(list_con[0])\n",
    "\n",
    "        df = pd.DataFrame(target_sname, columns=['symbol', 'name', 'ISIN'])\n",
    "    else:\n",
    "        df = df_db\n",
    "    ticker_list = []\n",
    "\n",
    "    for d in range(len(df)):\n",
    "        ticker = df['symbol'].iloc[d]\n",
    "        s_name = df['name'].iloc[d]\n",
    "        s_code = df['ISIN'].iloc[d]\n",
    "        ticker_list.append([ticker, s_name, s_code])\n",
    "\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "###################야후 파이낸스 구역 ############################\n",
    "\n",
    "def yahoo_crawler(db, mtime):\n",
    "    ## 미국 주식 거래시간\n",
    "    us_market_time_dic = {'BMO(UST)': ['07:30:00', '09:30:00'], 'AMO(UST)': ['10:30:00', '11:30:00'],\n",
    "                          'AMC(UST)': ['17:00:00', '18:30:00'], 'BMO(DST)': ['06:30:00', '09:00:00'],\n",
    "                          'AMO(DST)': ['09:30:00', '10:30:00'], 'AMC(DST)': ['16:00:00', '17:30:00']}\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks',\n",
    "                    'Fortune' 'LA Times', 'Kiplinger', 'Business Insider', 'USA TODAY', 'Insider']\n",
    "\n",
    "    o_press_list = [\"Investor's  Daily\", 'TheStreet.com']  # 아웃링크 뉴스 언론사\n",
    "    c_press_list = c_press_list + o_press_list\n",
    "    # c_press_list = ['Bloomberg', 'Reuters', 'Kiplinger', 'Business Insider', 'Yahoo Finance', 'USA TODAY', 'LA Times']\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    url_list = yahoo_crawling(db)\n",
    "    market_time = us_market_time_dic[mtime]\n",
    "    # c = category\n",
    "    # 임시 리턴\n",
    "    pre_urls = []\n",
    "    data_list = []\n",
    "    error_list = []\n",
    "    for u in range(len(url_list)):\n",
    "        source = url_list[u][0]\n",
    "        w_press = url_list[u][1]\n",
    "        w_press = w_press.replace('News', '').lstrip()\n",
    "        pre_urls.append([w_press, source])\n",
    "        if w_press in c_press_list:  # 검증한 언론사들이 포함되어 있다면,\n",
    "            # 시간대 좁히기\n",
    "            date_time, limit_start, limit_end = yahoo_market_search_time(source, market_time)\n",
    "            if date_time >= limit_start and date_time < limit_end:  # 시황 마감후 1시 30분 까지가 아니라면\n",
    "                try:\n",
    "                    news_parsing = fyahoo_stock_news_inlink_extract(source, headers)\n",
    "                except:\n",
    "                    error_news = [w_press, source]\n",
    "                    print('error :', error_news)\n",
    "                    error_list.append(error_news)\n",
    "                    continue\n",
    "                title, body = news_parsing[1], news_parsing[2]\n",
    "                # 시황 뉴스의 경우, 시간대 체크 -> 범용으로 쓰이기 위해 따로 둠 v1 -> v2\n",
    "                if title_classifier(title) == 0:\n",
    "                    continue\n",
    "                w_date, n_date, press = news_parsing[3], news_parsing[4], news_parsing[5]\n",
    "                in_data = [source, title, body, w_date, n_date, press, mtime]\n",
    "                # print(in_data)\n",
    "                data_list.append(in_data)\n",
    "                market_news_insert(db, source, title, body, w_date, n_date, press, mtime)\n",
    "\n",
    "    return pre_urls, data_list, error_list\n",
    "\n",
    "\n",
    "## 220614-15 임시 수집 타이밍  :  야후 파이낸스\n",
    "def yahoo_crawling(db):\n",
    "    url = 'https://finance.yahoo.com/topic/stock-market-news/'\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # chrome_options = webdriver.ChromeOptions()\n",
    "    # chrome_options.add_argument('--headless')\n",
    "    # chrome_options.add_argument('--no-sandbox')\n",
    "    # chrome_options.add_argument(\"--single-process\")\n",
    "    # chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    path = 'chromedriver_107.exe'\n",
    "    driver = webdriver.Chrome(path)  # , chrome_options=chrome_options)\n",
    "    count = 200  # 페이지 한번에 보여줄수 있는양\n",
    "    url_list = []\n",
    "    driver.get(url)\n",
    "    # 스크롤 전 높이\n",
    "    before_h = driver.execute_script(\"return window.scrollY\")\n",
    "    # 무한 스크롤\n",
    "    while True:\n",
    "        # 맨 아래로 스크롤을 내린다.\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_css_selector(\"body\").send_keys(Keys.END)\n",
    "        # 스크롤 사이 페이지 로딩 시간\n",
    "        time.sleep(2)\n",
    "        # 스크롤 후 높이\n",
    "        after_h = driver.execute_script(\"return window.scrollY\")\n",
    "        if after_h == before_h:\n",
    "            break\n",
    "        before_h = after_h\n",
    "    #### 페이지내 url 긁어오기\n",
    "    url_list = []\n",
    "\n",
    "    for i in range(count):  # 페이지 한계로 200 개 정도 확인\n",
    "        press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[2]'\n",
    "        href_i = press_i + '/h3/a'\n",
    "        href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "        press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[1]'\n",
    "            href_i = press_i + '/h3/a'\n",
    "            href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "            press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            continue\n",
    "\n",
    "        href = href_xpath[0].get_attribute('href')\n",
    "        title = href_xpath[0].text\n",
    "        press = press_xpath[0].text\n",
    "        press = press.split('•')\n",
    "        press = press[0].replace('Business', '').lstrip()\n",
    "        url_list.append([href, press])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "################### 파싱 함수구역 ############################\n",
    "\n",
    "# 인링크 방식 뉴스 내용 파싱 : 야후 버전 실상 버전 2이나 구별을 위해 inlink만 사용\n",
    "def fyahoo_stock_news_inlink_extract(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    news_data = []\n",
    "\n",
    "    date_time = yahoo_market_time(url)\n",
    "    now = dtime.now()\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    press = soup.find(\"img\", {'class': \"caas-img\"})\n",
    "    press = press['alt']\n",
    "\n",
    "    conts = soup.find_all('p')  # .text\n",
    "    content = soup.find_all('div', {'class': 'caas-body'})\n",
    "    body_p = content[0]('p')\n",
    "    body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'USA TODAY':\n",
    "        headline = context_preprocessing(conts[1:3])\n",
    "        # body = context_preprocessing(conts)\n",
    "    if press == 'Reuters':\n",
    "        # body_p = period_index(body_p)\n",
    "        body = body_p_clean(body_p, press)\n",
    "        body = body.split('(Reporting')\n",
    "        bodys = body[0]\n",
    "        body = press_local_find(bodys, press)\n",
    "    if press == 'LA Times':\n",
    "        body = body.split('This story originally')\n",
    "        body = body[0]\n",
    "    if press == 'Bloomberg':\n",
    "        body = body.split('(Updates')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Investing.com':\n",
    "        body = body.split('Related Articles')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Zacks':\n",
    "        body = body.split('Want the latest')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == \"Investor's Business Daily\":  # 아웃링크\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        content = soup.find_all('div', {'class': 'single-post-content post-content drop-cap'})\n",
    "        body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'TheStreet.com':\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if out_url.find('realmoney') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-author-rail__body'})\n",
    "            body_p = content[0]('p')\n",
    "        elif out_url.find('aap.') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-back-header__body'})\n",
    "            body_p = content[0]('div')\n",
    "        else:\n",
    "            content = soup.find_all('div', {'class': 'm-detail--body'})\n",
    "            body_p = content[0]('p')\n",
    "        # body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    # body = body_split(body)\n",
    "    # headline_token = sent_tokenize(body)\n",
    "    # if len(headline_token) <= 2:\n",
    "    #    headline = body\n",
    "    # else:\n",
    "    #    headline = headline_token[0] + ' ' + headline_token[1]\n",
    "\n",
    "    inlink_news = [url, title, body, date_time, now, press]\n",
    "\n",
    "    return inlink_news\n",
    "\n",
    "\n",
    "################### 기타 전처리 함수구역 ############################\n",
    "\n",
    "# 만일을 위해서 함수화 했는데.. 생각보다 전처리가 잘 되었기에 ...\n",
    "def clean_text(text):\n",
    "    # cleaned_text = text.replace(\"\\'\", \"'\")\n",
    "    cleaned_text = text.replace('  \\n ', '')\n",
    "    cleaned_text = cleaned_text.replace('(Bloomberg) -- ', '')\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# 날짜를 datetime format으로 변환 하기 위한 함수\n",
    "def dt_format(dt_texts):\n",
    "    a = dt_texts.find('(')\n",
    "    b = dt_texts.find(')')\n",
    "\n",
    "    result = dt_texts[a + 1:b]\n",
    "    result = result.replace(',', '')\n",
    "    dt = result.split(' ')\n",
    "    dt_pm = dt[3].find('PM')  # 있으면 + 없으면 -1 반환\n",
    "\n",
    "    if dt_pm != -1:  ## 오후 타임이면,\n",
    "        pre_dt = dt[3].replace('PM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        if int(pre_dt[0]) == 12:\n",
    "            t, m = pre_dt[0], pre_dt[1]\n",
    "        else:\n",
    "            pm_time = str(int(pre_dt[0]) + 12)\n",
    "            t, m = pm_time, pre_dt[1]\n",
    "\n",
    "    else:  # 오전타임이면\n",
    "        pre_dt = dt[3].replace('AM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        t, m = pre_dt[0], pre_dt[1]\n",
    "\n",
    "    dt_mon = strptime(dt[0], '%b').tm_mon\n",
    "\n",
    "    if dt_mon < 10:\n",
    "        dt_mon = '0' + str(dt_mon)\n",
    "\n",
    "    str_datetimed = dt[2] + '-' + str(dt_mon) + '-' + dt[1] + ' ' + t + \":\" + m + \":\" + '00'\n",
    "    currdate = datetime.datetime.strptime(str_datetimed, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return currdate\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def context_preprocessing(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    for i in range(len(cp)):\n",
    "        texts = texts + cp[i].text + ' '\n",
    "        texts_list.append(cp[i].text)\n",
    "    cleans = clean_text(texts)\n",
    "    return cleans\n",
    "\n",
    "\n",
    "### 뉴스 분류  : 키워드 방식인데, 아무래도 일부 키워드는 해결 불가하기에, 모델 구축 방향으로 가야함, 데이터가 쌓이면\n",
    "\n",
    "# 맞는 뉴스인지 아닌지 판단 하기  : 키워드 기반\n",
    "def title_classifier(title):\n",
    "    # headline  = data[2]\n",
    "    t_check = title_key_check(title)\n",
    "    # h_check= news_close_time(headline )\n",
    "    classifier = 0\n",
    "    if t_check >= 1:\n",
    "        classifier = 1  # 제목에 시황 뉴스를 암시하는 'stock market today' 가 있다면,\n",
    "    # 제목엔 없지만 헤드라인에 관련 키워드가 한개이상이라면,\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# 규칙 제목 한정 제목 분류\n",
    "def title_key_check(sentence):\n",
    "    title_key = ['smtody', 'cloftrade', 'wallst', 'pts', 'usastocks', 'liveupdate', 'usatocksturn',\n",
    "                 'marketshows', 'ustocksdrop', 'ustockshigh', 'stockmarketcloses', 'marketswrap', 'stockmarket',\n",
    "                 'stocksendlower']\n",
    "\n",
    "    #  'markets', 'market']  # 'futures',\n",
    "    un_key = ['emerging', 'EMERGING', 'global', 'GLOBAL']\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"stock market today\", title_key[0])\n",
    "    sentence = sentence.replace(\"stock market today:\", title_key[0])\n",
    "    sentence = sentence.replace(\"close of trade\", title_key[1])\n",
    "    sentence = sentence.replace(\"us stocks-wall\", title_key[2])\n",
    "    # sentence = sentence.replace(\"wall st\", title_key[2])\n",
    "    sentence = sentence.replace(\"pts;\", title_key[3])\n",
    "    sentence = sentence.replace(\"u.s. stocks\", title_key[4])\n",
    "    sentence = sentence.replace(\"live updates:\", title_key[5])\n",
    "    sentence = sentence.replace(\"us stocks turn\", title_key[6])\n",
    "    sentence = sentence.replace(\"market shows\", title_key[7])\n",
    "    sentence = sentence.replace(\"us stocks drop\", title_key[8])\n",
    "    sentence = sentence.replace(\"us stocks high\", title_key[9])\n",
    "    sentence = sentence.replace(\"stock market closes\", title_key[10])\n",
    "    sentence = sentence.replace(\"markets wrap\", title_key[11])\n",
    "    sentence = sentence.replace(\"stock market\", title_key[12])\n",
    "    sentence = sentence.replace(\"stocks end lower\", title_key[13])\n",
    "\n",
    "    word_count = 0\n",
    "    for t in range(len(title_key)):\n",
    "        if title_key[t] in sentence:\n",
    "            word_count += 1\n",
    "            # print(title_key[t])\n",
    "        # 제거\n",
    "        if title_key[t] in un_key:\n",
    "            word_count = 0\n",
    "            break\n",
    "\n",
    "    return word_count\n",
    "\n",
    "\n",
    "# 야후 파이낸스 전용 전처리\n",
    "def body_p_clean(body_p, press):\n",
    "    # body_p = soup.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for s in range(len(body_p)):\n",
    "        body_text = body_p[s].text\n",
    "        if press == 'Yahoo Finance':\n",
    "            if body_text == '—':\n",
    "                break\n",
    "        if press == 'Bloomberg':\n",
    "            if s == 0:\n",
    "                body_text = bloomberg_match(body_text)\n",
    "            else:\n",
    "                if body_p[s].find('a') is not None:\n",
    "                    continue\n",
    "                more_finding = body_text.find('Most Read')\n",
    "                if more_finding == 0:\n",
    "                    continue\n",
    "                read_finding = body_text.find('Read more:')\n",
    "                if read_finding == 0:\n",
    "                    continue\n",
    "                body_text = bloomberg_end_match(body_text)\n",
    "                if body_text.find('More market') != -1:\n",
    "                    break\n",
    "                if len(body_text) == 0:\n",
    "                    continue\n",
    "                if body_text.find('this week:') != -1:\n",
    "                    break\n",
    "                if body_text.find('Read:') != -1:\n",
    "                    break\n",
    "\n",
    "        # 여기서부터 언론사별 불필요한 패턴 자르기\n",
    "\n",
    "        if press == 'Kiplinger':\n",
    "            if body_text.find('SEE MORE') != -1:\n",
    "                continue\n",
    "            if body_text.find('Sign up') != -1:\n",
    "                continue\n",
    "            if body_text.find('YCharts') != -1:\n",
    "                break\n",
    "\n",
    "        if press == 'Zacks':\n",
    "            # check_tag = body_p[s].find('strong')\n",
    "            # if check_tag != None:\n",
    "            #    continue\n",
    "            zacks_finding = body_text.find('Zacks Investment Research?')\n",
    "\n",
    "            if zacks_finding >= 0:\n",
    "                continue\n",
    "\n",
    "            infull_finding = body_text.find('[In full disclosure,')\n",
    "            if infull_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Yahoo Finance':\n",
    "            check_tag = body_p[s].find('em')\n",
    "            if check_tag != None:\n",
    "                continue\n",
    "            yahoo_click_finding = body_text.find('Click here for')\n",
    "            if yahoo_click_finding >= 0:\n",
    "                continue\n",
    "            yahoo_read_finding = body_text.find('Read the latest')\n",
    "            if yahoo_read_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Benzinga':\n",
    "            see_benzinga_finding = body_text.find('See more')\n",
    "            if see_benzinga_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Insider Monkey':\n",
    "            insider_monkey_c_finding = body_text.find('Click to continue')\n",
    "            if insider_monkey_c_finding >= 0:\n",
    "                break\n",
    "            insider_monkey_s_finding = body_text.find('Suggested Articles:')\n",
    "            if insider_monkey_s_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Wire':\n",
    "            business_w_finding = body_text.find('NOTE TO')\n",
    "            if business_w_finding >= 0:\n",
    "                break\n",
    "            business_c_finding = body_text.find('businesswire.com')\n",
    "            if business_c_finding >= 0:\n",
    "                break\n",
    "\n",
    "            local_date_press = '[A-Z]+\\s[A-Z]+,\\s[A-Z]{1}[a-z]+\\s[0-9]{2},\\s[0-9]{4}[-]{1,2}[(][A-Z]{8}\\s[A-Z]{4}[)][-]{1,2}'\n",
    "            body_text = re.sub(local_date_press, '', body_text)\n",
    "\n",
    "        if press == 'Fortune':\n",
    "            fortune_finding = body_text.find('featured on Fortune.com')\n",
    "            if fortune_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Simply Wall St.':\n",
    "            sw_st_finding = body_text.find('free platform')\n",
    "            if sw_st_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Variety':\n",
    "            variety_finding = body_text.find('Best of Variety')\n",
    "            if variety_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'PR Newswire':\n",
    "            pr_newswire_finding = body_text.find('Trademarks')\n",
    "            if pr_newswire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Skift':\n",
    "            skift_finding = body_text.find('Subscribe to Skift')\n",
    "            if skift_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Poets & Quants':\n",
    "            pq_finding = body_text.find('MISS POLL')\n",
    "            if pq_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'ACCESSWIRE':\n",
    "            accwire_finding = body_text.find('View additional')\n",
    "            if accwire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'InvestorPlace':\n",
    "            inplace_finding = body_text.find('More From InvestorPlace')\n",
    "            if inplace_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'GuruFocus.com':\n",
    "            gf_finding = body_text.find('appeared on GuruFocus')\n",
    "            if gf_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Insider':\n",
    "            binsider_finding = body_text.find('Read the ')\n",
    "            if binsider_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == \"Investor's Business Daily\":\n",
    "            invest_bd_finding = body_text.find('YOU MAY ALSO LIKE:')\n",
    "            if invest_bd_finding >= 0:\n",
    "                break\n",
    "            invest_plz_finding = body_text.find('Please follow')\n",
    "            if invest_plz_finding >= 0:\n",
    "                break\n",
    "            invest_follow_finding = body_text.find('Follow')\n",
    "            if invest_follow_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == \"TheStreet.com\":\n",
    "            thestreet_finding = body_text.find('Updated at')\n",
    "            if thestreet_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if s == 0:  # 첫 빈 공간 고려하기\n",
    "            body += body_text\n",
    "        else:\n",
    "            body = body + ' ' + body_text\n",
    "        body = body.replace('\\n', '')\n",
    "        body = body.replace('\\xa0', ' ')\n",
    "        body = body.strip()\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def body_split(text):\n",
    "    tlink = ''\n",
    "    tokenized_text = sent_tokenize(text)\n",
    "\n",
    "    for d in range(len(tokenized_text)):\n",
    "        tokend = tokenized_text[d]\n",
    "        report_kill = reporter_match(tokend)\n",
    "        report_kill = report_kill.lstrip()\n",
    "        site_kill = investing_match(report_kill)\n",
    "        tokend = site_kill.strip()\n",
    "        if len(tokend) == 0:\n",
    "            continue\n",
    "        if d == 0:\n",
    "            tlink = tokend\n",
    "        else:\n",
    "            tlink = tlink + ' ' + tokend\n",
    "\n",
    "    return tlink\n",
    "\n",
    "\n",
    "def reporter_match(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def reporter_match_and(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return reporter\n",
    "\n",
    "\n",
    "def investing_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_ = 'Investing.com'\n",
    "    spect_ = '[–|-]{1,3}'\n",
    "\n",
    "    site_pattern = re.compile(site_ + ' ' + spect_)\n",
    "    site_name = site_pattern.match(text)\n",
    "    # print(site_name)\n",
    "    if site_name != None:\n",
    "        site_name = site_name.group(0)\n",
    "\n",
    "        text = text.replace(site_name, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_pattern = '(Bloomberg)'\n",
    "    site_name = text.replace(site_pattern, '').lstrip()\n",
    "    spect_pattern = re.compile('[-]+')\n",
    "\n",
    "    doc = spect_pattern.match(site_name)\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = site_name.replace(doc, '').lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_end_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    # text =  '©2022 Bloomberg L.P.'\n",
    "    c_spect = '©[0-9]+\\s[a-zA-Z]+\\s[a-zA-Z]+.[a-zA-Z]+.'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def period_index(body_p):\n",
    "    period_index = ''\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "\n",
    "    for r in range(len(body_p)):\n",
    "        text = body_p[r].text\n",
    "        report_split = reporter_pattern.match(text)\n",
    "        if report_split != None:\n",
    "            rs_ = body_p.index(body_p[r])\n",
    "            period_index = rs_\n",
    "\n",
    "    body_p = body_p[period_index + 1:]\n",
    "\n",
    "    return body_p\n",
    "\n",
    "\n",
    "# 로이터 용 언론사명 본문에서 끄집어서 짜르기\n",
    "def press_local_find(text, press):\n",
    "    # 본문내 불필요한 요일 언론사 이름 지우기\n",
    "    # press = 'Reuters'\n",
    "    spect_p = '(' + press + ')' + ' ' + '-'\n",
    "    dt_press = text.find(spect_p)\n",
    "    text = text[dt_press + len(spect_p):]\n",
    "    text = text.lstrip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220722 로이터 전처리 최신 버전\n",
    "def reuters_preprocess(body_p):\n",
    "    # body_list = []\n",
    "    texts = ''\n",
    "    cnt = 0\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('(Updates with') >= 0:\n",
    "            continue\n",
    "        if text.find('For a Reuters') >= 0:\n",
    "            continue\n",
    "        if text.find('*') >= 0:\n",
    "            continue\n",
    "        reporter_name = reporter_match_and(text)\n",
    "        # print(len(str(reporter_name)),reporter_name )\n",
    "        if len(str(reporter_name)) > 5:\n",
    "            continue\n",
    "        if text == '':\n",
    "            continue\n",
    "        text = reuters_first_del(text)\n",
    "        if text.find('Reporting') >= 0:\n",
    "            text = text.split('(Reporting')\n",
    "            text = text[0]\n",
    "        if cnt == 0:\n",
    "            texts = text\n",
    "        else:\n",
    "            texts = texts + ' ' + text\n",
    "        # print(text)\n",
    "        cnt += 1\n",
    "        # body_list.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def reuters_first_del(text):\n",
    "    text = text.replace('NEW YORK,', '')\n",
    "    text = text.lstrip()\n",
    "    c_spect = '[a-zA-Z]+\\s[0-9]+\\s[^a-zA-Z0-9]{1}[Reuters]+[^a-zA-Z0-9]{1}'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "\n",
    "    text = text.lstrip('-')\n",
    "    text = text.lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220927 인베스팅 본문 오류 해결 함수\n",
    "def conts_clean(conts):\n",
    "    conts_del = ['Position added successfully to: \\n']\n",
    "    conts_full = ''\n",
    "\n",
    "    for c in range(len(conts)):\n",
    "        page_line = conts[c].text\n",
    "        if page_line in conts_del:\n",
    "            continue\n",
    "        page_line = reporter_match(page_line)\n",
    "        page_line = investing_match(page_line)\n",
    "        conts_full += page_line + ' '\n",
    "    conts_full = conts_full.strip()\n",
    "\n",
    "    return conts_full\n",
    "\n",
    "\n",
    "## 그외 기타 전처리 함수\n",
    "# close_time_headline_keywords = ['usastocks', 'usastock', 'djia' , 'nasdaq', 'nyse','gspc','point']\n",
    "\n",
    "def news_close_time(content):\n",
    "    headline_keywords = ['usastocks', 'usastock', 'djia', 'nasdaq', 'nyse', 'gspc', 'point']\n",
    "    checking_text = market_preprocessing(content)\n",
    "    word_count = 0\n",
    "    ## 내용 검사\n",
    "    for i in range(len(checking_text)):\n",
    "        word = checking_text[i]\n",
    "        if word in headline_keywords:\n",
    "            word_count += 1\n",
    "\n",
    "    return word_count  # 1 이상이면 맞다는 것.\n",
    "\n",
    "\n",
    "def market_preprocessing(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"standard & poor's 500\", 'gspc')\n",
    "    sentence = sentence.replace('s&p 500', 'gspc')\n",
    "    sentence = sentence.replace('s&p', 'gspc')\n",
    "    sentence = sentence.replace('u.s. stock', 'usastock')\n",
    "    sentence = sentence.replace('dow jones industrial average', 'djia')\n",
    "    sentence = sentence.replace('%', ' ')\n",
    "    sentence = sentence.replace('%,', ' ')\n",
    "    word_tokens = word_tokenize(sentence)  # 월스트리트저널 기반 분해 -> s&p 500 을 산산조각냄\n",
    "    result = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            result.append(w)\n",
    "    lemm = WordNetLemmatizer()\n",
    "    wpos = []\n",
    "    # 이건 필요없을 듯\n",
    "    for w in result:\n",
    "        wn = lemm.lemmatize(w, pos='n')\n",
    "        wpos.append(wn)\n",
    "    tagged_list = pos_tag(wpos)\n",
    "\n",
    "    sw = ['.', ',', ':', 'CD', '(', ')', '``', \"''\", 'POS']\n",
    "    removed_list = []\n",
    "    for word in tagged_list:\n",
    "        if word[1] not in sw:\n",
    "            removed_list.append(word[0])\n",
    "    removed_list = set(removed_list)\n",
    "    removed_list = list(removed_list)\n",
    "\n",
    "    return list(removed_list)\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def invest_context_end_kill(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    con = 'continue reading'\n",
    "    e_num_pattern = re.compile('^©[0-9]{1,4}\\s[a-zA-Z]+')  # \\s[a-zA-Z]+\n",
    "\n",
    "    if cp[-1].text.lower().find(con) >= 0:\n",
    "        del cp[-1]\n",
    "    if cp[0].text == '\\n':\n",
    "        del cp[0]\n",
    "\n",
    "    for i in range(len(cp)):\n",
    "        m = e_num_pattern.match(cp[i].text)\n",
    "        if m != None:\n",
    "            pm = m.group(0)\n",
    "            if pm.find(pm) >= 0:\n",
    "                del cp[i]\n",
    "                break\n",
    "\n",
    "    return cp\n",
    "\n",
    "\n",
    "###### 데이터 불러오는 것 ###########################################\n",
    "\n",
    "def korean_like_data_split(df):\n",
    "    used_dk = df[df['사용 여부'] == 1]\n",
    "    df_sample = used_dk.loc[:, ['종목코드', '종목명', '종목 뉴스 주소']]\n",
    "    category_ids = []\n",
    "    category_news = []\n",
    "    category_names = []\n",
    "\n",
    "    for s in range(len(df_sample)):\n",
    "        stock_news = df_sample['종목 뉴스 주소'].iloc[s].split('/')\n",
    "        stock_news = stock_news[-1]\n",
    "        stock_ids = df_sample['종목코드'].iloc[s]\n",
    "        stock_name = df_sample['종목명'].iloc[s]\n",
    "        category_ids.append(stock_ids)\n",
    "        category_news.append(stock_news)\n",
    "        category_names.append(stock_name)\n",
    "\n",
    "    return category_ids, category_news, category_names\n",
    "\n",
    "\n",
    "## 아마존 DB 티커 불러올때, 핀비즈에서 검색 안되는 리스트\n",
    "def notfind_symbol_load():\n",
    "    unfind_df = pd.read_excel('./data/notfind_symbol.xlsx')\n",
    "    unfind_df_list = unfind_df.values.tolist()\n",
    "    un_symbol_list = []\n",
    "\n",
    "    for u in range(len(unfind_df_list)):\n",
    "        un_symbol = unfind_df_list[u][0]\n",
    "        un_symbol_list.append(un_symbol)\n",
    "    return un_symbol_list\n",
    "\n",
    "\n",
    "##### DB 함수들 ################################################\n",
    "\n",
    "\n",
    "## DB 저장용 -> 추후 오라클 DB화\n",
    "# 시황 뉴스 데이터 추가 - Question table\n",
    "def market_news_insert(db, url, title, body, w_date, n_date, press, m_time):\n",
    "    category = 'market_news'\n",
    "    cur = db.cursor()\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, market_time) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, m_time))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        pass\n",
    "\n",
    "\n",
    "# 종목/ETF 뉴스 데이터 추가 - Question table\n",
    "def stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker):\n",
    "    cur = db.cursor()  # ISIN\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, symbol) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, ticker))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        print(msg)\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793691e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
