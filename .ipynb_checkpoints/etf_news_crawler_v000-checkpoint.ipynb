{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4882f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import load !\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import openpyxl\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime as dtime\n",
    "from datetime import timedelta\n",
    "from dateutil.parser import parse\n",
    "from time import strptime\n",
    "import pytz\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import time\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c593d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 DB TEST 접속\n",
    "db = pymysql.connect(host='127.0.0.1',  port = 3306 , user = 'root',\n",
    "                           password = \"1234\", db = 'jisoo', charset = 'utf8')\n",
    "\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ed283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221117 20221122\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "https://www.fxstreet.com/analysis/week-ahead-on-wall-street-spy-qqq-thanksgiving-expected-to-see-markets-calm-on-shortened-week-202211211236 FXStreet\n",
      "['FXStreet', 'https://www.fxstreet.com/analysis/week-ahead-on-wall-street-spy-qqq-thanksgiving-expected-to-see-markets-calm-on-shortened-week-202211211236']\n",
      "2\n",
      "https://www.fxstreet.com/analysis/week-ahead-on-wall-street-spy-qqq-thanksgiving-expected-to-see-markets-calm-on-shortened-week-202211211236 FXStreet\n",
      "['FXStreet', 'https://www.fxstreet.com/analysis/week-ahead-on-wall-street-spy-qqq-thanksgiving-expected-to-see-markets-calm-on-shortened-week-202211211236']\n",
      "2\n",
      "https://www.fxstreet.com/analysis/week-ahead-on-wall-street-spy-qqq-thanksgiving-expected-to-see-markets-calm-on-shortened-week-202211211236 FXStreet\n",
      "['FXStreet', 'https://www.fxstreet.com/analysis/week-ahead-on-wall-street-spy-qqq-thanksgiving-expected-to-see-markets-calm-on-shortened-week-202211211236']\n"
     ]
    }
   ],
   "source": [
    "google_etf_news_crawler_main(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5674201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_etf_news_crawler_main(db):\n",
    "    symbol_list = symbols_load()\n",
    "    usa_est = dtime.now(pytz.timezone('America/New_York'))\n",
    "    yesterday = timedelta(days = 1)\n",
    "    usa_est_yesterday = usa_est - yesterday\n",
    "\n",
    "    start_date_input =  google_date_time_str(usa_est_yesterday)\n",
    "    end_date_input = google_date_time_str(usa_est)\n",
    "    \n",
    "    start_date_input = 20221117\n",
    "    end_date_input = 20221122\n",
    "    print(start_date_input, end_date_input)\n",
    "\n",
    "    page_num = 3\n",
    "\n",
    "    category = 'etf_news'\n",
    "\n",
    "    for s in range(len(symbol_list)):\n",
    "        symbol = symbol_list[s]\n",
    "        query = symbol +' '+'etf'  # 검색어 \n",
    "        google_news_crawler(db , symbol, query, start_date_input, end_date_input, page_num,  category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d48581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 자동화 테스트 \n",
    "def etf_news_main():\n",
    "    # 로컬 DB TEST 접속\n",
    "    db = pymysql.connect(host='127.0.0.1',  port = 3306 , user = 'root',\n",
    "                           password = \"1234\", db = 'jisoo', charset = 'utf8')\n",
    "    # 시간 재기용\n",
    "    start = time.time()\n",
    "    try:\n",
    "        google_etf_news_crawler_main(db)\n",
    "    except:\n",
    "        print('error!')\n",
    "        time.sleep(10)\n",
    "    print(\"WorkingTime: {} sec\".format(time.time() - start))  # 현재시각 - 시작시간 = 실행 시간\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "987a663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 미국 ETF 뉴스 크롤링 中 =====\n",
      "20221116 20221117\n",
      "WorkingTime: 67.26748967170715 sec\n",
      "20221116 20221117\n",
      "WorkingTime: 59.00272727012634 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('===== 미국 ETF 뉴스 크롤링 中 =====')\n",
    "\n",
    "# 10분에 한번씩 함수 실행\n",
    "schedule.every(10).minutes.do(etf_news_main)\n",
    "\n",
    "\n",
    "# 무한 반복\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb995580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221118 20221121\n",
      "https://www.google.com/search?q=QQQ etf&tbs=cdr:1,cd_min:11/18/2022,cd_max:11/21/2022&tbm=nws&start=0&gl=us\n",
      "[['ETF Trends', 'https://www.etftrends.com/etf-education-channel/it-could-be-time-to-revisit-tech-growth-stocks-and-qqq/'], ['Nasdaq', 'https://www.nasdaq.com/articles/should-invesco-qqq-qqq-be-on-your-investing-radar-4'], ['ETF Trends', 'https://www.etftrends.com/etf-education-channel/buffets-investment-could-enhance-allure-of-chip-heavy-qqq/'], ['Yahoo Finance', 'https://finance.yahoo.com/news/brent-crude-earnings-2-etfs-120012379.html'], ['Nasdaq', 'https://www.nasdaq.com/articles/gold-mining-and-large-cap:-2-etfs-to-watch-for-outsized-volume']]\n",
      "\n",
      "https://www.etftrends.com/etf-education-channel/it-could-be-time-to-revisit-tech-growth-stocks-and-qqq/ ETF Trends\n",
      "https://www.nasdaq.com/articles/should-invesco-qqq-qqq-be-on-your-investing-radar-4 Nasdaq\n",
      "https://www.etftrends.com/etf-education-channel/buffets-investment-could-enhance-allure-of-chip-heavy-qqq/ ETF Trends\n",
      "https://finance.yahoo.com/news/brent-crude-earnings-2-etfs-120012379.html Yahoo Finance\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m symbol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQQQ\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m query \u001b[38;5;241m=\u001b[39m symbol \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metf\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 검색어 \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mgoogle_news_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mgoogle_news_crawler\u001b[1;34m(db, symbol, query, start_date_input, end_date_input, page_num, category)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# 더이상 페이지 없음 \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 각 뉴스 언론사/주소를 통해 아웃링크 함수 입력 -> 아래엔 임의의 예시 \u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m parser_runing \u001b[38;5;241m=\u001b[39m \u001b[43moutlink_parsing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36moutlink_parsing\u001b[1;34m(db, purl, symbol, category)\u001b[0m\n\u001b[0;32m      5\u001b[0m press_t \u001b[38;5;241m=\u001b[39m purl[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m url_t \u001b[38;5;241m=\u001b[39m  purl[i][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m news_checking \u001b[38;5;241m=\u001b[39m \u001b[43mnews_duplicate_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m news_checking \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# 중복된 url + 종목의 뉴스가 있다면 스킵\u001b[39;00m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mnews_duplicate_check\u001b[1;34m(db, url, ticker)\u001b[0m\n\u001b[0;32m     59\u001b[0m checking_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM US_Stock_Market_News WHERE url = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and symbol =\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ticker \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m cur \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m---> 62\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchecking_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m result \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymysql\\cursors.py:148\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, query, args)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    146\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmogrify(query, args)\n\u001b[1;32m--> 148\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymysql\\cursors.py:310\u001b[0m, in \u001b[0;36mCursor._query\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_executed \u001b[38;5;241m=\u001b[39m q\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_result()\n\u001b[1;32m--> 310\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_get_result()\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrowcount\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py:548\u001b[0m, in \u001b[0;36mConnection.query\u001b[1;34m(self, sql, unbuffered)\u001b[0m\n\u001b[0;32m    546\u001b[0m     sql \u001b[38;5;241m=\u001b[39m sql\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_command(COMMAND\u001b[38;5;241m.\u001b[39mCOM_QUERY, sql)\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_query_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43munbuffered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munbuffered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py:775\u001b[0m, in \u001b[0;36mConnection._read_query_result\u001b[1;34m(self, unbuffered)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[38;5;241m=\u001b[39m MySQLResult(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 775\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mserver_status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py:1156\u001b[0m, in \u001b[0;36mMySQLResult.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1156\u001b[0m         first_packet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m first_packet\u001b[38;5;241m.\u001b[39mis_ok_packet():\n\u001b[0;32m   1159\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_ok_packet(first_packet)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py:692\u001b[0m, in \u001b[0;36mConnection._read_packet\u001b[1;34m(self, packet_type)\u001b[0m\n\u001b[0;32m    690\u001b[0m buff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m()\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 692\u001b[0m     packet_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;66;03m# if DEBUG: dump_packet(packet_header)\u001b[39;00m\n\u001b[0;32m    695\u001b[0m     btrl, btrh, packet_number \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<HBB\u001b[39m\u001b[38;5;124m\"\u001b[39m, packet_header)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py:732\u001b[0m, in \u001b[0;36mConnection._read_bytes\u001b[1;34m(self, num_bytes)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_date_input = 20221118\n",
    "end_date_input = 20221121\n",
    "print(start_date_input, end_date_input)\n",
    "\n",
    "page_num = 1\n",
    "\n",
    "category = 'etf_news'\n",
    "\n",
    "symbol = 'QQQ'\n",
    "query = symbol +' '+'etf'  # 검색어 \n",
    "google_news_crawler(db , symbol, query, start_date_input, end_date_input, page_num,  category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46920805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9954719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "217b0eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [429]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.google.com/search?q=qqq+etf&biw=2133&bih=1041&source=lnt&tbs=cdr%3A1%2Ccd_min%3A11%2F18%2F2022%2Ccd_max%3A11%2F21%2F2022&tbm=nws&gl=us'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3289fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "MAX_SLEEP_TIME = 10\n",
    "rand_value = randint(1, MAX_SLEEP_TIME)\n",
    "print(rand_value)\n",
    "time.sleep(rand_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071c0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7fcc80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def google_etf_news_crawler_main(db):\n",
    "    symbol_list = symbols_load()\n",
    "    usa_est = dtime.now(pytz.timezone('America/New_York'))\n",
    "    yesterday = timedelta(days = 1)\n",
    "    usa_est_yesterday = usa_est - yesterday\n",
    "\n",
    "    start_date_input =  google_date_time_str(usa_est_yesterday)\n",
    "    end_date_input = google_date_time_str(usa_est)\n",
    "    \n",
    "    start_date_input = 20221117\n",
    "    end_date_input = 20221122\n",
    "    print(start_date_input, end_date_input)\n",
    "\n",
    "    page_num = 3\n",
    "\n",
    "    category = 'etf_news'\n",
    "\n",
    "    for s in range(len(symbol_list)):\n",
    "        symbol = symbol_list[s]\n",
    "        query = symbol +' '+'etf'  # 검색어 \n",
    "        google_news_crawler(db , symbol, query, start_date_input, end_date_input, page_num,  category)\n",
    "\n",
    "def symbols_load():\n",
    "    etf_path = './data/ETF_top100_sample.xlsx'\n",
    "    df = pd.read_excel(etf_path)\n",
    "\n",
    "    symbol_list = []\n",
    "\n",
    "    for d in range(len(df['Symbol'])):\n",
    "        ticker = df['Symbol'].iloc[d]\n",
    "        symbol_list.append(ticker)\n",
    "    return symbol_list\n",
    "\n",
    "\n",
    "def google_date_time_str(time_pre):\n",
    "    month = date_zero_padding(time_pre.month)\n",
    "    days = date_zero_padding(time_pre.day)\n",
    "    year = date_zero_padding(time_pre.year)\n",
    "    ymd_date = str(year) + str(month)  + str(days)\n",
    "    \n",
    "    return ymd_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3470fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def outlink_parsing(db, purl, symbol,category):\n",
    "    news_info_list =[]\n",
    "    #print()\n",
    "    for i in range(len(purl)):\n",
    "        press_t = purl[i][0]\n",
    "        url_t =  purl[i][1]\n",
    "        news_checking = news_duplicate_check(db, url_t, symbol)\n",
    "        if news_checking == 1:\n",
    "            continue  # 중복된 url + 종목의 뉴스가 있다면 스킵\n",
    "        print(url_t, press_t)\n",
    "        ## 아웃링크 파서 함수     \n",
    "        if press_t == 'Yahoo Finance':\n",
    "            news_info = fyahoo_stock_news_inlink_extract(url_t, headers)\n",
    "\n",
    "        if press_t == 'Nasdaq':\n",
    "            news_info = nasdaq_news_parser(url_t, headers, press_t)\n",
    "\n",
    "        if press_t == 'ETF Trends':\n",
    "            news_info = etf_trands_news_parser(url_t, headers, press_t)\n",
    "\n",
    "        if press_t == 'ETF Daily News':\n",
    "            news_info = etf_daily_news_parser(url_t, headers, press_t)\n",
    "\n",
    "        if press_t =='The Motley Fool':\n",
    "            news_info = the_montley_fool_news_parser(url_t, headers, press_t)\n",
    "        try:\n",
    "            if press_t == 'Benzinga':\n",
    "                news_info = benzinga_news_parser(url_t, headers, press_t)\n",
    "        except:\n",
    "            print('date time missing')\n",
    "            continue\n",
    "\n",
    "        if press_t == 'Equities News':\n",
    "            news_info = equities_news_parser(url_t, headers, press_t)\n",
    "        try:\n",
    "            if press_t == 'MoneyShow':\n",
    "                news_info = moneyshow_news_parser(url_t, headers, press_t)\n",
    "        except:\n",
    "            print('date time missing')\n",
    "            continue\n",
    "\n",
    "        if press_t == 'Fxstreet':\n",
    "            news_info = fxstreet_news_parser(url_t, headers, press_t)\n",
    "\n",
    "        if press_t == 'Entrepreneur':\n",
    "            news_info = entrepreneur_news_parser(url_t, headers, press_t)\n",
    "\n",
    "        if press_t == 'Compound Advisors':\n",
    "            news_info = compound_advisors_news_parser(url_t, headers, press_t)\n",
    "        try: \n",
    "            nurl, title, body, w_date, n_date, press = news_info[0], news_info[1], news_info[2], news_info[3], news_info[4], news_info[5]\n",
    "            news_info_input = stock_news_insert(db, nurl, title, body, w_date, n_date, press, category, symbol)\n",
    "            news_info_list.append(news_info)\n",
    "        except: \n",
    "            print(purl[i])\n",
    "            continue\n",
    "    return news_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99e9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 아웃링크 파서 : 221116 기준 10개 (야후 파이낸스 제외)\n",
    "\n",
    "# Nasdaq\n",
    "def nasdaq_news_parser(url, headers, press): #v2\n",
    "\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('time' )\n",
    "\n",
    "    if dt == None:\n",
    "        dt = soup.find('p',{'class':'jupiter22-c-author-byline__timestamp'} )\n",
    "        dt_text = dt.text.replace(',','')\n",
    "        dt_split =  dt_text.split('—')\n",
    "        time_split = dt_split[1].split(' ')[1]\n",
    "        ampm = dt_split[1].split(' ')[2]\n",
    "        if ampm == 'am':\n",
    "            time_stamp = time_split\n",
    "        if ampm == 'pm':\n",
    "            time_hour = time_split.split(':')[0]\n",
    "            if time_hour == '12':\n",
    "                hour = time_hour\n",
    "            else:\n",
    "                hour = str(int(time_hour) + 12) \n",
    "            time_stamp = hour + ':' + time_split.split(':')[1]\n",
    "        dt_string = dt_split[0].strip()  + ' ' + time_stamp \n",
    "        \n",
    "        datetime_type = dtime.strptime(dt_string, '%B %d %Y %H:%M')\n",
    "      \n",
    "            \n",
    "    else:\n",
    "        dt = dt.text\n",
    "        if dt.find('AM') >= 0:\n",
    "            dt_string = dt.split('AM')[0]\n",
    "        if dt.find('PM') >= 0:\n",
    "            date_fram = dt.split('PM')[0]\n",
    "            date_split = date_fram.split(' ')\n",
    "            time_split = date_split[2].split(':')\n",
    "            if time_split[0] == '12':\n",
    "                hour = time_split[0] \n",
    "            else:\n",
    "                hour = str(int(time_split[0])+ 12)\n",
    "            time_hm = hour + ':' + time_split[1]\n",
    "            dt_string = date_split[0] + ' ' + date_split[1] + ' ' + time_hm\n",
    "        datetime_type = dtime.strptime(dt_string, '%b %d, %Y %H:%M')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'body__content'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('please visit') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\n','')        \n",
    "    news_data = [url, title, body, datetime_type, now, press]\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# ETF Trends    \n",
    "\n",
    "def etf_trands_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('meta', property='article:published_time')\n",
    "    dt = dt['content']\n",
    "    dt_split = dt.split('T')\n",
    "    dt_ymd = dt_split[0]\n",
    "    dt_time = dt_split[1].split('+')[0]\n",
    "    #dt = newsdate_preprocessing(dt)\n",
    "    dt_string = dt_ymd + ' '+dt_time \n",
    "    datetime_type = dtime.strptime(dt_string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'post-content post-dynamic description'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('For more news') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+ text\n",
    "            \n",
    "    news_data = [url, title, body, datetime_type, now, press]\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# ETF Daily News\n",
    "\n",
    "def etf_daily_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('meta', property='article:published_time')\n",
    "    dt = dt['content']\n",
    "    dt_split = dt.split('T')\n",
    "    dt_ymd = dt_split[0]\n",
    "    dt_time = dt_split[1].split('+')[0]\n",
    "    dt_string = dt_ymd + ' '+dt_time \n",
    "    datetime_type = dtime.strptime(dt_string, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'entry'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('FREE') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\n','')\n",
    "    \n",
    "    news_data = [url, title, body, datetime_type, now, press]\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "#  The Motley Fool\n",
    "\n",
    "def the_montley_fool_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('meta',{'name' :'date'})\n",
    "    dt = dt['content']\n",
    "    dt_split = dt.split('T')\n",
    "    dt_ymd = dt_split[0]\n",
    "    dt_time = dt_split[1].split('+')[0].replace('Z','')\n",
    "    dt_string = dt_ymd + ' '+dt_time\n",
    "    datetime_type = dtime.strptime(dt_string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'md:w-3/4 md:pr-80'})\n",
    "\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('no position') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "            \n",
    "    news_data = [url, title, body, datetime_type, now, press]        \n",
    "    return news_data\n",
    "\n",
    "#  Benzinga\n",
    "def benzinga_news_parser(url, headers, press):    \n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('span',{'class' :'date'})\n",
    "    dt_text = dt.text\n",
    "    if dt_text.find('PM') >= 1:\n",
    "        dt_text =  dt_text.replace('PM','')\n",
    "        dt_string = dt_text.strip()\n",
    "        time_split = dt_string.split(' ')\n",
    "        time_num = time_split[-1].split(':')\n",
    "        if time_num[0] == '12':\n",
    "            hour = time_num[0]\n",
    "        else:\n",
    "            hour = str(int(time_num[0]) + 12) \n",
    "        minute = time_num[1]\n",
    "        dt_string = time_split[0] + ' ' + time_split[1] + ' '+ time_split[2] + ' ' + hour + ':' + minute\n",
    "\n",
    "    if dt_text.find('AM') >= 1:\n",
    "        dt_text =  dt_text.replace('AM','')\n",
    "        dt_string = dt_text.strip()\n",
    "\n",
    "    datetime_type = dtime.strptime(dt_string, '%B %d, %Y %H:%M')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'ArticleBody__ArticleBodyDiv-sc-l6jpud-0 fZXicg article-content-body-only'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('Read Next') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\xa0','')\n",
    "    \n",
    "    news_data = [url, title, body, datetime_type, now, press] \n",
    "    \n",
    "    return news_data\n",
    "\n",
    "#  Equities News\n",
    "def equities_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('meta', {'name':'article.published'})\n",
    "    dt = dt['content']\n",
    "    datetime_type = newsdate_preprocessing(dt )\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find_all('p')\n",
    "    del_a = soup.find('p', {'class':'text-center margin-top-20'}).decompose()\n",
    "    del_b = soup.find('p', {'id':'signupSuccessMessageText'}).decompose()\n",
    "    del_c = soup.find('p', {'id':'modsignupErrorMessageText'}).decompose()\n",
    "    del_d = soup.find('p', {'class':'text small'}).decompose()\n",
    "    conts = soup.find_all('p')\n",
    "\n",
    "    body_list = []\n",
    "    body = ''\n",
    "\n",
    "    for b in range(len(conts)):\n",
    "        p_text = conts[b].text.strip()\n",
    "        if p_text.find('DISCLOSURE') >= 0:\n",
    "            break\n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "        body_list.append(p_text)\n",
    "\n",
    "        if b == 0:\n",
    "            body = p_text\n",
    "        else:\n",
    "            body = body + ' ' + p_text\n",
    "    body = body.replace('\\xa0', ' ')\n",
    "\n",
    "    news_data = [url, title, body, datetime_type, now, press] \n",
    "    \n",
    "    return news_data\n",
    "\n",
    "#  MoneyShow   - > 기사 내용이 너무 적음... 일단 ...\n",
    "    \n",
    "def moneyshow_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('div',{'class' :'my-2 text-muted font-roboto font-14'})\n",
    "    dt = dt.find('span')\n",
    "    dt_text = dt.text\n",
    "    date_split_list = dt_text.split(' ')\n",
    "    ymd_s, time_s, ampm  = date_split_list [0], date_split_list [1], date_split_list [2]\n",
    "    ymd_three = ymd_s.split('/')\n",
    "    dt_ymd = ymd_three[2] + '-' + ymd_three[0]  + '-' + ymd_three[1]  \n",
    "    if ampm == 'am' :\n",
    "        dt_time = time_s\n",
    "    if ampm == 'pm':\n",
    "        ts_list = time_s.split(':')\n",
    "        if ts_list[0] == '12':\n",
    "            hour = ts_list[0] \n",
    "        else:\n",
    "            hour = str(int(ts_list[0]) + 12)\n",
    "        dt_time = hour + ':' + ts_list[1]\n",
    "    dt_string = dt_ymd + ' '+ dt_time\n",
    "    datetime_type = dtime.strptime(dt_string, '%Y-%m-%d %H:%M')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'mt-4 pt-4 font-roboto article-body'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('To learn more') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\xa0','')\n",
    "    news_data = [url, title, body, datetime_type, now, press] \n",
    "    \n",
    "    return news_data\n",
    "\n",
    "\n",
    "#  Fxstreet\n",
    "    \n",
    "def fxstreet_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('time')\n",
    "    dt = dt['datetime']\n",
    "    dt_split = dt.split('T')\n",
    "    dt_ymd = dt_split[0]\n",
    "    dt_time = dt_split[1].split('+')[0].replace('Z','')\n",
    "    dt_string = dt_ymd + ' '+dt_time\n",
    "    datetime_type = dtime.strptime(dt_string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'id' : 'fxs_article_content'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('WSJ') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\xa0',' ')\n",
    "    news_data = [url, title, body, datetime_type, now, press] \n",
    "    \n",
    "    return news_data\n",
    "\n",
    "#  Entrepreneur\n",
    "    \n",
    "def entrepreneur_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('meta', property='article:published_time')\n",
    "    dt = dt['content']\n",
    "    dt = dt.replace('T', ' ')\n",
    "    dt = dt.replace('+', '')\n",
    "    dt = dt.replace('00:00', '')\n",
    "    datetime_type = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'max-w-3xl prose prose-blue text-lg leading-8 mb-8'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('Learn more') >= 0:\n",
    "            break\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\xa0','')\n",
    "    body = body.replace('\\n\\n','')\n",
    "    news_data = [url, title, body, datetime_type, now, press] \n",
    "    \n",
    "    return news_data\n",
    "\n",
    "#  Compound Advisors\n",
    "\n",
    "def compound_advisors_news_parser(url, headers, press):\n",
    "    now = dtime.now()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #  제목\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    #  날짜 \n",
    "    dt = soup.find('meta', property='article:published_time')\n",
    "    dt = dt['content']\n",
    "    dt = dt.replace('T', ' ')\n",
    "    dt = dt.replace('+', '')\n",
    "    dt = dt.replace('00:00', '')\n",
    "    dt_split = dt.split(' ')\n",
    "    times = dt_split[1].split('-')[0]\n",
    "    dt = dt_split[0]+ ' ' +times\n",
    "    datetime_type = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #  본문 \n",
    "    conts = soup.find('div', {'class' : 'single-content content'})\n",
    "    body_p = conts.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('click here') >= 0:\n",
    "            continue\n",
    "        if i == 0:\n",
    "            body = text\n",
    "        else:\n",
    "            body = body + ' '+text\n",
    "    body = body.replace('\\xa0','')\n",
    "    body = body.replace('\\n\\n','')\n",
    "    \n",
    "    news_data = [url, title, body, datetime_type, now, press] \n",
    "    \n",
    "    return news_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98a1287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 추가되는 함수들 ###\n",
    "def google_news_crawler(db , symbol, query, start_date_input, end_date_input, page_num,  category):\n",
    "    start_date = google_date_process(start_date_input)# 시작 날짜   월 일 년도\n",
    "    end_date = google_date_process(end_date_input) # 마지막 날짜   월 일 년도\n",
    "    page_cnt = (page_num -1 ) * 10 \n",
    "\n",
    "    for p in range(page_num):\n",
    "        url = \"https://www.google.com/search?q=\" + query +  \"&tbs=cdr:1,cd_min:\" + start_date +  \",cd_max:\" + end_date +  \"&tbm=nws&start=\" + str(page_cnt) +\"&gl=us\"\n",
    "        purl, no_ = google_crawling(url, headers)\n",
    "        print(len(purl))\n",
    "        #print(url)\n",
    "        #print(purl)\n",
    "        if no_ == 1 :\n",
    "            break # 더이상 페이지 없음 \n",
    "        # 각 뉴스 언론사/주소를 통해 아웃링크 함수 입력 -> 아래엔 임의의 예시 \n",
    "        parser_runing = outlink_parsing(db, purl, symbol, category)\n",
    "        time.sleep(40)\n",
    "        \n",
    "def google_crawling(url, headers):\n",
    "    c_press_list = ['Nasdaq', 'ETF Trends', 'FXStreet', 'ETF Daily News','Compound Advisors', 'Benzinga', 'Equities News', 'MoneyShow',\n",
    "                'The Motley Fool', 'Entrepreneur',  'Yahoo Finance']\n",
    "    purl_list = []\n",
    "    break_cnt = 0\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    #print(soup)\n",
    "\n",
    "    for i in range(10): # 기본 1페이지당 10 개의 글\n",
    "        find_select_url = '#rso > div > div > div:nth-child('+str(i+1)+') > div > div > a'\n",
    "        find_select_press = find_select_url  + ' > div > div > div.CEMjEf.NUnG9d'\n",
    "\n",
    "        date_find_url = soup.select(find_select_url)\n",
    "        date_find_press = soup.select(find_select_press)\n",
    "        if len(date_find_url) == 0: # 더 이상 뉴스 게시글이 없을 때, \n",
    "            break_cnt = 1\n",
    "            break\n",
    "        news_link = date_find_url[0]['href']\n",
    "        news_press = date_find_press [0].text\n",
    "\n",
    "        if news_press not in c_press_list:\n",
    "            continue\n",
    "    \n",
    "        press_data = [news_press, news_link ] \n",
    "        purl_list.append(press_data)\n",
    "    return purl_list, break_cnt\n",
    "\n",
    "def google_date_process(date_input):\n",
    "    years, month, days = str(date_input)[:4], str(date_input)[4:6], str(date_input)[6:]\n",
    "    str_date = month + '/' + days + '/' + years \n",
    "    \n",
    "    return str_date\n",
    "        \n",
    "def newsdate_preprocessing(dt):\n",
    "    dt = dt.replace('T', ' ')\n",
    "    dt = dt.replace('+09:00', '')\n",
    "    dt = datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    return dt\n",
    "\n",
    "def news_duplicate_check(db, url, ticker):\n",
    "    # db에 url 가 있다면 break! ]\n",
    "    checking_sql = \"SELECT * FROM US_Stock_Market_News WHERE url = '\" + url + \"' and symbol ='\" + ticker + \"'\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    if len(result) >= 1:\n",
    "        result = 1  # 1이면 contine\n",
    "    else:\n",
    "        result = 0\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91b6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################### 시장 시간대 구역 ############################\n",
    "# 시간 관련 라이브러리에서 종종 1자리수 숫자에 0 안붙이는거 해결...\n",
    "def date_zero_padding(m):\n",
    "    if m < 10:\n",
    "        m = '0' + str(m)\n",
    "    else:\n",
    "        m = str(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "# 노멀한 메타 데이터에서 시간 추출\n",
    "def yahoo_market_time(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def yahoo_market_search_time(url, market_time):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if market_time == 0:\n",
    "        market_time = ['00:10:00', '00:20:00']\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "\n",
    "    start_hms = market_time[0]\n",
    "    end_hms = market_time[1]\n",
    "    # start_hms = '16:00:00'\n",
    "    # end_hms = '16:30:00'\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "    ymd_date = str(usa_est.year) + '-' + str(usa_est.month) + '-' + str(usa_est.day)\n",
    "\n",
    "    limit_start = ymd_date + ' ' + start_hms\n",
    "    limit_end = ymd_date + ' ' + end_hms\n",
    "\n",
    "    limit_start = dtime.strptime(limit_start, '%Y-%m-%d %H:%M:%S')\n",
    "    limit_end = dtime.strptime(limit_end, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return date_time, limit_start, limit_end\n",
    "\n",
    "\n",
    "###################핀 비즈 구역 ############################\n",
    "\n",
    "def finviz_news_crawler(db, category, ticker_list, ymd_date):\n",
    "    news_info_list = []\n",
    "    error_cnt = 0\n",
    "\n",
    "    for t in range(len(ticker_list)):\n",
    "        ticker = ticker_list[t][0]\n",
    "        # ticker, s_name, s_code = ticker_list[1][0], ticker_list[1][1], ticker_list[1][2]\n",
    "        try:\n",
    "            fin_dic = finviz_news_crawling(ticker, ymd_date)\n",
    "        except:\n",
    "            error_cnt += 1\n",
    "            print('[' + str(error_cnt) + ']', ticker)\n",
    "\n",
    "            continue\n",
    "        if len(fin_dic) == 0:\n",
    "            continue  # 오늘 데이터 없으면 다음 티커 를 탐색\n",
    "        p_list = fin_dic[ymd_date]\n",
    "\n",
    "        for u in range(len(p_list)):\n",
    "            url = p_list[u][-1]\n",
    "            news_checking = finviz_news_check(db, url, ticker)\n",
    "            if news_checking == 1:\n",
    "                continue  # 중복된 url + 종목의 뉴스가 있다면 스킵\n",
    "\n",
    "            try:\n",
    "                news_info = fyahoo_stock_news_inlink_extract(url, headers)\n",
    "                # [news_info] ... + 분류명 (종목 뉴스) , 티커, 종목명, 종목 코드\n",
    "                # [news_info] ... + categroy ('stock_news') , ticker, stock_name, stock_code\n",
    "                # category column 변경 필요 ->  market_time 으로, 기존의 시황 뉴스는 category = 'market_news' 로\n",
    "\n",
    "                news_info_add = news_info + [category] + [ticker]\n",
    "                url, title, body = news_info_add[0], news_info_add[1], news_info_add[2]\n",
    "                w_date, n_date, press = news_info_add[3], news_info_add[4], news_info_add[5]\n",
    "                category, ticker = news_info_add[6], news_info_add[7]\n",
    "\n",
    "                news_input = stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker)\n",
    "                nadd_info = [url, title, body, w_date, n_date, press, category, ticker]\n",
    "                # 나중엔 삭제 똔느 스킵\n",
    "                news_info_list.append(nadd_info)\n",
    "                # print(url,nadd_info[6:])  # 확인용\n",
    "\n",
    "                ## db 저장하는 함수 필요 : 최근 글 중복 처리하는 방법도 필요\n",
    "\n",
    "            except:\n",
    "                print('Abnormal detected :', url)\n",
    "                continue\n",
    "            time.sleep(2)\n",
    "        # print(ticker,len(news_info_list))\n",
    "    print(error_cnt)\n",
    "    return news_info_list\n",
    "\n",
    "\n",
    "def finviz_news_crawling(name, ymd_date):\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks', 'The Telegraph',\n",
    "                    'Insider Monkey',\n",
    "                    'Benzinga', 'Simply Wall St.', 'Business Wire', 'The Independent', 'Fortune', 'CoinDesk', 'Variety',\n",
    "                    'PR Newswire', 'WWD', 'Skift', 'LA Times', 'The Guardian', 'Poets & Quants', 'GuruFocus.com']\n",
    "\n",
    "    url = \"https://finviz.com/quote.ashx?t=\" + name + \"&p=d\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    date_find = soup.find_all('table', {'class': 'fullview-news-outer'})\n",
    "    date_finding = date_find[0].find_all('td', {'align': 'right'})\n",
    "\n",
    "    news_link = date_find[0].find_all('div', {'class': 'news-link-left'})\n",
    "    news_press = date_find[0].find_all('div', {'class': 'news-link-right'})\n",
    "\n",
    "    purl_list = []\n",
    "\n",
    "    date_dic = {}  # 만약을 위해  view에 보이는 날짜 전부를 수집, 평소엔 가장 상단의 키만을 사용 -> 다음 키 생성시 break\n",
    "\n",
    "    for i in range(len(news_link)):\n",
    "        press_name = news_press[i].find('span').text.lstrip()\n",
    "        press_title = news_link[i].text\n",
    "        press_url = news_link[i].find('a')['href']\n",
    "\n",
    "        date_time = date_finding[i].text\n",
    "        date_split_time = date_time.split(' ')\n",
    "        if len(date_split_time) >= 2:\n",
    "            ymdate = date_split_time[0].split('-')\n",
    "            m, d, y = ymdate[0], ymdate[1], ymdate[2]\n",
    "            m = strptime(m, '%b').tm_mon\n",
    "            if m < 10:\n",
    "                m = '0' + str(m)\n",
    "            else:\n",
    "                m = str(m)\n",
    "            ymd_str = '20' + y + '-' + m + '-' + d  # dict 키 용\n",
    "            dtime_pro = date_split_time[1]\n",
    "\n",
    "            if ymd_str != ymd_date:\n",
    "                break  # 오늘 아니면 탈출\n",
    "\n",
    "            date_dic[ymd_str] = []\n",
    "\n",
    "            if len(date_dic) > 2:\n",
    "                del date_dic[ymd_str]\n",
    "                break  # 딱 하루치만 보기\n",
    "\n",
    "        else:\n",
    "            dtime_pro = date_split_time[0]\n",
    "\n",
    "        if press_url.find('finance.yahoo') == -1:\n",
    "            continue\n",
    "        if press_name not in c_press_list:\n",
    "            continue\n",
    "\n",
    "        date_dic[ymd_str].append([dtime_pro, press_name, press_title, press_url])\n",
    "    return date_dic\n",
    "\n",
    "\n",
    "def finviz_news_check(db, url, ticker):\n",
    "    # db에 url 가 있다면 break! ]\n",
    "    checking_sql = \"SELECT * FROM US_Stock_Market_News WHERE url = '\" + url + \"' and symbol ='\" + ticker + \"'\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    if len(result) >= 1:\n",
    "        result = 1  # 1이면 contine\n",
    "    else:\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "\n",
    "def stock_info_load(db, select_ticker):\n",
    "    checking_sql = \"SELECT symbol, name, ISIN FROM Company_Information_Data\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    df_db = pd.DataFrame(result, columns=['symbol', 'name', 'ISIN'])\n",
    "\n",
    "    target_sname = []\n",
    "    if select_ticker != 0:\n",
    "        # 선택한 것만 하는 것\n",
    "        for s in range(len(select_ticker)):\n",
    "            ticker = select_ticker[s]\n",
    "            stock_info = df_db[df_db['symbol'] == ticker]\n",
    "            list_con = stock_info.values.tolist()\n",
    "            target_sname.append(list_con[0])\n",
    "\n",
    "        df = pd.DataFrame(target_sname, columns=['symbol', 'name', 'ISIN'])\n",
    "    else:\n",
    "        df = df_db\n",
    "    ticker_list = []\n",
    "\n",
    "    for d in range(len(df)):\n",
    "        ticker = df['symbol'].iloc[d]\n",
    "        s_name = df['name'].iloc[d]\n",
    "        s_code = df['ISIN'].iloc[d]\n",
    "        ticker_list.append([ticker, s_name, s_code])\n",
    "\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "###################야후 파이낸스 구역 ############################\n",
    "\n",
    "def yahoo_crawler(db, mtime):\n",
    "    ## 미국 주식 거래시간\n",
    "    us_market_time_dic = {'BMO(UST)': ['07:30:00', '09:30:00'], 'AMO(UST)': ['10:30:00', '11:30:00'],\n",
    "                          'AMC(UST)': ['17:00:00', '18:30:00'], 'BMO(DST)': ['06:30:00', '09:00:00'],\n",
    "                          'AMO(DST)': ['09:30:00', '10:30:00'], 'AMC(DST)': ['16:00:00', '17:30:00']}\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks',\n",
    "                    'Fortune' 'LA Times', 'Kiplinger', 'Business Insider', 'USA TODAY', 'Insider']\n",
    "\n",
    "    o_press_list = [\"Investor's  Daily\", 'TheStreet.com']  # 아웃링크 뉴스 언론사\n",
    "    c_press_list = c_press_list + o_press_list\n",
    "    # c_press_list = ['Bloomberg', 'Reuters', 'Kiplinger', 'Business Insider', 'Yahoo Finance', 'USA TODAY', 'LA Times']\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    url_list = yahoo_crawling(db)\n",
    "    market_time = us_market_time_dic[mtime]\n",
    "    # c = category\n",
    "    # 임시 리턴\n",
    "    pre_urls = []\n",
    "    data_list = []\n",
    "    error_list = []\n",
    "    for u in range(len(url_list)):\n",
    "        source = url_list[u][0]\n",
    "        w_press = url_list[u][1]\n",
    "        w_press = w_press.replace('News', '').lstrip()\n",
    "        pre_urls.append([w_press, source])\n",
    "        if w_press in c_press_list:  # 검증한 언론사들이 포함되어 있다면,\n",
    "            # 시간대 좁히기\n",
    "            date_time, limit_start, limit_end = yahoo_market_search_time(source, market_time)\n",
    "            if date_time >= limit_start and date_time < limit_end:  # 시황 마감후 1시 30분 까지가 아니라면\n",
    "                try:\n",
    "                    news_parsing = fyahoo_stock_news_inlink_extract(source, headers)\n",
    "                except:\n",
    "                    error_news = [w_press, source]\n",
    "                    print('error :', error_news)\n",
    "                    error_list.append(error_news)\n",
    "                    continue\n",
    "                title, body = news_parsing[1], news_parsing[2]\n",
    "                # 시황 뉴스의 경우, 시간대 체크 -> 범용으로 쓰이기 위해 따로 둠 v1 -> v2\n",
    "                if title_classifier(title) == 0:\n",
    "                    continue\n",
    "                w_date, n_date, press = news_parsing[3], news_parsing[4], news_parsing[5]\n",
    "                in_data = [source, title, body, w_date, n_date, press, mtime]\n",
    "                # print(in_data)\n",
    "                data_list.append(in_data)\n",
    "                market_news_insert(db, source, title, body, w_date, n_date, press, mtime)\n",
    "\n",
    "    return pre_urls, data_list, error_list\n",
    "\n",
    "\n",
    "## 220614-15 임시 수집 타이밍  :  야후 파이낸스\n",
    "def yahoo_crawling(db):\n",
    "    url = 'https://finance.yahoo.com/topic/stock-market-news/'\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # chrome_options = webdriver.ChromeOptions()\n",
    "    # chrome_options.add_argument('--headless')\n",
    "    # chrome_options.add_argument('--no-sandbox')\n",
    "    # chrome_options.add_argument(\"--single-process\")\n",
    "    # chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    path = 'chromedriver_107.exe'\n",
    "    driver = webdriver.Chrome(path)  # , chrome_options=chrome_options)\n",
    "    count = 200  # 페이지 한번에 보여줄수 있는양\n",
    "    url_list = []\n",
    "    driver.get(url)\n",
    "    # 스크롤 전 높이\n",
    "    before_h = driver.execute_script(\"return window.scrollY\")\n",
    "    # 무한 스크롤\n",
    "    while True:\n",
    "        # 맨 아래로 스크롤을 내린다.\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_css_selector(\"body\").send_keys(Keys.END)\n",
    "        # 스크롤 사이 페이지 로딩 시간\n",
    "        time.sleep(2)\n",
    "        # 스크롤 후 높이\n",
    "        after_h = driver.execute_script(\"return window.scrollY\")\n",
    "        if after_h == before_h:\n",
    "            break\n",
    "        before_h = after_h\n",
    "    #### 페이지내 url 긁어오기\n",
    "    url_list = []\n",
    "\n",
    "    for i in range(count):  # 페이지 한계로 200 개 정도 확인\n",
    "        press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[2]'\n",
    "        href_i = press_i + '/h3/a'\n",
    "        href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "        press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[1]'\n",
    "            href_i = press_i + '/h3/a'\n",
    "            href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "            press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            continue\n",
    "\n",
    "        href = href_xpath[0].get_attribute('href')\n",
    "        title = href_xpath[0].text\n",
    "        press = press_xpath[0].text\n",
    "        press = press.split('•')\n",
    "        press = press[0].replace('Business', '').lstrip()\n",
    "        url_list.append([href, press])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "################### 파싱 함수구역 ############################\n",
    "\n",
    "# 인링크 방식 뉴스 내용 파싱 : 야후 버전 실상 버전 2이나 구별을 위해 inlink만 사용\n",
    "def fyahoo_stock_news_inlink_extract(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    news_data = []\n",
    "\n",
    "    date_time = yahoo_market_time(url)\n",
    "    now = dtime.now()\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    press = soup.find(\"img\", {'class': \"caas-img\"})\n",
    "    press = press['alt']\n",
    "\n",
    "    conts = soup.find_all('p')  # .text\n",
    "    content = soup.find_all('div', {'class': 'caas-body'})\n",
    "    body_p = content[0]('p')\n",
    "    body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'USA TODAY':\n",
    "        headline = context_preprocessing(conts[1:3])\n",
    "        # body = context_preprocessing(conts)\n",
    "    if press == 'Reuters':\n",
    "        # body_p = period_index(body_p)\n",
    "        body = body_p_clean(body_p, press)\n",
    "        body = body.split('(Reporting')\n",
    "        bodys = body[0]\n",
    "        body = press_local_find(bodys, press)\n",
    "    if press == 'LA Times':\n",
    "        body = body.split('This story originally')\n",
    "        body = body[0]\n",
    "    if press == 'Bloomberg':\n",
    "        body = body.split('(Updates')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Investing.com':\n",
    "        body = body.split('Related Articles')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Zacks':\n",
    "        body = body.split('Want the latest')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == \"Investor's Business Daily\":  # 아웃링크\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        content = soup.find_all('div', {'class': 'single-post-content post-content drop-cap'})\n",
    "        body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'TheStreet.com':\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if out_url.find('realmoney') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-author-rail__body'})\n",
    "            body_p = content[0]('p')\n",
    "        elif out_url.find('aap.') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-back-header__body'})\n",
    "            body_p = content[0]('div')\n",
    "        else:\n",
    "            content = soup.find_all('div', {'class': 'm-detail--body'})\n",
    "            body_p = content[0]('p')\n",
    "        # body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    # body = body_split(body)\n",
    "    # headline_token = sent_tokenize(body)\n",
    "    # if len(headline_token) <= 2:\n",
    "    #    headline = body\n",
    "    # else:\n",
    "    #    headline = headline_token[0] + ' ' + headline_token[1]\n",
    "\n",
    "    inlink_news = [url, title, body, date_time, now, press]\n",
    "\n",
    "    return inlink_news\n",
    "\n",
    "\n",
    "################### 기타 전처리 함수구역 ############################\n",
    "\n",
    "# 만일을 위해서 함수화 했는데.. 생각보다 전처리가 잘 되었기에 ...\n",
    "def clean_text(text):\n",
    "    # cleaned_text = text.replace(\"\\'\", \"'\")\n",
    "    cleaned_text = text.replace('  \\n ', '')\n",
    "    cleaned_text = cleaned_text.replace('(Bloomberg) -- ', '')\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# 날짜를 datetime format으로 변환 하기 위한 함수\n",
    "def dt_format(dt_texts):\n",
    "    a = dt_texts.find('(')\n",
    "    b = dt_texts.find(')')\n",
    "\n",
    "    result = dt_texts[a + 1:b]\n",
    "    result = result.replace(',', '')\n",
    "    dt = result.split(' ')\n",
    "    dt_pm = dt[3].find('PM')  # 있으면 + 없으면 -1 반환\n",
    "\n",
    "    if dt_pm != -1:  ## 오후 타임이면,\n",
    "        pre_dt = dt[3].replace('PM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        if int(pre_dt[0]) == 12:\n",
    "            t, m = pre_dt[0], pre_dt[1]\n",
    "        else:\n",
    "            pm_time = str(int(pre_dt[0]) + 12)\n",
    "            t, m = pm_time, pre_dt[1]\n",
    "\n",
    "    else:  # 오전타임이면\n",
    "        pre_dt = dt[3].replace('AM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        t, m = pre_dt[0], pre_dt[1]\n",
    "\n",
    "    dt_mon = strptime(dt[0], '%b').tm_mon\n",
    "\n",
    "    if dt_mon < 10:\n",
    "        dt_mon = '0' + str(dt_mon)\n",
    "\n",
    "    str_datetimed = dt[2] + '-' + str(dt_mon) + '-' + dt[1] + ' ' + t + \":\" + m + \":\" + '00'\n",
    "    currdate = datetime.datetime.strptime(str_datetimed, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return currdate\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def context_preprocessing(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    for i in range(len(cp)):\n",
    "        texts = texts + cp[i].text + ' '\n",
    "        texts_list.append(cp[i].text)\n",
    "    cleans = clean_text(texts)\n",
    "    return cleans\n",
    "\n",
    "\n",
    "### 뉴스 분류  : 키워드 방식인데, 아무래도 일부 키워드는 해결 불가하기에, 모델 구축 방향으로 가야함, 데이터가 쌓이면\n",
    "\n",
    "# 맞는 뉴스인지 아닌지 판단 하기  : 키워드 기반\n",
    "def title_classifier(title):\n",
    "    # headline  = data[2]\n",
    "    t_check = title_key_check(title)\n",
    "    # h_check= news_close_time(headline )\n",
    "    classifier = 0\n",
    "    if t_check >= 1:\n",
    "        classifier = 1  # 제목에 시황 뉴스를 암시하는 'stock market today' 가 있다면,\n",
    "    # 제목엔 없지만 헤드라인에 관련 키워드가 한개이상이라면,\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# 규칙 제목 한정 제목 분류\n",
    "def title_key_check(sentence):\n",
    "    title_key = ['smtody', 'cloftrade', 'wallst', 'pts', 'usastocks', 'liveupdate', 'usatocksturn',\n",
    "                 'marketshows', 'ustocksdrop', 'ustockshigh', 'stockmarketcloses', 'marketswrap', 'stockmarket',\n",
    "                 'stocksendlower']\n",
    "\n",
    "    #  'markets', 'market']  # 'futures',\n",
    "    un_key = ['emerging', 'EMERGING', 'global', 'GLOBAL']\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"stock market today\", title_key[0])\n",
    "    sentence = sentence.replace(\"stock market today:\", title_key[0])\n",
    "    sentence = sentence.replace(\"close of trade\", title_key[1])\n",
    "    sentence = sentence.replace(\"us stocks-wall\", title_key[2])\n",
    "    # sentence = sentence.replace(\"wall st\", title_key[2])\n",
    "    sentence = sentence.replace(\"pts;\", title_key[3])\n",
    "    sentence = sentence.replace(\"u.s. stocks\", title_key[4])\n",
    "    sentence = sentence.replace(\"live updates:\", title_key[5])\n",
    "    sentence = sentence.replace(\"us stocks turn\", title_key[6])\n",
    "    sentence = sentence.replace(\"market shows\", title_key[7])\n",
    "    sentence = sentence.replace(\"us stocks drop\", title_key[8])\n",
    "    sentence = sentence.replace(\"us stocks high\", title_key[9])\n",
    "    sentence = sentence.replace(\"stock market closes\", title_key[10])\n",
    "    sentence = sentence.replace(\"markets wrap\", title_key[11])\n",
    "    sentence = sentence.replace(\"stock market\", title_key[12])\n",
    "    sentence = sentence.replace(\"stocks end lower\", title_key[13])\n",
    "\n",
    "    word_count = 0\n",
    "    for t in range(len(title_key)):\n",
    "        if title_key[t] in sentence:\n",
    "            word_count += 1\n",
    "            # print(title_key[t])\n",
    "        # 제거\n",
    "        if title_key[t] in un_key:\n",
    "            word_count = 0\n",
    "            break\n",
    "\n",
    "    return word_count\n",
    "\n",
    "\n",
    "# 야후 파이낸스 전용 전처리\n",
    "def body_p_clean(body_p, press):\n",
    "    # body_p = soup.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for s in range(len(body_p)):\n",
    "        body_text = body_p[s].text\n",
    "        if press == 'Yahoo Finance':\n",
    "            if body_text == '—':\n",
    "                break\n",
    "        if press == 'Bloomberg':\n",
    "            if s == 0:\n",
    "                body_text = bloomberg_match(body_text)\n",
    "            else:\n",
    "                if body_p[s].find('a') is not None:\n",
    "                    continue\n",
    "                more_finding = body_text.find('Most Read')\n",
    "                if more_finding == 0:\n",
    "                    continue\n",
    "                read_finding = body_text.find('Read more:')\n",
    "                if read_finding == 0:\n",
    "                    continue\n",
    "                body_text = bloomberg_end_match(body_text)\n",
    "                if body_text.find('More market') != -1:\n",
    "                    break\n",
    "                if len(body_text) == 0:\n",
    "                    continue\n",
    "                if body_text.find('this week:') != -1:\n",
    "                    break\n",
    "                if body_text.find('Read:') != -1:\n",
    "                    break\n",
    "\n",
    "        # 여기서부터 언론사별 불필요한 패턴 자르기\n",
    "\n",
    "        if press == 'Kiplinger':\n",
    "            if body_text.find('SEE MORE') != -1:\n",
    "                continue\n",
    "            if body_text.find('Sign up') != -1:\n",
    "                continue\n",
    "            if body_text.find('YCharts') != -1:\n",
    "                break\n",
    "\n",
    "        if press == 'Zacks':\n",
    "            # check_tag = body_p[s].find('strong')\n",
    "            # if check_tag != None:\n",
    "            #    continue\n",
    "            zacks_finding = body_text.find('Zacks Investment Research?')\n",
    "\n",
    "            if zacks_finding >= 0:\n",
    "                continue\n",
    "\n",
    "            infull_finding = body_text.find('[In full disclosure,')\n",
    "            if infull_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Yahoo Finance':\n",
    "            check_tag = body_p[s].find('em')\n",
    "            if check_tag != None:\n",
    "                continue\n",
    "            yahoo_click_finding = body_text.find('Click here for')\n",
    "            if yahoo_click_finding >= 0:\n",
    "                continue\n",
    "            yahoo_read_finding = body_text.find('Read the latest')\n",
    "            if yahoo_read_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Benzinga':\n",
    "            see_benzinga_finding = body_text.find('See more')\n",
    "            if see_benzinga_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Insider Monkey':\n",
    "            insider_monkey_c_finding = body_text.find('Click to continue')\n",
    "            if insider_monkey_c_finding >= 0:\n",
    "                break\n",
    "            insider_monkey_s_finding = body_text.find('Suggested Articles:')\n",
    "            if insider_monkey_s_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Wire':\n",
    "            business_w_finding = body_text.find('NOTE TO')\n",
    "            if business_w_finding >= 0:\n",
    "                break\n",
    "            business_c_finding = body_text.find('businesswire.com')\n",
    "            if business_c_finding >= 0:\n",
    "                break\n",
    "\n",
    "            local_date_press = '[A-Z]+\\s[A-Z]+,\\s[A-Z]{1}[a-z]+\\s[0-9]{2},\\s[0-9]{4}[-]{1,2}[(][A-Z]{8}\\s[A-Z]{4}[)][-]{1,2}'\n",
    "            body_text = re.sub(local_date_press, '', body_text)\n",
    "\n",
    "        if press == 'Fortune':\n",
    "            fortune_finding = body_text.find('featured on Fortune.com')\n",
    "            if fortune_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Simply Wall St.':\n",
    "            sw_st_finding = body_text.find('free platform')\n",
    "            if sw_st_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Variety':\n",
    "            variety_finding = body_text.find('Best of Variety')\n",
    "            if variety_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'PR Newswire':\n",
    "            pr_newswire_finding = body_text.find('Trademarks')\n",
    "            if pr_newswire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Skift':\n",
    "            skift_finding = body_text.find('Subscribe to Skift')\n",
    "            if skift_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Poets & Quants':\n",
    "            pq_finding = body_text.find('MISS POLL')\n",
    "            if pq_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'ACCESSWIRE':\n",
    "            accwire_finding = body_text.find('View additional')\n",
    "            if accwire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'InvestorPlace':\n",
    "            inplace_finding = body_text.find('More From InvestorPlace')\n",
    "            if inplace_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'GuruFocus.com':\n",
    "            gf_finding = body_text.find('appeared on GuruFocus')\n",
    "            if gf_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Insider':\n",
    "            binsider_finding = body_text.find('Read the ')\n",
    "            if binsider_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == \"Investor's Business Daily\":\n",
    "            invest_bd_finding = body_text.find('YOU MAY ALSO LIKE:')\n",
    "            if invest_bd_finding >= 0:\n",
    "                break\n",
    "            invest_plz_finding = body_text.find('Please follow')\n",
    "            if invest_plz_finding >= 0:\n",
    "                break\n",
    "            invest_follow_finding = body_text.find('Follow')\n",
    "            if invest_follow_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == \"TheStreet.com\":\n",
    "            thestreet_finding = body_text.find('Updated at')\n",
    "            if thestreet_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if s == 0:  # 첫 빈 공간 고려하기\n",
    "            body += body_text\n",
    "        else:\n",
    "            body = body + ' ' + body_text\n",
    "        body = body.replace('\\n', '')\n",
    "        body = body.replace('\\xa0', ' ')\n",
    "        body = body.strip()\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def body_split(text):\n",
    "    tlink = ''\n",
    "    tokenized_text = sent_tokenize(text)\n",
    "\n",
    "    for d in range(len(tokenized_text)):\n",
    "        tokend = tokenized_text[d]\n",
    "        report_kill = reporter_match(tokend)\n",
    "        report_kill = report_kill.lstrip()\n",
    "        site_kill = investing_match(report_kill)\n",
    "        tokend = site_kill.strip()\n",
    "        if len(tokend) == 0:\n",
    "            continue\n",
    "        if d == 0:\n",
    "            tlink = tokend\n",
    "        else:\n",
    "            tlink = tlink + ' ' + tokend\n",
    "\n",
    "    return tlink\n",
    "\n",
    "\n",
    "def reporter_match(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def reporter_match_and(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return reporter\n",
    "\n",
    "\n",
    "def investing_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_ = 'Investing.com'\n",
    "    spect_ = '[–|-]{1,3}'\n",
    "\n",
    "    site_pattern = re.compile(site_ + ' ' + spect_)\n",
    "    site_name = site_pattern.match(text)\n",
    "    # print(site_name)\n",
    "    if site_name != None:\n",
    "        site_name = site_name.group(0)\n",
    "\n",
    "        text = text.replace(site_name, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_pattern = '(Bloomberg)'\n",
    "    site_name = text.replace(site_pattern, '').lstrip()\n",
    "    spect_pattern = re.compile('[-]+')\n",
    "\n",
    "    doc = spect_pattern.match(site_name)\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = site_name.replace(doc, '').lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_end_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    # text =  '©2022 Bloomberg L.P.'\n",
    "    c_spect = '©[0-9]+\\s[a-zA-Z]+\\s[a-zA-Z]+.[a-zA-Z]+.'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def period_index(body_p):\n",
    "    period_index = ''\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "\n",
    "    for r in range(len(body_p)):\n",
    "        text = body_p[r].text\n",
    "        report_split = reporter_pattern.match(text)\n",
    "        if report_split != None:\n",
    "            rs_ = body_p.index(body_p[r])\n",
    "            period_index = rs_\n",
    "\n",
    "    body_p = body_p[period_index + 1:]\n",
    "\n",
    "    return body_p\n",
    "\n",
    "\n",
    "# 로이터 용 언론사명 본문에서 끄집어서 짜르기\n",
    "def press_local_find(text, press):\n",
    "    # 본문내 불필요한 요일 언론사 이름 지우기\n",
    "    # press = 'Reuters'\n",
    "    spect_p = '(' + press + ')' + ' ' + '-'\n",
    "    dt_press = text.find(spect_p)\n",
    "    text = text[dt_press + len(spect_p):]\n",
    "    text = text.lstrip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220722 로이터 전처리 최신 버전\n",
    "def reuters_preprocess(body_p):\n",
    "    # body_list = []\n",
    "    texts = ''\n",
    "    cnt = 0\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('(Updates with') >= 0:\n",
    "            continue\n",
    "        if text.find('For a Reuters') >= 0:\n",
    "            continue\n",
    "        if text.find('*') >= 0:\n",
    "            continue\n",
    "        reporter_name = reporter_match_and(text)\n",
    "        # print(len(str(reporter_name)),reporter_name )\n",
    "        if len(str(reporter_name)) > 5:\n",
    "            continue\n",
    "        if text == '':\n",
    "            continue\n",
    "        text = reuters_first_del(text)\n",
    "        if text.find('Reporting') >= 0:\n",
    "            text = text.split('(Reporting')\n",
    "            text = text[0]\n",
    "        if cnt == 0:\n",
    "            texts = text\n",
    "        else:\n",
    "            texts = texts + ' ' + text\n",
    "        # print(text)\n",
    "        cnt += 1\n",
    "        # body_list.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def reuters_first_del(text):\n",
    "    text = text.replace('NEW YORK,', '')\n",
    "    text = text.lstrip()\n",
    "    c_spect = '[a-zA-Z]+\\s[0-9]+\\s[^a-zA-Z0-9]{1}[Reuters]+[^a-zA-Z0-9]{1}'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "\n",
    "    text = text.lstrip('-')\n",
    "    text = text.lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220927 인베스팅 본문 오류 해결 함수\n",
    "def conts_clean(conts):\n",
    "    conts_del = ['Position added successfully to: \\n']\n",
    "    conts_full = ''\n",
    "\n",
    "    for c in range(len(conts)):\n",
    "        page_line = conts[c].text\n",
    "        if page_line in conts_del:\n",
    "            continue\n",
    "        page_line = reporter_match(page_line)\n",
    "        page_line = investing_match(page_line)\n",
    "        conts_full += page_line + ' '\n",
    "    conts_full = conts_full.strip()\n",
    "\n",
    "    return conts_full\n",
    "\n",
    "\n",
    "## 그외 기타 전처리 함수\n",
    "# close_time_headline_keywords = ['usastocks', 'usastock', 'djia' , 'nasdaq', 'nyse','gspc','point']\n",
    "\n",
    "def news_close_time(content):\n",
    "    headline_keywords = ['usastocks', 'usastock', 'djia', 'nasdaq', 'nyse', 'gspc', 'point']\n",
    "    checking_text = market_preprocessing(content)\n",
    "    word_count = 0\n",
    "    ## 내용 검사\n",
    "    for i in range(len(checking_text)):\n",
    "        word = checking_text[i]\n",
    "        if word in headline_keywords:\n",
    "            word_count += 1\n",
    "\n",
    "    return word_count  # 1 이상이면 맞다는 것.\n",
    "\n",
    "\n",
    "def market_preprocessing(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"standard & poor's 500\", 'gspc')\n",
    "    sentence = sentence.replace('s&p 500', 'gspc')\n",
    "    sentence = sentence.replace('s&p', 'gspc')\n",
    "    sentence = sentence.replace('u.s. stock', 'usastock')\n",
    "    sentence = sentence.replace('dow jones industrial average', 'djia')\n",
    "    sentence = sentence.replace('%', ' ')\n",
    "    sentence = sentence.replace('%,', ' ')\n",
    "    word_tokens = word_tokenize(sentence)  # 월스트리트저널 기반 분해 -> s&p 500 을 산산조각냄\n",
    "    result = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            result.append(w)\n",
    "    lemm = WordNetLemmatizer()\n",
    "    wpos = []\n",
    "    # 이건 필요없을 듯\n",
    "    for w in result:\n",
    "        wn = lemm.lemmatize(w, pos='n')\n",
    "        wpos.append(wn)\n",
    "    tagged_list = pos_tag(wpos)\n",
    "\n",
    "    sw = ['.', ',', ':', 'CD', '(', ')', '``', \"''\", 'POS']\n",
    "    removed_list = []\n",
    "    for word in tagged_list:\n",
    "        if word[1] not in sw:\n",
    "            removed_list.append(word[0])\n",
    "    removed_list = set(removed_list)\n",
    "    removed_list = list(removed_list)\n",
    "\n",
    "    return list(removed_list)\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def invest_context_end_kill(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    con = 'continue reading'\n",
    "    e_num_pattern = re.compile('^©[0-9]{1,4}\\s[a-zA-Z]+')  # \\s[a-zA-Z]+\n",
    "\n",
    "    if cp[-1].text.lower().find(con) >= 0:\n",
    "        del cp[-1]\n",
    "    if cp[0].text == '\\n':\n",
    "        del cp[0]\n",
    "\n",
    "    for i in range(len(cp)):\n",
    "        m = e_num_pattern.match(cp[i].text)\n",
    "        if m != None:\n",
    "            pm = m.group(0)\n",
    "            if pm.find(pm) >= 0:\n",
    "                del cp[i]\n",
    "                break\n",
    "\n",
    "    return cp\n",
    "\n",
    "\n",
    "###### 데이터 불러오는 것 ###########################################\n",
    "\n",
    "def korean_like_data_split(df):\n",
    "    used_dk = df[df['사용 여부'] == 1]\n",
    "    df_sample = used_dk.loc[:, ['종목코드', '종목명', '종목 뉴스 주소']]\n",
    "    category_ids = []\n",
    "    category_news = []\n",
    "    category_names = []\n",
    "\n",
    "    for s in range(len(df_sample)):\n",
    "        stock_news = df_sample['종목 뉴스 주소'].iloc[s].split('/')\n",
    "        stock_news = stock_news[-1]\n",
    "        stock_ids = df_sample['종목코드'].iloc[s]\n",
    "        stock_name = df_sample['종목명'].iloc[s]\n",
    "        category_ids.append(stock_ids)\n",
    "        category_news.append(stock_news)\n",
    "        category_names.append(stock_name)\n",
    "\n",
    "    return category_ids, category_news, category_names\n",
    "\n",
    "\n",
    "## 아마존 DB 티커 불러올때, 핀비즈에서 검색 안되는 리스트\n",
    "def notfind_symbol_load():\n",
    "    unfind_df = pd.read_excel('./data/notfind_symbol.xlsx')\n",
    "    unfind_df_list = unfind_df.values.tolist()\n",
    "    un_symbol_list = []\n",
    "\n",
    "    for u in range(len(unfind_df_list)):\n",
    "        un_symbol = unfind_df_list[u][0]\n",
    "        un_symbol_list.append(un_symbol)\n",
    "    return un_symbol_list\n",
    "\n",
    "\n",
    "##### DB 함수들 ################################################\n",
    "\n",
    "\n",
    "## DB 저장용 -> 추후 오라클 DB화\n",
    "# 시황 뉴스 데이터 추가 - Question table\n",
    "def market_news_insert(db, url, title, body, w_date, n_date, press, m_time):\n",
    "    category = 'market_news'\n",
    "    cur = db.cursor()\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, market_time) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, m_time))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        pass\n",
    "\n",
    "\n",
    "# 종목/ETF 뉴스 데이터 추가 - Question table\n",
    "def stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker):\n",
    "    cur = db.cursor()  # ISIN\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, symbol) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, ticker))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        print(msg)\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463d965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
