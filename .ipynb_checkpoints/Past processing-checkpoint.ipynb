{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3589ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import load !\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import openpyxl\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime as dtime\n",
    "from dateutil.parser import parse\n",
    "from time import strptime\n",
    "import pytz\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import time\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3a4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 DB TEST 접속\n",
    "db = pymysql.connect(host='127.0.0.1',  port = 3306 , user = 'root',\n",
    "                           password = \"1234\", db = 'jisoo', charset = 'utf8')\n",
    "\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0c7b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6484 / 11352 = 4868\n"
     ]
    }
   ],
   "source": [
    "# 시간 재기용\n",
    "start = time.time()\n",
    "\n",
    "#select_ticker = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'PEP', 'COST', 'ASML', 'AVGO']\n",
    "select_ticker = 0  ## 선택없이 전체\n",
    "ticker_list = stock_info_load(db, select_ticker) # 0 이면 전체 할수있도록\n",
    "nofind_symbol_list = notfind_symbol_load()\n",
    "\n",
    "select_stock = []\n",
    "\n",
    "cnt = 0\n",
    "for n in range(len(ticker_list)):\n",
    "    symbol = ticker_list[n][0]\n",
    "    if symbol in nofind_symbol_list:\n",
    "        cnt += 1\n",
    "        continue\n",
    "    select_stock.append(ticker_list[n])\n",
    "\n",
    "print( len(nofind_symbol_list) , '/',len(ticker_list) , '=', len(select_stock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b58f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "\n",
    "month = date_zero_padding(usa_est.month)\n",
    "days = date_zero_padding(usa_est.day)\n",
    "ymd_date = str(usa_est.year) + '-' + str(month) + '-' + str(days)\n",
    "\n",
    "## 복구 용 \n",
    "\n",
    "ymd_date_list = ['2022-11-08', '2022-11-09','2022-11-10', '2022-11-11', '2022-11-12',  '2022-11-13',  '2022-11-14']\n",
    "category = 'stock_news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ymd_date_list)):\n",
    "    ymd_date = ymd_date_list[i]\n",
    "    print(ymd_date)\n",
    "    result = finviz_news_crawler(db, category, select_stock, ymd_date)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9ee30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589696a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee19ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0a95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################### 시장 시간대 구역 ############################\n",
    "# 시간 관련 라이브러리에서 종종 1자리수 숫자에 0 안붙이는거 해결...\n",
    "def date_zero_padding(m):\n",
    "    if m < 10:\n",
    "        m = '0' + str(m)\n",
    "    else:\n",
    "        m = str(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "# 노멀한 메타 데이터에서 시간 추출\n",
    "def yahoo_market_time(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def yahoo_market_search_time(url, market_time):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if market_time == 0:\n",
    "        market_time = ['00:10:00', '00:20:00']\n",
    "    datetimes = soup.find(\"time\")\n",
    "    datetimes = datetimes['datetime']\n",
    "    dt = datetimes.split('T')\n",
    "    d = dt[0]\n",
    "    t = dt[1].split('.')\n",
    "    t = t[0]\n",
    "    dt = d + ' ' + t\n",
    "    dt = dtime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "    one_hour_ago = dt - datetime.timedelta(hours=4)\n",
    "    date_time = one_hour_ago  # html에서 GMT 기준으로 나오기에, -4 시간 빼야 미국 현재 시각, 표기로는 GMT+9 를 따름\n",
    "\n",
    "    start_hms = market_time[0]\n",
    "    end_hms = market_time[1]\n",
    "    # start_hms = '16:00:00'\n",
    "    # end_hms = '16:30:00'\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "    ymd_date = str(usa_est.year) + '-' + str(usa_est.month) + '-' + str(usa_est.day)\n",
    "\n",
    "    limit_start = ymd_date + ' ' + start_hms\n",
    "    limit_end = ymd_date + ' ' + end_hms\n",
    "\n",
    "    limit_start = dtime.strptime(limit_start, '%Y-%m-%d %H:%M:%S')\n",
    "    limit_end = dtime.strptime(limit_end, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return date_time, limit_start, limit_end\n",
    "\n",
    "\n",
    "###################핀 비즈 구역 ############################\n",
    "\n",
    "def finviz_news_crawler(db, category, ticker_list, ymd_date):\n",
    "    news_info_list = []\n",
    "    error_cnt = 0\n",
    "\n",
    "    for t in range(len(ticker_list)):\n",
    "        ticker = ticker_list[t][0]\n",
    "        # ticker, s_name, s_code = ticker_list[1][0], ticker_list[1][1], ticker_list[1][2]\n",
    "        try:\n",
    "            fin_dic = finviz_news_crawling(ticker, ymd_date)\n",
    "        except:\n",
    "            error_cnt += 1\n",
    "            print('[' + str(error_cnt) + ']', ticker)\n",
    "\n",
    "            continue\n",
    "        if len(fin_dic) == 0:\n",
    "            continue  # 오늘 데이터 없으면 다음 티커 를 탐색\n",
    "        p_list = fin_dic[ymd_date]\n",
    "\n",
    "        for u in range(len(p_list)):\n",
    "            url = p_list[u][-1]\n",
    "            news_checking = finviz_news_check(db, url, ticker)\n",
    "            if news_checking == 1:\n",
    "                continue  # 중복된 url + 종목의 뉴스가 있다면 스킵\n",
    "\n",
    "            try:\n",
    "                news_info = fyahoo_stock_news_inlink_extract(url, headers)\n",
    "                # [news_info] ... + 분류명 (종목 뉴스) , 티커, 종목명, 종목 코드\n",
    "                # [news_info] ... + categroy ('stock_news') , ticker, stock_name, stock_code\n",
    "                # category column 변경 필요 ->  market_time 으로, 기존의 시황 뉴스는 category = 'market_news' 로\n",
    "\n",
    "                news_info_add = news_info + [category] + [ticker]\n",
    "                url, title, body = news_info_add[0], news_info_add[1], news_info_add[2]\n",
    "                w_date, n_date, press = news_info_add[3], news_info_add[4], news_info_add[5]\n",
    "                category, ticker = news_info_add[6], news_info_add[7]\n",
    "\n",
    "                news_input = stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker)\n",
    "                nadd_info = [url, title, body, w_date, n_date, press, category, ticker]\n",
    "                # 나중엔 삭제 똔느 스킵\n",
    "                news_info_list.append(nadd_info)\n",
    "                # print(url,nadd_info[6:])  # 확인용\n",
    "\n",
    "                ## db 저장하는 함수 필요 : 최근 글 중복 처리하는 방법도 필요\n",
    "\n",
    "            except:\n",
    "                print('Abnormal detected :', url)\n",
    "                continue\n",
    "            time.sleep(2)\n",
    "        # print(ticker,len(news_info_list))\n",
    "    print(error_cnt)\n",
    "    return news_info_list\n",
    "\n",
    "\n",
    "def finviz_news_crawling(name, ymd_date):\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks', 'The Telegraph',\n",
    "                    'Insider Monkey',\n",
    "                    'Benzinga', 'Simply Wall St.', 'Business Wire', 'The Independent', 'Fortune', 'CoinDesk', 'Variety',\n",
    "                    'PR Newswire', 'WWD', 'Skift', 'LA Times', 'The Guardian', 'Poets & Quants', 'GuruFocus.com']\n",
    "\n",
    "    url = \"https://finviz.com/quote.ashx?t=\" + name + \"&p=d\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    date_find = soup.find_all('table', {'class': 'fullview-news-outer'})\n",
    "    date_finding = date_find[0].find_all('td', {'align': 'right'})\n",
    "\n",
    "    news_link = date_find[0].find_all('div', {'class': 'news-link-left'})\n",
    "    news_press = date_find[0].find_all('div', {'class': 'news-link-right'})\n",
    "\n",
    "    purl_list = []\n",
    "\n",
    "    date_dic = {}  # 만약을 위해  view에 보이는 날짜 전부를 수집, 평소엔 가장 상단의 키만을 사용 -> 다음 키 생성시 break\n",
    "\n",
    "    for i in range(len(news_link)):\n",
    "        press_name = news_press[i].find('span').text.lstrip()\n",
    "        press_title = news_link[i].text\n",
    "        press_url = news_link[i].find('a')['href']\n",
    "\n",
    "        date_time = date_finding[i].text\n",
    "        date_split_time = date_time.split(' ')\n",
    "        if len(date_split_time) >= 2:\n",
    "            ymdate = date_split_time[0].split('-')\n",
    "            m, d, y = ymdate[0], ymdate[1], ymdate[2]\n",
    "            m = strptime(m, '%b').tm_mon\n",
    "            if m < 10:\n",
    "                m = '0' + str(m)\n",
    "            else:\n",
    "                m = str(m)\n",
    "            ymd_str = '20' + y + '-' + m + '-' + d  # dict 키 용\n",
    "            dtime_pro = date_split_time[1]\n",
    "\n",
    "            if ymd_str != ymd_date:\n",
    "                break  # 오늘 아니면 탈출\n",
    "\n",
    "            date_dic[ymd_str] = []\n",
    "\n",
    "            if len(date_dic) > 2:\n",
    "                del date_dic[ymd_str]\n",
    "                break  # 딱 하루치만 보기\n",
    "\n",
    "        else:\n",
    "            dtime_pro = date_split_time[0]\n",
    "\n",
    "        if press_url.find('finance.yahoo') == -1:\n",
    "            continue\n",
    "        if press_name not in c_press_list:\n",
    "            continue\n",
    "\n",
    "        date_dic[ymd_str].append([dtime_pro, press_name, press_title, press_url])\n",
    "    return date_dic\n",
    "\n",
    "\n",
    "def finviz_news_check(db, url, ticker):\n",
    "    # db에 url 가 있다면 break! ]\n",
    "    checking_sql = \"SELECT * FROM US_Stock_Market_News WHERE url = '\" + url + \"' and symbol ='\" + ticker + \"'\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    if len(result) >= 1:\n",
    "        result = 1  # 1이면 contine\n",
    "    else:\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "\n",
    "def stock_info_load(db, select_ticker):\n",
    "    checking_sql = \"SELECT symbol, name, ISIN FROM Company_Information_Data\"\n",
    "\n",
    "    cur = db.cursor()\n",
    "    cur.execute(checking_sql)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    df_db = pd.DataFrame(result, columns=['symbol', 'name', 'ISIN'])\n",
    "\n",
    "    target_sname = []\n",
    "    if select_ticker != 0:\n",
    "        # 선택한 것만 하는 것\n",
    "        for s in range(len(select_ticker)):\n",
    "            ticker = select_ticker[s]\n",
    "            stock_info = df_db[df_db['symbol'] == ticker]\n",
    "            list_con = stock_info.values.tolist()\n",
    "            target_sname.append(list_con[0])\n",
    "\n",
    "        df = pd.DataFrame(target_sname, columns=['symbol', 'name', 'ISIN'])\n",
    "    else:\n",
    "        df = df_db\n",
    "    ticker_list = []\n",
    "\n",
    "    for d in range(len(df)):\n",
    "        ticker = df['symbol'].iloc[d]\n",
    "        s_name = df['name'].iloc[d]\n",
    "        s_code = df['ISIN'].iloc[d]\n",
    "        ticker_list.append([ticker, s_name, s_code])\n",
    "\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "###################야후 파이낸스 구역 ############################\n",
    "\n",
    "def yahoo_crawler(db, mtime):\n",
    "    ## 미국 주식 거래시간\n",
    "    us_market_time_dic = {'BMO(UST)': ['07:30:00', '09:30:00'], 'AMO(UST)': ['10:30:00', '11:30:00'],\n",
    "                          'AMC(UST)': ['17:00:00', '18:30:00'], 'BMO(DST)': ['06:30:00', '09:00:00'],\n",
    "                          'AMO(DST)': ['09:30:00', '10:30:00'], 'AMC(DST)': ['16:00:00', '17:30:00']}\n",
    "    usa_est = datetime.datetime.now(pytz.timezone('America/New_York'))\n",
    "\n",
    "    c_press_list = ['Bloomberg', 'Reuters', 'Yahoo Finance', 'Investing.com', 'Zacks',\n",
    "                    'Fortune' 'LA Times', 'Kiplinger', 'Business Insider', 'USA TODAY', 'Insider']\n",
    "\n",
    "    o_press_list = [\"Investor's  Daily\", 'TheStreet.com']  # 아웃링크 뉴스 언론사\n",
    "    c_press_list = c_press_list + o_press_list\n",
    "    # c_press_list = ['Bloomberg', 'Reuters', 'Kiplinger', 'Business Insider', 'Yahoo Finance', 'USA TODAY', 'LA Times']\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    url_list = yahoo_crawling(db)\n",
    "    market_time = us_market_time_dic[mtime]\n",
    "    # c = category\n",
    "    # 임시 리턴\n",
    "    pre_urls = []\n",
    "    data_list = []\n",
    "    error_list = []\n",
    "    for u in range(len(url_list)):\n",
    "        source = url_list[u][0]\n",
    "        w_press = url_list[u][1]\n",
    "        w_press = w_press.replace('News', '').lstrip()\n",
    "        pre_urls.append([w_press, source])\n",
    "        if w_press in c_press_list:  # 검증한 언론사들이 포함되어 있다면,\n",
    "            # 시간대 좁히기\n",
    "            date_time, limit_start, limit_end = yahoo_market_search_time(source, market_time)\n",
    "            if date_time >= limit_start and date_time < limit_end:  # 시황 마감후 1시 30분 까지가 아니라면\n",
    "                try:\n",
    "                    news_parsing = fyahoo_stock_news_inlink_extract(source, headers)\n",
    "                except:\n",
    "                    error_news = [w_press, source]\n",
    "                    print('error :', error_news)\n",
    "                    error_list.append(error_news)\n",
    "                    continue\n",
    "                title, body = news_parsing[1], news_parsing[2]\n",
    "                # 시황 뉴스의 경우, 시간대 체크 -> 범용으로 쓰이기 위해 따로 둠 v1 -> v2\n",
    "                if title_classifier(title) == 0:\n",
    "                    continue\n",
    "                w_date, n_date, press = news_parsing[3], news_parsing[4], news_parsing[5]\n",
    "                in_data = [source, title, body, w_date, n_date, press, mtime]\n",
    "                # print(in_data)\n",
    "                data_list.append(in_data)\n",
    "                market_news_insert(db, source, title, body, w_date, n_date, press, mtime)\n",
    "\n",
    "    return pre_urls, data_list, error_list\n",
    "\n",
    "\n",
    "## 220614-15 임시 수집 타이밍  :  야후 파이낸스\n",
    "def yahoo_crawling(db):\n",
    "    url = 'https://finance.yahoo.com/topic/stock-market-news/'\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # chrome_options = webdriver.ChromeOptions()\n",
    "    # chrome_options.add_argument('--headless')\n",
    "    # chrome_options.add_argument('--no-sandbox')\n",
    "    # chrome_options.add_argument(\"--single-process\")\n",
    "    # chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    path = 'chromedriver_107.exe'\n",
    "    driver = webdriver.Chrome(path)  # , chrome_options=chrome_options)\n",
    "    count = 200  # 페이지 한번에 보여줄수 있는양\n",
    "    url_list = []\n",
    "    driver.get(url)\n",
    "    # 스크롤 전 높이\n",
    "    before_h = driver.execute_script(\"return window.scrollY\")\n",
    "    # 무한 스크롤\n",
    "    while True:\n",
    "        # 맨 아래로 스크롤을 내린다.\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_css_selector(\"body\").send_keys(Keys.END)\n",
    "        # 스크롤 사이 페이지 로딩 시간\n",
    "        time.sleep(2)\n",
    "        # 스크롤 후 높이\n",
    "        after_h = driver.execute_script(\"return window.scrollY\")\n",
    "        if after_h == before_h:\n",
    "            break\n",
    "        before_h = after_h\n",
    "    #### 페이지내 url 긁어오기\n",
    "    url_list = []\n",
    "\n",
    "    for i in range(count):  # 페이지 한계로 200 개 정도 확인\n",
    "        press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[2]'\n",
    "        href_i = press_i + '/h3/a'\n",
    "        href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "        press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            press_i = '//*[@id=\"Fin-Stream\"]/ul/li[' + str(i + 1) + ']/div/div/div[1]'\n",
    "            href_i = press_i + '/h3/a'\n",
    "            href_xpath = driver.find_elements_by_xpath(href_i)\n",
    "            press_xpath = driver.find_elements_by_xpath(press_i)\n",
    "        if len(href_xpath) == 0:\n",
    "            continue\n",
    "\n",
    "        href = href_xpath[0].get_attribute('href')\n",
    "        title = href_xpath[0].text\n",
    "        press = press_xpath[0].text\n",
    "        press = press.split('•')\n",
    "        press = press[0].replace('Business', '').lstrip()\n",
    "        url_list.append([href, press])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "################### 파싱 함수구역 ############################\n",
    "\n",
    "# 인링크 방식 뉴스 내용 파싱 : 야후 버전 실상 버전 2이나 구별을 위해 inlink만 사용\n",
    "def fyahoo_stock_news_inlink_extract(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    news_data = []\n",
    "\n",
    "    date_time = yahoo_market_time(url)\n",
    "    now = dtime.now()\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    title = title['content']\n",
    "\n",
    "    press = soup.find(\"img\", {'class': \"caas-img\"})\n",
    "    press = press['alt']\n",
    "\n",
    "    conts = soup.find_all('p')  # .text\n",
    "    content = soup.find_all('div', {'class': 'caas-body'})\n",
    "    body_p = content[0]('p')\n",
    "    body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'USA TODAY':\n",
    "        headline = context_preprocessing(conts[1:3])\n",
    "        # body = context_preprocessing(conts)\n",
    "    if press == 'Reuters':\n",
    "        # body_p = period_index(body_p)\n",
    "        body = body_p_clean(body_p, press)\n",
    "        body = body.split('(Reporting')\n",
    "        bodys = body[0]\n",
    "        body = press_local_find(bodys, press)\n",
    "    if press == 'LA Times':\n",
    "        body = body.split('This story originally')\n",
    "        body = body[0]\n",
    "    if press == 'Bloomberg':\n",
    "        body = body.split('(Updates')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Investing.com':\n",
    "        body = body.split('Related Articles')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == 'Zacks':\n",
    "        body = body.split('Want the latest')\n",
    "        body = body[0]\n",
    "\n",
    "    if press == \"Investor's Business Daily\":  # 아웃링크\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        content = soup.find_all('div', {'class': 'single-post-content post-content drop-cap'})\n",
    "        body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    if press == 'TheStreet.com':\n",
    "        out_url = soup.find(\"meta\", property=\"og:url\")\n",
    "        out_url = out_url['content']\n",
    "        response = requests.get(out_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if out_url.find('realmoney') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-author-rail__body'})\n",
    "            body_p = content[0]('p')\n",
    "        elif out_url.find('aap.') >= 1:\n",
    "            content = soup.find_all('div', {'class': 'article__body article-back-header__body'})\n",
    "            body_p = content[0]('div')\n",
    "        else:\n",
    "            content = soup.find_all('div', {'class': 'm-detail--body'})\n",
    "            body_p = content[0]('p')\n",
    "        # body_p = content[0]('p')\n",
    "        body = body_p_clean(body_p, press)\n",
    "\n",
    "    # body = body_split(body)\n",
    "    # headline_token = sent_tokenize(body)\n",
    "    # if len(headline_token) <= 2:\n",
    "    #    headline = body\n",
    "    # else:\n",
    "    #    headline = headline_token[0] + ' ' + headline_token[1]\n",
    "\n",
    "    inlink_news = [url, title, body, date_time, now, press]\n",
    "\n",
    "    return inlink_news\n",
    "\n",
    "\n",
    "################### 기타 전처리 함수구역 ############################\n",
    "\n",
    "# 만일을 위해서 함수화 했는데.. 생각보다 전처리가 잘 되었기에 ...\n",
    "def clean_text(text):\n",
    "    # cleaned_text = text.replace(\"\\'\", \"'\")\n",
    "    cleaned_text = text.replace('  \\n ', '')\n",
    "    cleaned_text = cleaned_text.replace('(Bloomberg) -- ', '')\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# 날짜를 datetime format으로 변환 하기 위한 함수\n",
    "def dt_format(dt_texts):\n",
    "    a = dt_texts.find('(')\n",
    "    b = dt_texts.find(')')\n",
    "\n",
    "    result = dt_texts[a + 1:b]\n",
    "    result = result.replace(',', '')\n",
    "    dt = result.split(' ')\n",
    "    dt_pm = dt[3].find('PM')  # 있으면 + 없으면 -1 반환\n",
    "\n",
    "    if dt_pm != -1:  ## 오후 타임이면,\n",
    "        pre_dt = dt[3].replace('PM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        if int(pre_dt[0]) == 12:\n",
    "            t, m = pre_dt[0], pre_dt[1]\n",
    "        else:\n",
    "            pm_time = str(int(pre_dt[0]) + 12)\n",
    "            t, m = pm_time, pre_dt[1]\n",
    "\n",
    "    else:  # 오전타임이면\n",
    "        pre_dt = dt[3].replace('AM', '')\n",
    "        pre_dt = pre_dt.split(':')\n",
    "        t, m = pre_dt[0], pre_dt[1]\n",
    "\n",
    "    dt_mon = strptime(dt[0], '%b').tm_mon\n",
    "\n",
    "    if dt_mon < 10:\n",
    "        dt_mon = '0' + str(dt_mon)\n",
    "\n",
    "    str_datetimed = dt[2] + '-' + str(dt_mon) + '-' + dt[1] + ' ' + t + \":\" + m + \":\" + '00'\n",
    "    currdate = datetime.datetime.strptime(str_datetimed, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return currdate\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def context_preprocessing(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    for i in range(len(cp)):\n",
    "        texts = texts + cp[i].text + ' '\n",
    "        texts_list.append(cp[i].text)\n",
    "    cleans = clean_text(texts)\n",
    "    return cleans\n",
    "\n",
    "\n",
    "### 뉴스 분류  : 키워드 방식인데, 아무래도 일부 키워드는 해결 불가하기에, 모델 구축 방향으로 가야함, 데이터가 쌓이면\n",
    "\n",
    "# 맞는 뉴스인지 아닌지 판단 하기  : 키워드 기반\n",
    "def title_classifier(title):\n",
    "    # headline  = data[2]\n",
    "    t_check = title_key_check(title)\n",
    "    # h_check= news_close_time(headline )\n",
    "    classifier = 0\n",
    "    if t_check >= 1:\n",
    "        classifier = 1  # 제목에 시황 뉴스를 암시하는 'stock market today' 가 있다면,\n",
    "    # 제목엔 없지만 헤드라인에 관련 키워드가 한개이상이라면,\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# 규칙 제목 한정 제목 분류\n",
    "def title_key_check(sentence):\n",
    "    title_key = ['smtody', 'cloftrade', 'wallst', 'pts', 'usastocks', 'liveupdate', 'usatocksturn',\n",
    "                 'marketshows', 'ustocksdrop', 'ustockshigh', 'stockmarketcloses', 'marketswrap', 'stockmarket',\n",
    "                 'stocksendlower']\n",
    "\n",
    "    #  'markets', 'market']  # 'futures',\n",
    "    un_key = ['emerging', 'EMERGING', 'global', 'GLOBAL']\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"stock market today\", title_key[0])\n",
    "    sentence = sentence.replace(\"stock market today:\", title_key[0])\n",
    "    sentence = sentence.replace(\"close of trade\", title_key[1])\n",
    "    sentence = sentence.replace(\"us stocks-wall\", title_key[2])\n",
    "    # sentence = sentence.replace(\"wall st\", title_key[2])\n",
    "    sentence = sentence.replace(\"pts;\", title_key[3])\n",
    "    sentence = sentence.replace(\"u.s. stocks\", title_key[4])\n",
    "    sentence = sentence.replace(\"live updates:\", title_key[5])\n",
    "    sentence = sentence.replace(\"us stocks turn\", title_key[6])\n",
    "    sentence = sentence.replace(\"market shows\", title_key[7])\n",
    "    sentence = sentence.replace(\"us stocks drop\", title_key[8])\n",
    "    sentence = sentence.replace(\"us stocks high\", title_key[9])\n",
    "    sentence = sentence.replace(\"stock market closes\", title_key[10])\n",
    "    sentence = sentence.replace(\"markets wrap\", title_key[11])\n",
    "    sentence = sentence.replace(\"stock market\", title_key[12])\n",
    "    sentence = sentence.replace(\"stocks end lower\", title_key[13])\n",
    "\n",
    "    word_count = 0\n",
    "    for t in range(len(title_key)):\n",
    "        if title_key[t] in sentence:\n",
    "            word_count += 1\n",
    "            # print(title_key[t])\n",
    "        # 제거\n",
    "        if title_key[t] in un_key:\n",
    "            word_count = 0\n",
    "            break\n",
    "\n",
    "    return word_count\n",
    "\n",
    "\n",
    "# 야후 파이낸스 전용 전처리\n",
    "def body_p_clean(body_p, press):\n",
    "    # body_p = soup.find_all('p')\n",
    "    body = ''\n",
    "\n",
    "    for s in range(len(body_p)):\n",
    "        body_text = body_p[s].text\n",
    "        if press == 'Yahoo Finance':\n",
    "            if body_text == '—':\n",
    "                break\n",
    "        if press == 'Bloomberg':\n",
    "            if s == 0:\n",
    "                body_text = bloomberg_match(body_text)\n",
    "            else:\n",
    "                if body_p[s].find('a') is not None:\n",
    "                    continue\n",
    "                more_finding = body_text.find('Most Read')\n",
    "                if more_finding == 0:\n",
    "                    continue\n",
    "                read_finding = body_text.find('Read more:')\n",
    "                if read_finding == 0:\n",
    "                    continue\n",
    "                body_text = bloomberg_end_match(body_text)\n",
    "                if body_text.find('More market') != -1:\n",
    "                    break\n",
    "                if len(body_text) == 0:\n",
    "                    continue\n",
    "                if body_text.find('this week:') != -1:\n",
    "                    break\n",
    "                if body_text.find('Read:') != -1:\n",
    "                    break\n",
    "\n",
    "        # 여기서부터 언론사별 불필요한 패턴 자르기\n",
    "\n",
    "        if press == 'Kiplinger':\n",
    "            if body_text.find('SEE MORE') != -1:\n",
    "                continue\n",
    "            if body_text.find('Sign up') != -1:\n",
    "                continue\n",
    "            if body_text.find('YCharts') != -1:\n",
    "                break\n",
    "\n",
    "        if press == 'Zacks':\n",
    "            # check_tag = body_p[s].find('strong')\n",
    "            # if check_tag != None:\n",
    "            #    continue\n",
    "            zacks_finding = body_text.find('Zacks Investment Research?')\n",
    "\n",
    "            if zacks_finding >= 0:\n",
    "                continue\n",
    "\n",
    "            infull_finding = body_text.find('[In full disclosure,')\n",
    "            if infull_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Yahoo Finance':\n",
    "            check_tag = body_p[s].find('em')\n",
    "            if check_tag != None:\n",
    "                continue\n",
    "            yahoo_click_finding = body_text.find('Click here for')\n",
    "            if yahoo_click_finding >= 0:\n",
    "                continue\n",
    "            yahoo_read_finding = body_text.find('Read the latest')\n",
    "            if yahoo_read_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == 'Benzinga':\n",
    "            see_benzinga_finding = body_text.find('See more')\n",
    "            if see_benzinga_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Insider Monkey':\n",
    "            insider_monkey_c_finding = body_text.find('Click to continue')\n",
    "            if insider_monkey_c_finding >= 0:\n",
    "                break\n",
    "            insider_monkey_s_finding = body_text.find('Suggested Articles:')\n",
    "            if insider_monkey_s_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Wire':\n",
    "            business_w_finding = body_text.find('NOTE TO')\n",
    "            if business_w_finding >= 0:\n",
    "                break\n",
    "            business_c_finding = body_text.find('businesswire.com')\n",
    "            if business_c_finding >= 0:\n",
    "                break\n",
    "\n",
    "            local_date_press = '[A-Z]+\\s[A-Z]+,\\s[A-Z]{1}[a-z]+\\s[0-9]{2},\\s[0-9]{4}[-]{1,2}[(][A-Z]{8}\\s[A-Z]{4}[)][-]{1,2}'\n",
    "            body_text = re.sub(local_date_press, '', body_text)\n",
    "\n",
    "        if press == 'Fortune':\n",
    "            fortune_finding = body_text.find('featured on Fortune.com')\n",
    "            if fortune_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Simply Wall St.':\n",
    "            sw_st_finding = body_text.find('free platform')\n",
    "            if sw_st_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Variety':\n",
    "            variety_finding = body_text.find('Best of Variety')\n",
    "            if variety_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'PR Newswire':\n",
    "            pr_newswire_finding = body_text.find('Trademarks')\n",
    "            if pr_newswire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Skift':\n",
    "            skift_finding = body_text.find('Subscribe to Skift')\n",
    "            if skift_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Poets & Quants':\n",
    "            pq_finding = body_text.find('MISS POLL')\n",
    "            if pq_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'ACCESSWIRE':\n",
    "            accwire_finding = body_text.find('View additional')\n",
    "            if accwire_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'InvestorPlace':\n",
    "            inplace_finding = body_text.find('More From InvestorPlace')\n",
    "            if inplace_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'GuruFocus.com':\n",
    "            gf_finding = body_text.find('appeared on GuruFocus')\n",
    "            if gf_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == 'Business Insider':\n",
    "            binsider_finding = body_text.find('Read the ')\n",
    "            if binsider_finding >= 0:\n",
    "                break\n",
    "\n",
    "        if press == \"Investor's Business Daily\":\n",
    "            invest_bd_finding = body_text.find('YOU MAY ALSO LIKE:')\n",
    "            if invest_bd_finding >= 0:\n",
    "                break\n",
    "            invest_plz_finding = body_text.find('Please follow')\n",
    "            if invest_plz_finding >= 0:\n",
    "                break\n",
    "            invest_follow_finding = body_text.find('Follow')\n",
    "            if invest_follow_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if press == \"TheStreet.com\":\n",
    "            thestreet_finding = body_text.find('Updated at')\n",
    "            if thestreet_finding >= 0:\n",
    "                continue\n",
    "\n",
    "        if s == 0:  # 첫 빈 공간 고려하기\n",
    "            body += body_text\n",
    "        else:\n",
    "            body = body + ' ' + body_text\n",
    "        body = body.replace('\\n', '')\n",
    "        body = body.replace('\\xa0', ' ')\n",
    "        body = body.strip()\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def body_split(text):\n",
    "    tlink = ''\n",
    "    tokenized_text = sent_tokenize(text)\n",
    "\n",
    "    for d in range(len(tokenized_text)):\n",
    "        tokend = tokenized_text[d]\n",
    "        report_kill = reporter_match(tokend)\n",
    "        report_kill = report_kill.lstrip()\n",
    "        site_kill = investing_match(report_kill)\n",
    "        tokend = site_kill.strip()\n",
    "        if len(tokend) == 0:\n",
    "            continue\n",
    "        if d == 0:\n",
    "            tlink = tokend\n",
    "        else:\n",
    "            tlink = tlink + ' ' + tokend\n",
    "\n",
    "    return tlink\n",
    "\n",
    "\n",
    "def reporter_match(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def reporter_match_and(text):\n",
    "    # 기자 이름 지우기\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_by = re.compile(by_)\n",
    "    reporter_first_name = re.compile(name_)\n",
    "    reporter_last_name = re.compile(name_)\n",
    "\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "    reporter = reporter_pattern.match(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "        text = text.replace(reporter, '')\n",
    "\n",
    "    return reporter\n",
    "\n",
    "\n",
    "def investing_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_ = 'Investing.com'\n",
    "    spect_ = '[–|-]{1,3}'\n",
    "\n",
    "    site_pattern = re.compile(site_ + ' ' + spect_)\n",
    "    site_name = site_pattern.match(text)\n",
    "    # print(site_name)\n",
    "    if site_name != None:\n",
    "        site_name = site_name.group(0)\n",
    "\n",
    "        text = text.replace(site_name, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    site_pattern = '(Bloomberg)'\n",
    "    site_name = text.replace(site_pattern, '').lstrip()\n",
    "    spect_pattern = re.compile('[-]+')\n",
    "\n",
    "    doc = spect_pattern.match(site_name)\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = site_name.replace(doc, '').lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def bloomberg_end_match(text):\n",
    "    # 언론사 이름 지우기\n",
    "    # text =  '©2022 Bloomberg L.P.'\n",
    "    c_spect = '©[0-9]+\\s[a-zA-Z]+\\s[a-zA-Z]+.[a-zA-Z]+.'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def period_index(body_p):\n",
    "    period_index = ''\n",
    "    by_ = 'By'\n",
    "    name_ = '[a-zA-Z]+'\n",
    "    reporter_pattern = re.compile(by_ + ' ' + name_ + ' ' + name_)\n",
    "\n",
    "    for r in range(len(body_p)):\n",
    "        text = body_p[r].text\n",
    "        report_split = reporter_pattern.match(text)\n",
    "        if report_split != None:\n",
    "            rs_ = body_p.index(body_p[r])\n",
    "            period_index = rs_\n",
    "\n",
    "    body_p = body_p[period_index + 1:]\n",
    "\n",
    "    return body_p\n",
    "\n",
    "\n",
    "# 로이터 용 언론사명 본문에서 끄집어서 짜르기\n",
    "def press_local_find(text, press):\n",
    "    # 본문내 불필요한 요일 언론사 이름 지우기\n",
    "    # press = 'Reuters'\n",
    "    spect_p = '(' + press + ')' + ' ' + '-'\n",
    "    dt_press = text.find(spect_p)\n",
    "    text = text[dt_press + len(spect_p):]\n",
    "    text = text.lstrip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220722 로이터 전처리 최신 버전\n",
    "def reuters_preprocess(body_p):\n",
    "    # body_list = []\n",
    "    texts = ''\n",
    "    cnt = 0\n",
    "    for i in range(len(body_p)):\n",
    "        text = body_p[i].text\n",
    "        if text.find('(Updates with') >= 0:\n",
    "            continue\n",
    "        if text.find('For a Reuters') >= 0:\n",
    "            continue\n",
    "        if text.find('*') >= 0:\n",
    "            continue\n",
    "        reporter_name = reporter_match_and(text)\n",
    "        # print(len(str(reporter_name)),reporter_name )\n",
    "        if len(str(reporter_name)) > 5:\n",
    "            continue\n",
    "        if text == '':\n",
    "            continue\n",
    "        text = reuters_first_del(text)\n",
    "        if text.find('Reporting') >= 0:\n",
    "            text = text.split('(Reporting')\n",
    "            text = text[0]\n",
    "        if cnt == 0:\n",
    "            texts = text\n",
    "        else:\n",
    "            texts = texts + ' ' + text\n",
    "        # print(text)\n",
    "        cnt += 1\n",
    "        # body_list.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def reuters_first_del(text):\n",
    "    text = text.replace('NEW YORK,', '')\n",
    "    text = text.lstrip()\n",
    "    c_spect = '[a-zA-Z]+\\s[0-9]+\\s[^a-zA-Z0-9]{1}[Reuters]+[^a-zA-Z0-9]{1}'\n",
    "    c_pattern = re.compile(c_spect)\n",
    "    doc = c_pattern.match(text)\n",
    "\n",
    "    if doc != None:\n",
    "        doc = doc.group(0)\n",
    "        text = text.replace(doc, '').strip()\n",
    "\n",
    "    text = text.lstrip('-')\n",
    "    text = text.lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# 220927 인베스팅 본문 오류 해결 함수\n",
    "def conts_clean(conts):\n",
    "    conts_del = ['Position added successfully to: \\n']\n",
    "    conts_full = ''\n",
    "\n",
    "    for c in range(len(conts)):\n",
    "        page_line = conts[c].text\n",
    "        if page_line in conts_del:\n",
    "            continue\n",
    "        page_line = reporter_match(page_line)\n",
    "        page_line = investing_match(page_line)\n",
    "        conts_full += page_line + ' '\n",
    "    conts_full = conts_full.strip()\n",
    "\n",
    "    return conts_full\n",
    "\n",
    "\n",
    "## 그외 기타 전처리 함수\n",
    "# close_time_headline_keywords = ['usastocks', 'usastock', 'djia' , 'nasdaq', 'nyse','gspc','point']\n",
    "\n",
    "def news_close_time(content):\n",
    "    headline_keywords = ['usastocks', 'usastock', 'djia', 'nasdaq', 'nyse', 'gspc', 'point']\n",
    "    checking_text = market_preprocessing(content)\n",
    "    word_count = 0\n",
    "    ## 내용 검사\n",
    "    for i in range(len(checking_text)):\n",
    "        word = checking_text[i]\n",
    "        if word in headline_keywords:\n",
    "            word_count += 1\n",
    "\n",
    "    return word_count  # 1 이상이면 맞다는 것.\n",
    "\n",
    "\n",
    "def market_preprocessing(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"standard & poor's 500\", 'gspc')\n",
    "    sentence = sentence.replace('s&p 500', 'gspc')\n",
    "    sentence = sentence.replace('s&p', 'gspc')\n",
    "    sentence = sentence.replace('u.s. stock', 'usastock')\n",
    "    sentence = sentence.replace('dow jones industrial average', 'djia')\n",
    "    sentence = sentence.replace('%', ' ')\n",
    "    sentence = sentence.replace('%,', ' ')\n",
    "    word_tokens = word_tokenize(sentence)  # 월스트리트저널 기반 분해 -> s&p 500 을 산산조각냄\n",
    "    result = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            result.append(w)\n",
    "    lemm = WordNetLemmatizer()\n",
    "    wpos = []\n",
    "    # 이건 필요없을 듯\n",
    "    for w in result:\n",
    "        wn = lemm.lemmatize(w, pos='n')\n",
    "        wpos.append(wn)\n",
    "    tagged_list = pos_tag(wpos)\n",
    "\n",
    "    sw = ['.', ',', ':', 'CD', '(', ')', '``', \"''\", 'POS']\n",
    "    removed_list = []\n",
    "    for word in tagged_list:\n",
    "        if word[1] not in sw:\n",
    "            removed_list.append(word[0])\n",
    "    removed_list = set(removed_list)\n",
    "    removed_list = list(removed_list)\n",
    "\n",
    "    return list(removed_list)\n",
    "\n",
    "\n",
    "# 본문 리스트화된 것을 문자열로 연결\n",
    "def invest_context_end_kill(cp):\n",
    "    texts_list = []\n",
    "    texts = ''\n",
    "    con = 'continue reading'\n",
    "    e_num_pattern = re.compile('^©[0-9]{1,4}\\s[a-zA-Z]+')  # \\s[a-zA-Z]+\n",
    "\n",
    "    if cp[-1].text.lower().find(con) >= 0:\n",
    "        del cp[-1]\n",
    "    if cp[0].text == '\\n':\n",
    "        del cp[0]\n",
    "\n",
    "    for i in range(len(cp)):\n",
    "        m = e_num_pattern.match(cp[i].text)\n",
    "        if m != None:\n",
    "            pm = m.group(0)\n",
    "            if pm.find(pm) >= 0:\n",
    "                del cp[i]\n",
    "                break\n",
    "\n",
    "    return cp\n",
    "\n",
    "\n",
    "###### 데이터 불러오는 것 ###########################################\n",
    "\n",
    "def korean_like_data_split(df):\n",
    "    used_dk = df[df['사용 여부'] == 1]\n",
    "    df_sample = used_dk.loc[:, ['종목코드', '종목명', '종목 뉴스 주소']]\n",
    "    category_ids = []\n",
    "    category_news = []\n",
    "    category_names = []\n",
    "\n",
    "    for s in range(len(df_sample)):\n",
    "        stock_news = df_sample['종목 뉴스 주소'].iloc[s].split('/')\n",
    "        stock_news = stock_news[-1]\n",
    "        stock_ids = df_sample['종목코드'].iloc[s]\n",
    "        stock_name = df_sample['종목명'].iloc[s]\n",
    "        category_ids.append(stock_ids)\n",
    "        category_news.append(stock_news)\n",
    "        category_names.append(stock_name)\n",
    "\n",
    "    return category_ids, category_news, category_names\n",
    "\n",
    "\n",
    "## 아마존 DB 티커 불러올때, 핀비즈에서 검색 안되는 리스트\n",
    "def notfind_symbol_load():\n",
    "    unfind_df = pd.read_excel('./data/notfind_symbol.xlsx')\n",
    "    unfind_df_list = unfind_df.values.tolist()\n",
    "    un_symbol_list = []\n",
    "\n",
    "    for u in range(len(unfind_df_list)):\n",
    "        un_symbol = unfind_df_list[u][0]\n",
    "        un_symbol_list.append(un_symbol)\n",
    "    return un_symbol_list\n",
    "\n",
    "\n",
    "##### DB 함수들 ################################################\n",
    "\n",
    "\n",
    "## DB 저장용 -> 추후 오라클 DB화\n",
    "# 시황 뉴스 데이터 추가 - Question table\n",
    "def market_news_insert(db, url, title, body, w_date, n_date, press, m_time):\n",
    "    category = 'market_news'\n",
    "    cur = db.cursor()\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, market_time) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, m_time))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        pass\n",
    "\n",
    "\n",
    "# 종목 뉴스 데이터 추가 - Question table\n",
    "def stock_news_insert(db, url, title, body, w_date, n_date, press, category, ticker):\n",
    "    cur = db.cursor()  # ISIN\n",
    "    sql = \"\"\"insert into US_Stock_Market_News(url, title, body, w_date, n_date, press, category, symbol) values (%s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    try:\n",
    "        cur.execute(sql, (url, title, body, w_date, n_date, press, category, ticker))\n",
    "        db.commit()\n",
    "    except pymysql.Error as msg:\n",
    "        print(msg)\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
